<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed inference with a Large Language Model (LLM) and node telemetry &mdash; Dragon  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Creating and Using a Pipeline with Multiprocessing" href="pipeline.html" />
    <link rel="prev" title="Multi-node process orchestration and node telemetry" href="torch-scipy-telemetry.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Dragon
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../start/start.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../uguide/uguide.html">Users Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pguide/pguide.html">Programming Guide</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="cbook.html">Solution Cookbook</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="cbook.html#multiprocessing-with-dragon">Multiprocessing with Dragon</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="jupyter.html">Running Jupyter Notebook inside of the Dragon</a></li>
<li class="toctree-l3"><a class="reference internal" href="mp_merge_sort.html">Parallel Merge Sort</a></li>
<li class="toctree-l3"><a class="reference internal" href="mp_queue_demo.html">Parallel Producer - Consumer Communication with Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="mp_scipy_image.html">SciPy Image Convolution Benchmark</a></li>
<li class="toctree-l3"><a class="reference internal" href="torch-scipy-telemetry.html">Multi-node process orchestration and node telemetry</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Distributed inference with a Large Language Model (LLM) and node telemetry</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#examples-of-input-and-output">Examples of input and Output</a></li>
<li class="toctree-l4"><a class="reference internal" href="#installation">Installation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#usage">Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#description-of-the-system-used">Description of the system used</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="pipeline.html">Creating and Using a Pipeline with Multiprocessing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="cbook.html#workflows-with-dragon">Workflows with Dragon</a></li>
<li class="toctree-l2"><a class="reference internal" href="cbook.html#dragon-native">Dragon Native</a></li>
<li class="toctree-l2"><a class="reference internal" href="cbook.html#dragon-data-preview">Dragon Data (Preview)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../benchmarks/benchmarks.html">Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ref/ref.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../infrastructure/infrastructure.html">Infrastructure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../services/services.html">Services</a></li>
<li class="toctree-l1"><a class="reference internal" href="../components/components.html">Low-level Components</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Dragon</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="cbook.html">Solution Cookbook</a></li>
      <li class="breadcrumb-item active">Distributed inference with a Large Language Model (LLM) and node telemetry</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/cbook/distr-inf-telemetry.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="distributed-inference-with-a-large-language-model-llm-and-node-telemetry">
<h1>Distributed inference with a Large Language Model (LLM) and node telemetry<a class="headerlink" href="#distributed-inference-with-a-large-language-model-llm-and-node-telemetry" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>This example presents an application where we perform distributed inference by
using an LLM with the Dragon runtime and standard Python multiprocessing interfaces. We also perform node
monitoring and visualization. Specifically, we run a chatbot service where a user provides questions and receives responses from the chatbot.
We use a Jupyter notebook with Dragon as the front end where the user can provide prompts/queries.
For the telemetry component, we use Prometheus Server to generate time-series data which are then ported for visualization into Grafana.
We used the <a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/blenderbot">Blenderbot</a> chatbot as our language model to respond to the prompts input by the user.</p>
<p>Our process architecture is as follows. We create a pool of workers, referred
to as inference workers, that perform inference. We also create another pool of workers that are responsible for
gathering telemetry data, and we start one process on each node in our allocation (<code class="code docutils literal notranslate"><span class="pre">telemetry</span> <span class="pre">workers</span></code>). Last, we have a third pool of workers, the <code class="code docutils literal notranslate"><span class="pre">response</span> <span class="pre">workers</span></code>,
which are responsible for returning the correct answer to the correct prompt. We also use two Dragon queues that are shared among the processes and nodes.
The first one (<code class="code docutils literal notranslate"><span class="pre">prompt</span> <span class="pre">queue</span></code>) is used for the inference work items from which the inference workers get work. The second one (<code class="code docutils literal notranslate"><span class="pre">response</span> <span class="pre">queue</span></code>) is used for the responses; each inference worker
puts the response into this queue and the response workers get each response and correspond it to the correct prompt.</p>
<p>In this example, we place four inference workers across two nodes.
Each worker utilizes a single Nvidia A100 GPU to perform the inference on the prompt. When a prompt is
input by the user, the prompt and a prompt ID are placed into the <code class="code docutils literal notranslate"><span class="pre">prompt</span> <span class="pre">queue</span></code> that is shared among all the inference workers. The inference workers greedily grab from
this queue, generate a response, and place the response and a response ID with the original prompt and the prompt ID into the <code class="code docutils literal notranslate"><span class="pre">response</span> <span class="pre">queue</span></code>.
We simulate an influx of prompts and, using the telemetry data, visualize the ability to balance this load among the inference workers.</p>
<p>The implementation of an inference worker is the following:</p>
<div class="literal-block-wrapper docutils container" id="id3">
<div class="code-block-caption"><span class="caption-number">Listing 8 </span><span class="caption-text"><strong>llm_backend.py: Inference Worker</strong></span><a class="headerlink" href="#id3" title="Permalink to this code">ÔÉÅ</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">class</span> <span class="nc">InfWorker</span><span class="p">:</span>
<span class="linenos"> 2</span>    <span class="sd">&quot;&quot;&quot; The main inference worker.</span>
<span class="linenos"> 3</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos"> 4</span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q_in</span><span class="p">,</span> <span class="n">q_out</span><span class="p">,</span> <span class="n">device_queue</span><span class="p">,</span> <span class="n">end_ev</span><span class="p">):</span>
<span class="linenos"> 5</span>        <span class="bp">self</span><span class="o">.</span><span class="n">q_in</span> <span class="o">=</span> <span class="n">q_in</span>
<span class="linenos"> 6</span>        <span class="bp">self</span><span class="o">.</span><span class="n">q_out</span> <span class="o">=</span> <span class="n">q_out</span>
<span class="linenos"> 7</span>        <span class="bp">self</span><span class="o">.</span><span class="n">end_ev</span> <span class="o">=</span> <span class="n">end_ev</span>
<span class="linenos"> 8</span>        <span class="bp">self</span><span class="o">.</span><span class="n">device_queue</span> <span class="o">=</span> <span class="n">device_queue</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span>    <span class="k">def</span> <span class="nf">infer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
<span class="linenos">11</span>        <span class="sd">&quot;&quot;&quot;Inference worker function. Worker idx gets a device, initializes the model, and places the model on</span>
<span class="linenos">12</span><span class="sd">        that device. It then enters a while loop that continually checks the shared prompt queue that contains the prompt</span>
<span class="linenos">13</span><span class="sd">        with the prompter&#39;s ID. It tokenizes the prompt, places the prompt on the device, and then generates a response.</span>
<span class="linenos">14</span><span class="sd">        It places this response in the shared response queue. The worker exits once both the prompt queue is empty and</span>
<span class="linenos">15</span><span class="sd">        the end event is set.</span>
<span class="linenos">16</span>
<span class="linenos">17</span><span class="sd">        :param idx: inference worker id</span>
<span class="linenos">18</span><span class="sd">        :type idx: int</span>
<span class="linenos">19</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos">20</span>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;inf_worker </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2"> started&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">21</span>        <span class="n">device</span> <span class="o">=</span> <span class="n">get_device</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device_queue</span><span class="p">)</span>
<span class="linenos">22</span>        <span class="c1"># starting the model and tokenizer in the worker avoids pickling and keeps start up costs equal</span>
<span class="linenos">23</span>        <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">start_model</span><span class="p">()</span>
<span class="linenos">24</span>        <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="linenos">25</span>        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot; worker </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2"> has device </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2"> on </span><span class="si">{</span><span class="n">socket</span><span class="o">.</span><span class="n">gethostname</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">26</span>        <span class="c1"># sit in this while loop and wait for work</span>
<span class="linenos">27</span>        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
<span class="linenos">28</span>            <span class="k">try</span><span class="p">:</span>
<span class="linenos">29</span>                <span class="c1"># gets the prompt from the queue</span>
<span class="linenos">30</span>                <span class="n">prompt_id_pair</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_in</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">31</span>                <span class="c1"># parses id and prompt pair</span>
<span class="linenos">32</span>                <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt_id_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="linenos">33</span>                <span class="nb">id</span> <span class="o">=</span> <span class="n">prompt_id_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="linenos">34</span>                <span class="c1"># generates reply from the model</span>
<span class="linenos">35</span>                <span class="n">reply</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_respond</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="linenos">36</span>                <span class="c1"># removes some special characters</span>
<span class="linenos">37</span>                <span class="n">reply</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">reply</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;&lt;s&gt;&#39;</span><span class="p">)</span>
<span class="linenos">38</span>                <span class="n">reply</span> <span class="o">=</span> <span class="n">reply</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;&lt;/s&gt;&#39;</span><span class="p">)</span>
<span class="linenos">39</span>                <span class="bp">self</span><span class="o">.</span><span class="n">q_out</span><span class="o">.</span><span class="n">put</span><span class="p">((</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">id</span><span class="p">,</span> <span class="n">reply</span><span class="p">,</span> <span class="n">idx</span><span class="p">))</span>
<span class="linenos">40</span>            <span class="k">except</span> <span class="n">queue</span><span class="o">.</span><span class="n">Empty</span><span class="p">:</span>
<span class="linenos">41</span>                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">end_ev</span><span class="o">.</span><span class="n">is_set</span><span class="p">():</span>
<span class="linenos">42</span>                    <span class="c1"># if the queue is empty and the end event is set then we shut down</span>
<span class="linenos">43</span>                    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Shutting down inference worker </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2"> &quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">44</span>                    <span class="k">break</span>
<span class="linenos">45</span>                <span class="k">else</span><span class="p">:</span>
<span class="linenos">46</span>                    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">47</span>            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
<span class="linenos">48</span>                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exception caught: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">49</span>
<span class="linenos">50</span>    <span class="k">def</span> <span class="nf">_respond</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="linenos">51</span>        <span class="sd">&quot;&quot;&quot;generates the response</span>
<span class="linenos">52</span>
<span class="linenos">53</span><span class="sd">        :param prompt: input prompt</span>
<span class="linenos">54</span><span class="sd">        :type prompt: str</span>
<span class="linenos">55</span><span class="sd">        :param model: language model</span>
<span class="linenos">56</span><span class="sd">        :type model: transformers.modeling_utils.PreTrainedModel</span>
<span class="linenos">57</span><span class="sd">        :param tokenizer: tokenizer</span>
<span class="linenos">58</span><span class="sd">        :type tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase</span>
<span class="linenos">59</span><span class="sd">        :param device: device where response is generated</span>
<span class="linenos">60</span><span class="sd">        :type device: torch.device</span>
<span class="linenos">61</span><span class="sd">        :return: response</span>
<span class="linenos">62</span><span class="sd">        :rtype: str</span>
<span class="linenos">63</span><span class="sd">        &quot;&quot;&quot;</span>
<span class="linenos">64</span>        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;reading prompt&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">65</span>        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">prompt</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="linenos">66</span>        <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="linenos">67</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">min_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="linenos">68</span>
<span class="linenos">69</span>        <span class="c1"># Decode the generated text and return it</span>
<span class="linenos">70</span>        <span class="n">reply_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="linenos">71</span>        <span class="k">return</span> <span class="n">reply_ids</span>
</pre></div>
</div>
</div>
<p>The queue that the response is placed in, <code class="code docutils literal notranslate"><span class="pre">q_out</span></code> in the code above, is shared among two response workers that parse the response and return the prompt, prompt ID, response,
and response ID back to the prompter. In this case, that is done by printing this output; however,
if you have multiple users, the response workers would be responsible for returning the response to the correct prompt ID. The structure of a <code class="code docutils literal notranslate"><span class="pre">response</span> <span class="pre">worker</span></code> is
similar to that of an <code class="code docutils literal notranslate"><span class="pre">inference</span> <span class="pre">worker</span></code> in that each worker enters a while loop where they greedily get from the shared <code class="code docutils literal notranslate"><span class="pre">response</span> <span class="pre">queue</span></code> and exit when the queue is
empty and the end event is set.</p>
<p>Below is the <code class="code docutils literal notranslate"><span class="pre">telem_work()</span></code> function that each of the <code class="code docutils literal notranslate"><span class="pre">telemetry</span> <span class="pre">workers</span></code> executes. It includes the metrics for the telemetry data in Prometheus-compatible format.
We define seven metrics in total (<code class="code docutils literal notranslate"><span class="pre">gpu_utilization</span></code>, <code class="code docutils literal notranslate"><span class="pre">gpu_memory_utilization</span></code>, <code class="code docutils literal notranslate"><span class="pre">gpu_memory_used</span></code>, <code class="code docutils literal notranslate"><span class="pre">gpu_memory_free</span></code>, <code class="code docutils literal notranslate"><span class="pre">gpu_memory_total</span></code>, <code class="code docutils literal notranslate"><span class="pre">system_load_average</span></code>, <code class="code docutils literal notranslate"><span class="pre">request_latency</span></code>),
which we update every second until the end event is set. Note line 17 where we start Prometheus metrics server and we set the port to <code class="code docutils literal notranslate"><span class="pre">8000</span></code>.</p>
<div class="literal-block-wrapper docutils container" id="id4">
<div class="code-block-caption"><span class="caption-number">Listing 9 </span><span class="caption-text"><strong>telem_work() function inside telemetry.py: Function that each telemetry worker executes</strong></span><a class="headerlink" href="#id4" title="Permalink to this code">ÔÉÅ</a></div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">def</span> <span class="nf">telem_work</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">end_ev</span><span class="p">):</span>
<span class="linenos"> 2</span>    <span class="sd">&quot;&quot;&quot;Updates a prometheus server with telemetry data from cpus and gpus on each node</span>
<span class="linenos"> 3</span><span class="sd">    :param end_ev: the event used to signal the end of the telemetry data collection</span>
<span class="linenos"> 4</span><span class="sd">    :type end_ev: mp.Event</span>
<span class="linenos"> 5</span><span class="sd">    &quot;&quot;&quot;</span>
<span class="linenos"> 6</span>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;This is a telemetry process on node </span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">uname</span><span class="p">()</span><span class="o">.</span><span class="n">nodename</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos"> 7</span>    <span class="c1"># Create Prometheus metrics</span>
<span class="linenos"> 8</span>    <span class="n">gpu_utilization</span> <span class="o">=</span> <span class="n">Gauge</span><span class="p">(</span><span class="s2">&quot;gpu_utilization&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU utilization percentage&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;hostname&quot;</span><span class="p">,</span> <span class="s2">&quot;gpu_index&quot;</span><span class="p">,</span> <span class="s2">&quot;uuid&quot;</span><span class="p">])</span>
<span class="linenos"> 9</span>    <span class="n">gpu_memory_utilization</span> <span class="o">=</span> <span class="n">Gauge</span><span class="p">(</span><span class="s2">&quot;gpu_memory_utilization&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU memory utilization percentage&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;hostname&quot;</span><span class="p">,</span> <span class="s2">&quot;gpu_index&quot;</span><span class="p">,</span> <span class="s2">&quot;uuid&quot;</span><span class="p">])</span>
<span class="linenos">10</span>    <span class="n">gpu_memory_used</span> <span class="o">=</span> <span class="n">Gauge</span><span class="p">(</span><span class="s2">&quot;gpu_memory_used&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU memory used &quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;hostname&quot;</span><span class="p">,</span> <span class="s2">&quot;gpu_index&quot;</span><span class="p">,</span> <span class="s2">&quot;uuid&quot;</span><span class="p">])</span>
<span class="linenos">11</span>    <span class="n">gpu_memory_free</span> <span class="o">=</span> <span class="n">Gauge</span><span class="p">(</span><span class="s2">&quot;gpu_memory_free&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU memory free &quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;hostname&quot;</span><span class="p">,</span> <span class="s2">&quot;gpu_index&quot;</span><span class="p">,</span> <span class="s2">&quot;uuid&quot;</span><span class="p">])</span>
<span class="linenos">12</span>    <span class="n">gpu_memory_total</span> <span class="o">=</span> <span class="n">Gauge</span><span class="p">(</span><span class="s2">&quot;gpu_memory_total&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU memory total &quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;hostname&quot;</span><span class="p">,</span> <span class="s2">&quot;gpu_index&quot;</span><span class="p">,</span> <span class="s2">&quot;uuid&quot;</span><span class="p">])</span>
<span class="linenos">13</span>    <span class="n">system_load_average</span> <span class="o">=</span> <span class="n">Gauge</span><span class="p">(</span><span class="s2">&quot;system_load_average&quot;</span><span class="p">,</span> <span class="s2">&quot;System load average over 1 minute&quot;</span><span class="p">)</span>
<span class="linenos">14</span>    <span class="n">request_latency</span> <span class="o">=</span> <span class="n">Histogram</span><span class="p">(</span><span class="s2">&quot;request_latency_seconds&quot;</span><span class="p">,</span> <span class="s2">&quot;Request latency in seconds&quot;</span><span class="p">)</span>
<span class="linenos">15</span>
<span class="linenos">16</span>    <span class="c1"># Start the Prometheus metrics server</span>
<span class="linenos">17</span>    <span class="n">start_http_server</span><span class="p">(</span><span class="mi">8000</span><span class="p">)</span>
<span class="linenos">18</span>
<span class="linenos">19</span>    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
<span class="linenos">20</span>        <span class="c1"># TELEMETRY WITH PROMETHEUS</span>
<span class="linenos">21</span>        <span class="c1"># Initialize NVML</span>
<span class="linenos">22</span>        <span class="n">nvmlInit</span><span class="p">()</span>
<span class="linenos">23</span>
<span class="linenos">24</span>        <span class="c1"># Process requests and update metrics</span>
<span class="linenos">25</span>        <span class="c1"># Record the start time of the request</span>
<span class="linenos">26</span>        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="linenos">27</span>
<span class="linenos">28</span>        <span class="c1"># Get the system load averages</span>
<span class="linenos">29</span>        <span class="n">load1</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getloadavg</span><span class="p">()</span>
<span class="linenos">30</span>
<span class="linenos">31</span>        <span class="c1"># Update the system_load_average gauge with the new value</span>
<span class="linenos">32</span>        <span class="n">system_load_average</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">load1</span><span class="p">)</span>
<span class="linenos">33</span>
<span class="linenos">34</span>        <span class="c1"># Get the GPU utilization and memory utilization for each device</span>
<span class="linenos">35</span>        <span class="n">device_count</span> <span class="o">=</span> <span class="n">nvmlDeviceGetCount</span><span class="p">()</span>
<span class="linenos">36</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">device_count</span><span class="p">):</span>
<span class="linenos">37</span>            <span class="n">handle</span> <span class="o">=</span> <span class="n">nvmlDeviceGetHandleByIndex</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
<span class="linenos">38</span>            <span class="n">uuid</span> <span class="o">=</span> <span class="n">nvmlDeviceGetUUID</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
<span class="linenos">39</span>            <span class="n">utilization</span> <span class="o">=</span> <span class="n">nvmlDeviceGetUtilizationRates</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
<span class="linenos">40</span>            <span class="n">memory</span> <span class="o">=</span> <span class="n">nvmlDeviceGetMemoryInfo</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
<span class="linenos">41</span>            <span class="n">gpu_utilization</span><span class="o">.</span><span class="n">labels</span><span class="p">(</span><span class="n">socket</span><span class="o">.</span><span class="n">gethostname</span><span class="p">(),</span> <span class="n">i</span><span class="p">,</span> <span class="n">uuid</span><span class="p">)</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">utilization</span><span class="o">.</span><span class="n">gpu</span><span class="p">)</span>
<span class="linenos">42</span>            <span class="n">gpu_memory_utilization</span><span class="o">.</span><span class="n">labels</span><span class="p">(</span><span class="n">socket</span><span class="o">.</span><span class="n">gethostname</span><span class="p">(),</span> <span class="n">i</span><span class="p">,</span> <span class="n">uuid</span><span class="p">)</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">utilization</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span>
<span class="linenos">43</span>            <span class="n">gpu_memory_used</span><span class="o">.</span><span class="n">labels</span><span class="p">(</span><span class="n">socket</span><span class="o">.</span><span class="n">gethostname</span><span class="p">(),</span> <span class="n">i</span><span class="p">,</span> <span class="n">uuid</span><span class="p">)</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">memory</span><span class="o">.</span><span class="n">used</span> <span class="o">&gt;&gt;</span> <span class="mi">20</span><span class="p">)</span>
<span class="linenos">44</span>            <span class="n">gpu_memory_free</span><span class="o">.</span><span class="n">labels</span><span class="p">(</span><span class="n">socket</span><span class="o">.</span><span class="n">gethostname</span><span class="p">(),</span> <span class="n">i</span><span class="p">,</span> <span class="n">uuid</span><span class="p">)</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">memory</span><span class="o">.</span><span class="n">free</span> <span class="o">&gt;&gt;</span> <span class="mi">20</span><span class="p">)</span>
<span class="linenos">45</span>            <span class="n">gpu_memory_total</span><span class="o">.</span><span class="n">labels</span><span class="p">(</span><span class="n">socket</span><span class="o">.</span><span class="n">gethostname</span><span class="p">(),</span> <span class="n">i</span><span class="p">,</span> <span class="n">uuid</span><span class="p">)</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">memory</span><span class="o">.</span><span class="n">total</span> <span class="o">&gt;&gt;</span> <span class="mi">20</span><span class="p">)</span>
<span class="linenos">46</span>
<span class="linenos">47</span>        <span class="c1"># Record the end time of the request and update the request_latency histogram</span>
<span class="linenos">48</span>        <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="linenos">49</span>        <span class="n">request_latency</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>
<span class="linenos">50</span>
<span class="linenos">51</span>        <span class="c1"># Shut down NVML</span>
<span class="linenos">52</span>        <span class="n">nvmlShutdown</span><span class="p">()</span>
<span class="linenos">53</span>        <span class="c1"># END</span>
<span class="linenos">54</span>
<span class="linenos">55</span>        <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="linenos">56</span>
<span class="linenos">57</span>        <span class="c1"># check if the end event is set. If yes, exit.</span>
<span class="linenos">58</span>        <span class="k">if</span> <span class="n">end_ev</span><span class="o">.</span><span class="n">is_set</span><span class="p">():</span>
<span class="linenos">59</span>            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Telemetry process on node </span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">uname</span><span class="p">()</span><span class="o">.</span><span class="n">nodename</span><span class="si">}</span><span class="s2"> exiting ...&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="linenos">60</span>            <span class="k">break</span>
</pre></div>
</div>
</div>
<section id="examples-of-input-and-output">
<h2>Examples of input and Output<a class="headerlink" href="#examples-of-input-and-output" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Figure 1 provides an example of an input and the response the user receives from the chatbot.</p>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="../_images/llm-grafana-single-prompt-response.jpg"><img alt="../_images/llm-grafana-single-prompt-response.jpg" src="../_images/llm-grafana-single-prompt-response.jpg" style="width: 884.4px; height: 273.59999999999997px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text"><strong>Figure 1: Input prompt and response with IDs for the prompter, inference worker, and response worker</strong></span><a class="headerlink" href="#id5" title="Permalink to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p>To simulate many different users iteracting with a chatbot, we loop over a list of fifteen prompts seven times giving a total of 105 prompts that the four inference workers
to respond to. The input loop and prompts are shown in Figure 2. A sample telemetry output as displayed in Grafana after all these prompts are processed is
shown in Figure 3. Note how the utilization is nearly equal among the GPUs with all starting and ending at the same time. The spikes in utilization prior to
the running of the many prompts are from the models being loaded onto the GPUs at the start up of the inference workers and the worker that responded to the prompt
in Figure 1.</p>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="../_images/llm-grafana-many-prompts.jpg"><img alt="../_images/llm-grafana-many-prompts.jpg" src="../_images/llm-grafana-many-prompts.jpg" style="width: 902.0px; height: 442.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text"><strong>Figure 2: Loop over list of prompts to simulate many users</strong></span><a class="headerlink" href="#id6" title="Permalink to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id7">
<a class="reference internal image-reference" href="../_images/llm-grafana-telem-data.jpg"><img alt="../_images/llm-grafana-telem-data.jpg" src="../_images/llm-grafana-telem-data.jpg" style="width: 906.0px; height: 796.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text"><strong>Figure 3: Node telemetry data that is visualized using Grafana GUI and highlights the load balanced nature of this example</strong></span><a class="headerlink" href="#id7" title="Permalink to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>After installing dragon, the remaining packages needed to install are located in the <code class="code docutils literal notranslate"><span class="pre">requirements_llm.txt</span></code> file.
The version of PyTorch and its dependencies may need to be changed to run on other systems.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="go">&gt; pip install -r requirements_llm.txt</span>
</pre></div>
</div>
<p>Alternatively, the packages and their dependencies can be installed individually. The PyTorch version and corresponding pip command
can be found <a class="reference external" href="https://pytorch.org/get-started/locally/">here</a>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="go">&gt; pip install torch torchvision torchaudio</span>
<span class="linenos">2</span><span class="go">&gt; pip install py3nvml</span>
<span class="linenos">3</span><span class="go">&gt; pip install huggingface-hub</span>
<span class="linenos">4</span><span class="go">&gt; pip install transformers</span>
</pre></div>
</div>
<section id="prometheus-server">
<h3>Prometheus Server<a class="headerlink" href="#prometheus-server" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>You can find information on how to install and configure Prometheus server in the <a class="reference external" href="https://prometheus.io/docs/prometheus/latest/getting_started/">Getting started Prometheus page</a>.</p>
<p>In our case, we used a system named <code class="code docutils literal notranslate"><span class="pre">pinoak</span></code> to run the server. Note that it can run on the login node and there is no need to use a compute node for the server.</p>
<p>Assuming that you have successfully installed the server, next you need to update prometheus yaml file. One of the main fields is the targets
that the server will scrape data from, i.e. in our case, the compute node(s)‚Äô hostnames that we used to run our application that generates the telemetry metrics.
In our example, we used the same system to run the Prometheus server and our application (<code class="code docutils literal notranslate"><span class="pre">pinoak</span></code>). We requested an allocation of two nodes to run our inference application
(<code class="code docutils literal notranslate"><span class="pre">pinoak0043</span></code> and <code class="code docutils literal notranslate"><span class="pre">pinoak0044</span></code>).</p>
<p>Below is a sample yaml file. The server scrapes data from two nodes in our example, <code class="code docutils literal notranslate"><span class="pre">pinoak0043</span></code> and <code class="code docutils literal notranslate"><span class="pre">pinoak0044</span></code>, which we provide as the scrape targets along with the port.</p>
<div class="literal-block-wrapper docutils container" id="id8">
<div class="code-block-caption"><span class="caption-number">Listing 10 </span><span class="caption-text"><strong>prometheus.yml: Example configuration file for Prometheus Server</strong></span><a class="headerlink" href="#id8" title="Permalink to this code">ÔÉÅ</a></div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span># my global config
<span class="linenos"> 2</span>global:
<span class="linenos"> 3</span>scrape_interval: 5s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
<span class="linenos"> 4</span>evaluation_interval: 5s # Evaluate rules every 15 seconds. The default is every 1 minute.
<span class="linenos"> 5</span># scrape_timeout is set to the global default (10s).
<span class="linenos"> 6</span>
<span class="linenos"> 7</span># Alertmanager configuration
<span class="linenos"> 8</span>alerting:
<span class="linenos"> 9</span>alertmanagers:
<span class="linenos">10</span>    - static_configs:
<span class="linenos">11</span>        - targets:
<span class="linenos">12</span>        # - alertmanager:9093
<span class="linenos">13</span>
<span class="linenos">14</span># A scrape configuration containing exactly one endpoint to scrape:
<span class="linenos">15</span># Here it&#39;s Prometheus itself.
<span class="linenos">16</span>#scrape_configs:
<span class="linenos">17</span># The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.
<span class="linenos">18</span># - job_name: &quot;prometheus&quot;
<span class="linenos">19</span>
<span class="linenos">20</span>    # metrics_path defaults to &#39;/metrics&#39;
<span class="linenos">21</span>    # scheme defaults to &#39;http&#39;.
<span class="linenos">22</span>
<span class="linenos">23</span>#static_configs:
<span class="linenos">24</span>#   - targets: [&quot;localhost:9090&quot;]
<span class="linenos">25</span>
<span class="linenos">26</span>scrape_configs:
<span class="linenos">27</span>- job_name: &#39;telemetry_full&#39;
<span class="linenos">28</span>    static_configs:
<span class="linenos">29</span>    - targets: [&#39;pinoak0043:8000&#39;, &#39;pinoak0044:8000&#39;]
</pre></div>
</div>
</div>
<p>The above yaml file is also provided as <code class="code docutils literal notranslate"><span class="pre">example_prometheus.yml</span></code> in the release package inside <code class="code docutils literal notranslate"><span class="pre">examples/jupyter</span></code> directory.
Just make sure to rename it to <code class="code docutils literal notranslate"><span class="pre">prometheus.yml</span></code> if you plan to use it as your prometheus configuration file, otherwise you‚Äôll need to provide
<code class="code docutils literal notranslate"><span class="pre">--config.file</span></code> argument with your configuration file name in the run command.
Remember that in our application, we set the port for the metrics port to <code class="code docutils literal notranslate"><span class="pre">8000</span></code>.</p>
<p>Last, we start the server with the following command:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>cd prometheus_folder
<span class="linenos">2</span>./prometheus
</pre></div>
</div>
</section>
<section id="grafana-server">
<h3>Grafana Server<a class="headerlink" href="#grafana-server" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>First, we need to install Grafana on a system. We follow instructions from the <a class="reference external" href="https://grafana.com/docs/grafana/latest/setup-grafana/installation/">Grafana official documentation</a>.</p>
<p>Assuming that we have it installed, we then start the Grafana server with the following command:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>cd grafana_folder
<span class="linenos">2</span>./bin/grafana-server web
</pre></div>
</div>
<p>Then, on our local computer we set up a tunnel as follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>ssh -NL localhost:1234:localhost:3000 username@system_name
</pre></div>
</div>
<p>where <code class="code docutils literal notranslate"><span class="pre">system_name</span></code> is the system where we installed and run Grafana.</p>
<p>Finally, we access Grafana in our web browser via the following URL:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span>http://localhost:1234
</pre></div>
</div>
<p>To complete the setup and have Prometheus server communicate and send data to Grafana, we need to configure Grafana via the web browser interface. We need to create a new Prometheus data
source by following the instructions <a class="reference external" href="https://grafana.com/docs/grafana/latest/datasources/prometheus/">here</a>. The most important field is the <code class="code docutils literal notranslate"><span class="pre">URL</span></code>, where we need to provide the URL
(ip address and port) of the system that Prometheus server runs on. For example, in our case it was <code class="code docutils literal notranslate"><span class="pre">http://pinoak.us.cray.com:9090</span></code>.
Last, we need to create a new dashboard for visualizing our metrics. You can find information <a class="reference external" href="https://grafana.com/docs/grafana/latest/dashboards/">here</a>.</p>
</section>
</section>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>To run this example, follow the multi-node start up instructions in <a class="reference internal" href="jupyter.html#running-jupyter-notebook-inside-of-the-dragon"><span class="std std-ref">Running Jupyter Notebook inside of the Dragon</span></a>
and then open the <code class="code docutils literal notranslate"><span class="pre">llm_example.ipynb</span></code> notebook which can be found in the release package inside <code class="code docutils literal notranslate"><span class="pre">examples/jupyter</span></code> directory. In order for the telemetry component to work and visualize the data with Grafana, you will
need to also have the Prometheus and Grafana servers started by following the instructions above.</p>
</section>
<section id="description-of-the-system-used">
<h2>Description of the system used<a class="headerlink" href="#description-of-the-system-used" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>For this example, an HPE Cray EX was used. Each node has AMD EPYC 7763 64-core
CPUs and 4x Nvidia A100 GPUs.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="torch-scipy-telemetry.html" class="btn btn-neutral float-left" title="Multi-node process orchestration and node telemetry" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="pipeline.html" class="btn btn-neutral float-right" title="Creating and Using a Pipeline with Multiprocessing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Hewlett Packard Enterprise.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>