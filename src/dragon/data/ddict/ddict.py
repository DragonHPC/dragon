"""
The Distributed Dictionary is a performant and distributed key-value store
that is available to applications and workflows written for the Dragon ecosystem.

This is Dragon's specialized implementation based on the Dragon file-like interface
which relies on Dragon Channels. The Distributed Dictionary works like a standard
Python dictionary except that the data that it holds may span multiple nodes and be
larger than any one node can hold.

The internals of the distributed dictionary rely on several processes include a
single orchestrator process and one or more manager processes. Each client attaches
to managers on an as-needed basis. Clients discover managers by attaching to the
serialized descriptor of a Distributed Dictionary. When using a Distributed Dictionary
in Python, the dictionary will be automatically pickled/serialized and sent to new
processes in the same way a Queue or other objects can be passed as parameters in
multiprocessing.

While the Distributed Dictionary does its best to evenly distributed data across all
managers, a localized wrapper class can be used to direct key/value pairs to user
chosen managers. See the Distributed Dictionary documentation for more details.

"""

from __future__ import annotations
import sys
import math
import logging
import traceback
import cloudpickle
import pickle
import threading
import time
import socket
import os
import copy
from dataclasses import dataclass, field
import heapq
from types import FunctionType
from collections.abc import Iterator
import random
from abc import ABC, abstractmethod
from pathlib import Path
from posixpath import basename
import subprocess


try:
    import pydaos

    DAOS_EXISTS = True
except:
    DAOS_EXISTS = False

from ...utils import b64decode, b64encode, hash as dragon_hash, get_local_kv, host_id
from ...infrastructure.parameters import this_process
from ...infrastructure import messages as dmsg
from ...infrastructure import policy
from ...channels import Channel
from ...native.process import Popen, Process
from ...native.event import Event
from ...native.queue import Queue
from ...dlogging.util import setup_BE_logging, DragonLoggingServices as dls
from ...dlogging.logger import DragonLoggingError
from ...native.machine import Node
from dragon.infrastructure.policy import Policy
from dragon.native.process import ProcessTemplate
from dragon.globalservices.node import query_all

from ... import fli
from dragon.fli import PickleWriteAdapter, PickleReadAdapter
from ... import managed_memory as dmem
from ...rc import DragonError

log = None
KEY_HINT = 1
VALUE_HINT = 2

# This is a default timeout value that is used for send/receive operations.
# Timeouts can be specified if needed by passing in a timeout on the
# distributed dictionary creation. The timeout applies to all operations that
# could timeout in the distributed dictionary. Likely causes of timeouts are
# a manager being overfilled, but some care is taken that does not occur.
# A timeout of None indicates to wait forever. Otherwise, positive timeout
# values are in seconds.
DDICT_DEFAULT_TIMEOUT = None

# This is the default size of a distributed dictionary which would normally be
# overridden.
DDICT_MIN_SIZE = 3 * 1024**2  # 3 MB


class DDictError(DragonLoggingError):
    """
    This is the generic error that all other Distributed Dictionary specific
        errors inherit from. Other types of exceptions my be raised while using the
        Distributed Dictionary, but specific errors generated by this code
        are provided here.

    """
    def __str__(self):
        if not self.lib_msg.strip():
            return f"DDict Exception: {self.msg}\nDragon Error Code: {self.lib_err}"
        return f"DDict Exception: {self.msg}\n*** Dragon C-level Traceback: ***\n{self.lib_msg}\n*** End C-level Traceback: ***\nDragon Error Code: {self.lib_err}"


# This will be raised when a Distributed Dictionary manager failed to reconstruct
# empty managers.
class DDictSyncError(DDictError):
    pass


# This will be raised when a Distributed Dictionary manager has filled to
# capacity. To rectify this you may need to increase the overall size of the
# dictionary and/or devise a better distributed hashing function.
class DDictFullError(DDictError):
    pass


# Timeout errors that occur may be either the generic TimeoutError or
# some exception that inherits from TimeoutError, including the
# DDictTimeoutError given below. If catching these errors in your program
# it is probably best to catch the generic TimeoutError so you catch
# all types of timeout errors.
class DDictTimeoutError(DDictError, TimeoutError):
    pass


class DDictPersistCheckpointError(DDictError):
    pass


# KeyErrors that can also be caught as DDictErrors.
class DDictKeyError(DDictError, KeyError):
    def __init__(self, err, msg, key):
        super().__init__(err, msg)
        super(KeyError, self).__init__(key)
        self.key = key

    def __str__(self):
        err_str = super().__str__()
        return err_str + ": " + self.key


# A Checkpoint Sync error can occur when a client is trying to work with
# a dictionary checkpoint that has now been retired from the working set
# of checkpoints.
class DDictCheckpointSyncError(DDictError):
    pass


class DDictUnableToCreateError(DDictError):
    pass


# A Future Checkpoint error can occur when a client tries to perform batch
# put on the checkpoint that has not existed yet.
class DDictFutureCheckpointError(DDictError):
    pass


# Make the key a little more precise by stripping off pickled data.
def strip_pickled_bytes(byte_str):
    if byte_str[-2:] == b"\x94.":
        byte_str = byte_str[:-2]

    if byte_str[-1:] == b".":
        byte_str = byte_str[:-1]

    if byte_str[:3] == b"\x80\x05\x95":
        byte_str = byte_str[3:]

    return byte_str


@dataclass
class DDictManagerStats:
    """
    Included in manager stats are the manager identifier (0 to num_managers-1), the
    total number of bytes in the manager's pool, the total used bytes in the manager's pool,
    the number of key/value pairs stored in the manager, and the dictionary of free blocks. The
    free blocks has the block size and the number of free blocks of that size. If the pool is
    empty, then there will be one free block of the same size as the total number of bytes of the
    manager's pool. Otherwise, free blocks can be used to see the amount of fragmentation within
    the pool by looking at the various block sizes and number of blocks available. NOTE: Any
    larger block (except the smallest block size) can be split into two smaller blocks for
    smaller allocations.
    """

    manager_id: int
    hostname: str
    total_bytes: int
    total_used_bytes: int
    pool_free_space: int
    pool_utilization: float
    num_keys: int
    free_blocks: dict
    max_pool_allocations: int
    max_pool_allocations_used: int
    current_pool_allocations_used: int


# A SentinelQueue is a queue that raise EOFError when end of
# file is reached. It knows to do this by writing a sentinel
# into the queue when the queue is closed. A SentinelQueue
# contains a multiprocessing Queue. Other methods could
# be implemented for SentinelQueue, but are not needed in
# this example.
class SentinelQueue:
    _SENTINEL = "sentinel_queue_sentinel"

    def __init__(self):
        self._queue = Queue()
        self._put_called = False
        self._get_called = False
        self._sentinel_sent = False
        self._closed = False
        self._sentinel_received = False

    def get(self):
        if self._closed:
            raise RuntimeError("Cannot get an item from a closed SentinelQueue")

        if not self._get_called:
            self._shutdown_event = self._queue.get()
            self._get_called = True

        item = self._queue.get()
        if item == SentinelQueue._SENTINEL:
            self._sentinel_received = True
            raise EOFError("SentinelQueue EOF")

        return item

    def put(self, item):
        if self._closed:
            raise RuntimeError("Cannot put an item on a closed SentinelQueue")

        if not self._put_called:
            self._shutdown_event = Event()
            self._queue.put(self._shutdown_event)
            self._put_called = True

        if self._shutdown_event.is_set():
            raise EOFError("SentinelQueue receiving end closed.")

        self._queue.put(item)

    def close(self):
        if self._closed:
            return

        if self._put_called and not self._sentinel_sent:
            self._sentinel_sent = True
            self._queue.put(SentinelQueue._SENTINEL)

        if self._get_called and not self._sentinel_received:
            # This is a receiving end of a SentinelQueue,
            # so set the event and empty the queue.
            self._shutdown_event.set()
            try:
                while True:
                    self.get()
            except:
                pass

        try:
            self._queue.close()
        except:
            pass

        self._closed = True


# The PQEntry class is needed by the priority
# queue which is used to always know which queue
# to get the next value from. The __lt__ orders
# the priority queue elements by their original
# values. But the queue index of where the value
# came from is carried along with the value in the
# priority queue so the merging algorithm knows where
# to get the next value. In this way, the total number
# of entries in the priority queue is never more than the
# fanin value of the MergePool.
class PQEntry:
    def __init__(self, value, queue_index, comparator):
        self._value = value
        self._queue_index = queue_index
        self._comparator = comparator

    def __lt__(self, other):
        # The comparator is supplied by the user and must take
        # to arguments to be compared and return True or False.
        return self._comparator(self._value, other._value)

    @property
    def queue_index(self):
        return self._queue_index

    @property
    def value(self):
        return self._value

    def __repr__(self):
        return "PQEntry(" + str(self.value) + "," + str(self.queue_index) + "," + str(self._comparator) + ")"

    def __str__(self):
        return repr(self)


def filter_manager(dd, pickled_src_code, pickled_src_args, out_queue, manager_id):
    try:
        src_code = cloudpickle.loads(pickled_src_code)
        src_args = cloudpickle.loads(pickled_src_args)
        my_manager = dd.manager(manager_id)
        args = (my_manager, out_queue) + src_args
        src_code(*args)
    except Exception as ex:
        tb = traceback.format_exc()
        print("There was an exception in filter_manager: %s\n Traceback: %s" % (ex, tb), flush=True, file=sys.stderr)
        raise ex
    finally:
        out_queue.close()


def filter_aggregator(
    dd: DDict, managers_hosts, branching_factor, pickled_src_code, pickled_src_args, pickled_comparator, out_queue
):
    from dragon.native.process_group import ProcessGroup

    try:
        comparator = cloudpickle.loads(pickled_comparator)
        # managers_hosts is a list of tuples (manager_id, hostname)
        if len(managers_hosts) == 1:
            # It was already a in a process started on the manager node, so just call
            # the filter_manager code.
            return filter_manager(dd, pickled_src_code, pickled_src_args, out_queue, managers_hosts[0][0])

        # At larger scales, it is useful create more than one of these processes,
        # themselves in a process group where each of them is given an output queue and
        # a set of managers to merge (instead of merging all managers in one process).
        # Then the second-level merging would merge their managers, writing the merged
        # values to their output queues while a third level merge is done to merge all the
        # second level merges together. In this way, this algorithm can scale to whatever
        # size is necessary.

        filter_queues = []
        shutdown_events = []

        if len(managers_hosts) > branching_factor:
            num_divisions = len(managers_hosts) // branching_factor

            if num_divisions == 1:
                num_divisions = 2

            if num_divisions > branching_factor:
                num_divisions = branching_factor

            division_sz = len(managers_hosts) // num_divisions

            if division_sz * num_divisions < len(managers_hosts):
                division_sz += 1

            grp = ProcessGroup(restart=False)

            while len(managers_hosts) > 0:
                managers_subset = managers_hosts[:division_sz]
                managers_hosts = managers_hosts[division_sz:]
                filter_queue = SentinelQueue()
                filter_queues.append(filter_queue)

                node_name = managers_subset[0][1]
                local_policy = Policy(placement=Policy.Placement.HOST_NAME, host_name=node_name)
                grp.add_process(
                    nproc=1,
                    template=ProcessTemplate(
                        target=filter_aggregator,
                        args=(
                            dd,
                            managers_subset,
                            branching_factor,
                            pickled_src_code,
                            pickled_src_args,
                            pickled_comparator,
                            filter_queue,
                        ),
                        policy=local_policy,
                    ),
                )
        else:
            grp = ProcessGroup(restart=False)

            for manager_id, node_name in managers_hosts:
                filter_queue = SentinelQueue()
                filter_queues.append(filter_queue)

                local_policy = Policy(placement=Policy.Placement.HOST_NAME, host_name=node_name)
                grp.add_process(
                    nproc=1,
                    template=ProcessTemplate(
                        target=filter_manager,
                        args=(dd, pickled_src_code, pickled_src_args, filter_queue, manager_id),
                        policy=local_policy,
                    ),
                )

        grp.init()
        grp.start()
        # prime the priority queue. The values coming from the
        priority_queue = []

        try:
            for i in range(len(filter_queues)):
                try:
                    item = filter_queues[i].get()
                    heapq.heappush(priority_queue, PQEntry(item, i, comparator))
                except EOFError:
                    pass

            # merge the values from different procs

            while len(priority_queue) > 0:
                # If items are not in strictly decreasing order for values, then
                # you need to reverse the < to a > in the PQEntry __lt__ method.
                item = heapq.heappop(priority_queue)
                out_queue.put(item.value)

                try:
                    next = filter_queues[item.queue_index].get()
                    heapq.heappush(priority_queue, PQEntry(next, item.queue_index, comparator))
                except EOFError:
                    pass
        except EOFError:
            # This could happen if the outqueue is closed before this proc is done.
            pass
        except Exception as ex:
            tb = traceback.format_exc()
            print(
                "There was an exception in filter_aggregator priming: %s\n Traceback: %s" % (ex, tb),
                flush=True,
                file=sys.stderr,
            )
            raise ex

        out_queue.close()

        while len(filter_queues) > 0:
            filter_queue = filter_queues.pop()
            try:
                filter_queue.close()
                del filter_queue
            except:
                pass

        grp.join()
        grp.close()

    except Exception as ex:
        tb = traceback.format_exc()
        print("There was an exception in filter_aggregator: %s\n Traceback: %s" % (ex, tb), flush=True, file=sys.stderr)
        raise ex


class FilterIterator:
    def __init__(self, out_queue):
        self._queue = out_queue

    def __iter__(self):
        return self

    def __next__(self):
        try:
            value = self._queue.get()
            return value
        except EOFError:
            raise StopIteration("End of filter stream")

    def close(self):
        self._queue.close()


class FilterContextManager:
    """

    A context manager that cleans up the filtering processes upon exit. See the
    :meth:`DDict.filter` method documentation for a description of how to
    create the context manager and use it in a program.

    """
    def __init__(self, filter_proc, filter_queue):
        self._proc = filter_proc
        self._queue = filter_queue

    def __enter__(self):
        # Simulate resource acquisition
        self._iter = FilterIterator(self._queue)
        return self._iter

    def __exit__(self, exc_type, exc_val, exc_tb):
        # Simulate resource release
        self._iter.close()
        self._proc.join()
        return False


class PickleFDReadAdapter:
    def __init__(self, file, sz):
        self._file = file
        self._sz = sz

    def read(self, sz=0):
        return self._file.read(self._sz)

    def readline(self):
        self.read()


class CheckpointPersister(ABC):
    """

    This class declares methods that all checkpoint persistence classes should
    implement.

    """

    @abstractmethod
    def dump(self, chkpt, force: bool = False):
        """

        Dump the given checkpoint to a persistent store. If force is True
        then overwrite a persisted checkpoint of the same id if necessary.
        See the Checkpoint class in the manager.py for internal details of
        checkpoints if implementing your own CheckpointPersister. Otherwise,
        use one of the subclasses provided by Dragon.

        :param force: Overwrite an existing persisted checkpoint if necessary.

        """
        pass

    @abstractmethod
    def load(self, pool: dmem.MemoryPool, cleanup: bool = False):
       pass

    @abstractmethod
    def advance(self):
        pass

    @abstractmethod
    def position(self, chkpt: int = 0):
        pass

    @abstractmethod
    def checkpoints(self) -> list[int]:
        pass

    @classmethod
    @abstractmethod
    def prepare(cls, orc_logger, persist_path, name, restore_from, read_only):
        pass

    @classmethod
    @abstractmethod
    def dump_metadata(cls, orc_logger, persist_path, name, metadata):
        pass

    @classmethod
    @abstractmethod
    def load_metadata(cls, orc_logger, persist_path, name):
        pass

    @classmethod
    @abstractmethod
    def cleanup_metadata(cls, persist_path, name):
        pass

    @property
    @abstractmethod
    def current_chkpt(self):
        pass

class NULLCheckpointPersister(CheckpointPersister):
    def __init__(
        self,
        ddict_name: str,
        persist_path: str,
        manager_id: int,
        traceit,
        manager,
        persist_freq: int = 0,
        persist_count: int = 0,
    ):
        pass

    def dump(self, chkpt, force: bool = False):
        chkpt.retire()
        pass

    def load(self, pool: dmem.MemoryPool, cleanup: bool = False):
        raise NotImplementedError("The load method is not implemented in NULL checkpoint persister.")

    def advance(self):
        raise NotImplementedError("The advance method is not implemented in NULL checkpoint persister.")

    def position(self, chkpt: int):
        raise NotImplementedError("The position method is not implemented in NULL checkpoint persister.")

    def checkpoints(self):
        return []

    @classmethod
    def prepare(cls, orc_logger, persist_path, name, restore_from, read_only):
        pass

    @classmethod
    def dump_metadata(cls, orc_logger, persist_path, name, metadata):
        path = Path(persist_path)
        fname = path / f"ddict_orc_{name}"
        with open(fname, "w") as f:
            f.write(metadata)

    @classmethod
    def load_metadata(cls, orc_logger, persist_path, name) -> str:
        path = Path(persist_path)
        fname = path / f"ddict_orc_{name}"
        metadata = ""
        with open(fname, "r") as f:
            metadata = f.read()
        return metadata

    @classmethod
    def cleanup_metadata(cls, persist_path, name):
        path = Path(persist_path)
        fname = path / f"ddict_orc_{name}"
        os.remove(fname)

    @property
    def current_chkpt(self):
        pass

class PosixCheckpointPersister(CheckpointPersister):
    def __init__(
        self,
        ddict_name: str,
        persist_path: str,
        manager_id: int,
        log,
        manager,
        persist_freq: int = 0,
        persist_count: int = 0,
    ):
        self._ddict_name = ddict_name
        self._persist_path = persist_path
        self._manager_id = manager_id
        self._persist_freq = persist_freq
        self._persist_count = persist_count
        self._log = log
        self._manager = manager
        self._num_persists = 0
        # Lock is required in PosixPersister for the dump method. The dump method tracks files in
        # persist path and updates the variable self._num_persists and self._available_persisted_checkpoints
        # as needed, which could lead to race condition when multiple threads dumping chkpts at the same time.
        self._lock = threading.Lock()

        # Get a list of available persisted checkpoints by checking the file names in the persist path.
        self._available_persisted_checkpoints = []
        self._path = Path(self._persist_path)
        self._FNAME_PREFIX = f"{self._ddict_name}_{self._manager_id}_"
        self._FNAME_SUFFIX = ".ddict"
        dat_files = list(self._path.glob(f"{self._FNAME_PREFIX}*{self._FNAME_SUFFIX}"))
        # Remove prefix and suffix of files to get available persisted checkpoint IDs.
        for f in dat_files:
            strf = str(basename(f))
            chkpt_id = strf[len(self._FNAME_PREFIX) :][: -len(self._FNAME_SUFFIX)]
            try:
                # Append valid persisted checkpoint ID to the list.
                self._available_persisted_checkpoints.append(int(chkpt_id))
            except Exception as ex:
                self._log(f"Caught exception while reading available persisted checkpoint IDs by parsing file names.")

        self._available_persisted_checkpoints.sort()
        self._num_persists = len(self._available_persisted_checkpoints)
        self._current_chkpt_id = 0

    def _cleanup_later_chkpts(self):
        self._log(f"Cleaning up persisted checkpoint files later than checkpoint {self._current_chkpt_id}.")
        while (
            len(self._available_persisted_checkpoints) != 0
            and self._available_persisted_checkpoints[-1] > self._current_chkpt_id
        ):
            remove_chkpt_id = self._available_persisted_checkpoints.pop()
            file_name = self._path / f"{self._FNAME_PREFIX}{remove_chkpt_id}{self._FNAME_SUFFIX}"
            os.remove(file_name)
        self._log("Cleanup completed.")

    def dump(self, chkpt, force: bool = False):
        """
        Dump retiring checkpoint to disk and retire the checkpoint.
        """
        new_id_added = False

        # If persist frequency or persist count is 0, no checkpoint is persisted.
        if not force and (self._persist_freq == 0 or self._persist_count == 0 or chkpt.id % self._persist_freq != 0):
            return
        # If persist_count is -1, we keep every checkpoint files without cleaning up.
        # TODO: any other mechanism to cleanup the files when disk is full?
        with self._lock:
            if self._persist_count != -1:
                # If the number of persisted checkpoints reach the max number of persist count allowed, remove
                # the oldest persisted checkpoints
                while self._num_persists >= self._persist_count:
                    oldest_persist_chkpt_id = self._available_persisted_checkpoints[0]
                    oldest_persist_chkpt_file_name = (
                        self._path / f"{self._FNAME_PREFIX}{oldest_persist_chkpt_id}{self._FNAME_SUFFIX}"
                    )
                    os.remove(oldest_persist_chkpt_file_name)
                    self._log(f"PosixPersister removed {oldest_persist_chkpt_file_name}.")
                    self._available_persisted_checkpoints.pop(0)
                    self._num_persists -= 1

            # Append chkptID to the list and start writing the chkpt to disk.
            if chkpt.id not in self._available_persisted_checkpoints:
                self._available_persisted_checkpoints.append(chkpt.id)
                self._available_persisted_checkpoints.sort()
                new_id_added = True
            file_name = self._path / f"{self._FNAME_PREFIX}{chkpt.id}{self._FNAME_SUFFIX}"
            self._log(f"About to dump checkpoint {chkpt.id} to {file_name}.")
            try:
                with open(file_name, "wb") as file:
                    # Get the length of serialized checkpoint and write.
                    ser_chkpt = cloudpickle.dumps(chkpt)
                    len_ser_chkpt = len(ser_chkpt)
                    bytes_len_ser_chkpt = len_ser_chkpt.to_bytes(8, byteorder=sys.byteorder)
                    file.write(bytes_len_ser_chkpt)
                    file.write(ser_chkpt)
                    # Map the memory allocations and write to disk.
                    # Each memory is written to disk in the order: memory ID, lenght of the memory, memory content
                    allocs_map = dict()
                    chkpt.build_allocs(allocs_map)
                    for id, mem in allocs_map.items():
                        # Write memory ID.
                        id_bytes = id.to_bytes(8, byteorder=sys.byteorder)
                        file.write(id_bytes)
                        # Get length of memory content and write.
                        mem_view = mem.get_memview()
                        mem_bytes_content = mem_view.tobytes()
                        mem_len_bytes = len(mem_bytes_content).to_bytes(8, byteorder=sys.byteorder)
                        file.write(mem_len_bytes)
                        file.write(mem_bytes_content)
                self._log(f"PosixPersister dump chkpt {chkpt.id} completed!")
                if new_id_added:
                    self._num_persists += 1
                chkpt.retire()
            except Exception as ex:
                # Restore self._available_persisted_checkpoints since the persistence failed.
                if new_id_added:
                    self._available_persisted_checkpoints.remove(chkpt.id)
                    self._available_persisted_checkpoints.sort()
                raise

    def load(self, pool: dmem.MemoryPool, cleanup: bool = False):
        """
        Load persisted checkpoint from disk and return the checkpoint.
        """
        file_name = self._path / f"{self._FNAME_PREFIX}{self._current_chkpt_id}{self._FNAME_SUFFIX}"
        indirect = {}
        chkpt = None
        self._log(f"About to load checkpoint {self._current_chkpt_id} from {file_name}.")
        try:
            with open(file_name, "rb") as file:
                # Read the length of the serialized persisted checkpoint.
                bytes_len_serialize_chkpt = file.read(8)
                if not bytes_len_serialize_chkpt:
                    raise RuntimeError("Could not read the length of serialized persisted checkpoint from disk.")
                len_serialized_chkpt = int.from_bytes(bytes_len_serialize_chkpt, byteorder=sys.byteorder)
                # Load the serialized chkpt with length.
                chkpt = cloudpickle.load(PickleFDReadAdapter(file=file, sz=len_serialized_chkpt))
                # Keep reading files to reload memory to pool.
                # Each memory is read in the order: memory ID, lenght of the memory, memory content
                done = False
                while not done:
                    try:
                        # Read memory ID.
                        id_bytes = file.read(8)
                        if not id_bytes:  # reach to the end of file, the bytes should be empty
                            raise EOFError
                        id = int.from_bytes(id_bytes, byteorder=sys.byteorder)
                        # Read the length of the memory.
                        mem_len_bytes = file.read(8)
                        if not mem_len_bytes:
                            raise RuntimeError("Could not read the length of memory from disk.")
                        mem_len = int.from_bytes(mem_len_bytes, byteorder=sys.byteorder)
                        # Allocate a memory space from pool.
                        mem = pool.alloc(size=mem_len)
                        mem_view = mem.get_memview()
                        # Read memory content.
                        mem_bytes_content = file.read(mem_len)
                        if not mem_bytes_content:
                            raise RuntimeError("Could not read the memory content from disk.")
                        mem_view[:] = mem_bytes_content
                        indirect[id] = mem
                    except EOFError:
                        done = True

            self._log(f"PosixPersister load checkpoint {chkpt.id} completed!")
            chkpt.redirect(indirect)
            chkpt.set_manager(self._manager)
            chkpt.set_pool_mover(self._manager._move_to_pool)
            self._log(f"Checkpoint {chkpt.id} redirection completed!")

            if cleanup:
                self._cleanup_later_chkpts()

            return chkpt
        except Exception as ex:
            tb = traceback.format_exc()
            log.debug("There is an exception while loading persisted checkpoint from disk: %s\n %s", ex, tb)
            raise

    def advance(self):
        """
        Used internally to advance the persisted checkpoint ID to the next available one.
        """
        next_chkpt_id = self._current_chkpt_id + self._persist_freq
        self.position(next_chkpt_id)

    def position(self, chkpt_id: int):
        """
        Used internally to set the checkpoint ID to an available persistsed checkpoint.
        """
        # Check if chkpt_id is available
        if chkpt_id not in self._available_persisted_checkpoints:
            raise DDictPersistCheckpointError(DragonError.DDICT_PERSIST_CHECKPOINT_UNAVAILABLE, "")
        self._current_chkpt_id = chkpt_id

    def checkpoints(self) -> list[int]:
        """
        Return an iterator of all valid persisted checkpoint IDs in ascending order.
        """
        return self._available_persisted_checkpoints

    @classmethod
    def prepare(cls, orc_logger, persist_path, name, restore_from, read_only):

        if not read_only and restore_from is not None:
            # Remove any persisted file with chkpt later than restore_from so that the persisted file
            # won't be overwritten later.
            FNAME_PREFIX = f"{name}_"
            FNAME_SUFFIX = f".ddict"
            # filename = {FNAME_PREFIX}{managerID}_{chkptID}{FNAME_SUFFIX}
            path = Path(persist_path)
            dat_files = list(path.glob(f"{FNAME_PREFIX}*{FNAME_SUFFIX}"))
            for f in dat_files:
                strf = str(basename(f))
                # managerID_chkptID = {managerID}_{chkptID}
                managerID_chkptID = strf[len(FNAME_PREFIX) :][: -len(FNAME_SUFFIX)]
                chkptID = int(managerID_chkptID.split("_")[-1])
                if chkptID > restore_from:
                    os.remove(f)

    @classmethod
    def dump_metadata(cls, orc_logger, persist_path, name, metadata):
        path = Path(persist_path)
        fname = path / f"ddict_orc_{name}"
        with open(fname, "w") as f:
            f.write(metadata)

    @classmethod
    def load_metadata(cls, orc_logger, persist_path, name) -> str:
        path = Path(persist_path)
        fname = path / f"ddict_orc_{name}"
        metadata = ""
        with open(fname, "r") as f:
            metadata = f.read()
        return metadata

    @classmethod
    def cleanup_metadata(cls, persist_path, name):
        path = Path(persist_path)
        fname = path / f"ddict_orc_{name}"
        os.remove(fname)

    @property
    def current_chkpt(self):
        return self._current_chkpt_id


class DAOSCheckpointPersister(CheckpointPersister):
    def __init__(
        self,
        ddict_name: str,
        persist_path: str,
        manager_id: int,
        log,
        manager,
        persist_freq: int = 0,
        persist_count: int = 0,
    ):
        self._manager_id = manager_id
        self._persist_freq = persist_freq
        self._persist_count = persist_count
        self._log = log
        self._manager = manager
        self._num_persists = 0
        self._lock = threading.Lock()

        self._POOL_NAME = persist_path
        self._CONTAINER_NAME = ddict_name
        self._available_persisted_checkpoints = []
        # connect or create a container with name ddict_name
        if self._manager._restore_from is None:
            self._dcont = pydaos.DCont(self._POOL_NAME, self._CONTAINER_NAME)
            self._log(f"Connected to DAOS container {self._CONTAINER_NAME} with pool {self._POOL_NAME}")

            metadata_daos_dd = self._dcont.get("chkpt_metadata")
            self._log("Found and cleared the existing checkpoint metadata DAOS dictionary.")

            metadata_daos_dd[str(self._manager_id)] = ""
        else:
            # Container exists, connect to the container.
            self._dcont = pydaos.DCont(self._POOL_NAME, self._CONTAINER_NAME)
            self._log(f"Connected to DAOS container {self._CONTAINER_NAME} with pool {self._POOL_NAME}")
            # read the metadata of the manager (a list of available persisted chkpts)
            metadata_daos_dd = self._dcont.get("chkpt_metadata")
            avail_chkpt_ids = metadata_daos_dd[str(self._manager_id)]
            if avail_chkpt_ids != None:
                avail_chkpt_ids = str(avail_chkpt_ids, encoding="utf-8")
                avail_chkpt_ids = avail_chkpt_ids.split(",")
                for chkpt_id in avail_chkpt_ids:
                    self._available_persisted_checkpoints.append(int(chkpt_id))
            self._available_persisted_checkpoints.sort()

        self._num_persists = len(self._available_persisted_checkpoints)
        self._current_chkpt_id = 0

    def dump(self, chkpt, force: bool = False):
        """
        Dump retiring checkpoint to disk and retire the checkpoint.
        """
        # The retiring chkpt will be written into the DAOS dictionary with name <manager_id>_<chkpt_id> in
        # in the container named <ddict_name>

        new_id_added = False

        # If persist frequency or persist count is 0, no checkpoint is persisted.
        if not force and (self._persist_freq == 0 or self._persist_count == 0 or chkpt.id % self._persist_freq != 0):
            return

        with self._lock:
            # Cannot destroy DAOS dictionary as there's no such API supports. -> Users need to delete the whole
            # container.

            # Write chkpt in to the DAOS dictionary named <manager ID>_<chkpt ID> in container <ddict_name>

            # Append chkptID to the list and start writing the chkpt to disk.
            if chkpt.id not in self._available_persisted_checkpoints:
                self._available_persisted_checkpoints.append(chkpt.id)
                self._available_persisted_checkpoints.sort()
                new_id_added = True

            daos_dict_name = f"{self._manager_id}_{chkpt.id}"
            self._log(f"About to create or connect to DAOS dictionary {daos_dict_name}")

            try:
                daos_chkpt_dict = self._dcont.dict(daos_dict_name, {})
                self._log("New DAOS dictionary created!")
            except pydaos.PyDError as ex:
                # The dictionary already exists in the container, connect to the dictionary and clear it.
                daos_chkpt_dict = self._dcont.get(daos_dict_name)
                # daos_chkpt_dict.clear() # Clear operation is not supported by DAOS dictionary.
                for key in daos_chkpt_dict:
                    del daos_chkpt_dict[key]
                self._log("Found and cleared the existing DAOS dictionary.")

            self._log(f"About to dump checkpoint {chkpt.id} to DAOS dictionary {daos_dict_name}.")
            try:
                ser_chkpt = cloudpickle.dumps(chkpt)
                allocs_map = dict()
                chkpt.build_allocs_str_key_bytes_val(allocs_map)

                allocs_map["serialized_chkpt"] = ser_chkpt
                daos_chkpt_dict.bput(allocs_map)
                self._log(f"DAOSPersister dump chkpt {chkpt.id} completed!")
                self._num_persists += 1

                # update the metadata DAOS ddict
                avail_chkpt_ids_str = [str(i) for i in self._available_persisted_checkpoints]
                avail_chkpt_ids_str = ",".join(avail_chkpt_ids_str)
                metadata_daos_dd = self._dcont.get("chkpt_metadata")
                metadata_daos_dd[str(self._manager_id)] = avail_chkpt_ids_str

                chkpt.retire()
                self._log(f"DAOSPersister retired chkpt {chkpt.id}!")

            except Exception as ex:
                tb = traceback.format_exc()
                # Restore self._available_persisted_checkpoints since the persistence failed.
                if new_id_added:
                    self._available_persisted_checkpoints.remove(chkpt.id)
                    self._available_persisted_checkpoints.sort()
                self._log(f"Caught exception while dumping retiring chkpt using DAOS checkpoint persister: {ex}\n{tb}")
                raise

    def load(self, pool: dmem.MemoryPool, cleanup: bool = False):
        """
        Load persisted checkpoint from disk and return the checkpoint.
        """
        daos_dict_name = f"{self._manager_id}_{self._current_chkpt_id}"
        indirect = {}
        chkpt = None
        self._log(f"About to load checkpoint {self._current_chkpt_id} from DAOS dictionary {daos_dict_name}.")

        try:
            # connect to the dictionary
            daos_chkpt_dict = self._dcont.get(daos_dict_name)
            self._log(f"connected to the DAOS dictionary.")
            # read the serialized chkpt
            ser_chkpt = daos_chkpt_dict["serialized_chkpt"]
            chkpt = cloudpickle.loads(ser_chkpt)
            self._log(f"finished loading serialized checkpoint")

            for id in daos_chkpt_dict:
                if id != 'serialized_chkpt':
                    val = daos_chkpt_dict[id]
                    mem = pool.alloc(size=len(val))
                    mem_view = mem.get_memview()
                    mem_view[:] = val
                    indirect[int(id)] = mem

            self._log(f"DAOSPersister load checkpoint {chkpt.id} completed!")
            chkpt.redirect(indirect)
            chkpt.set_manager(self._manager)
            chkpt.set_pool_mover(self._manager._move_to_pool)
            self._log(f"Checkpoint {chkpt.id} redirection completed!")

            return chkpt
        except Exception as ex:
            tb = traceback.format_exc()
            self._log("There is an exception while loading persisted checkpoint from disk: %s\n %s", ex, tb)
            raise

    def advance(self):
        """
        Used internally to advance the persisted checkpoint ID to the next available one.
        """
        next_chkpt_id = self._current_chkpt_id + self._persist_freq
        self.position(next_chkpt_id)

    def position(self, chkpt_id: int):
        """
        Used internally to set the checkpoint ID to an available persistsed checkpoint.
        """
        # Check if chkpt_id is available
        if chkpt_id not in self._available_persisted_checkpoints:
            raise DDictPersistCheckpointError(DragonError.DDICT_PERSIST_CHECKPOINT_UNAVAILABLE, "")
        self._current_chkpt_id = chkpt_id

    def checkpoints(self) -> list[int]:
        """
        Return an iterator of all valid persisted checkpoint IDs in ascending order.
        """
        return self._available_persisted_checkpoints

    @classmethod
    def prepare(cls, orc_logger, persist_path, name, restore_from, read_only: bool = False):

        pool_name = persist_path
        if restore_from is None:
            # If not restore, remove the existing containers with name ddict_name
            orc_logger.debug(f"Removing DAOS container {name} from pool {pool_name}")
            subprocess.run(
                f"daos cont destroy {pool_name} {name}",
                shell=True,
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL,
            )
            # TBD: should we catch the exception if the container doesn't exist?
            orc_logger.debug(f"Creating a new DAOS container {name} from pool {pool_name}")
            subprocess.run(f"daos cont create {pool_name} {name} --type PYTHON", shell=True)
            dcont = pydaos.DCont(pool_name, name)
            orc_logger.debug(f"DAOS container created. Now creating metadata dictionary.")
            metadata_daos_dd = dcont.dict("chkpt_metadata", {})
            orc_logger.debug(f"DAOS checkpoint metadata dictionary created.")

    @classmethod
    def dump_metadata(cls, orc_logger, persist_path, name, metadata):
        pool_name = persist_path
        name = f"ddict_orc_{name}"
        # must remove the old ddict metadata container as we will create one later with the same name
        orc_logger.debug(f"about to remove ddict metadata container {name}")
        subprocess.run(
            f"daos cont destroy {pool_name} {name}",
            shell=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
        orc_logger.debug(f"about to dump ddict metadata to container {name}")
        subprocess.run(f"daos cont create {pool_name} {name} --type PYTHON", shell=True)
        orc_logger.debug(f"ddict metadata container created!")
        dcont = pydaos.DCont(pool_name, name)
        # create a new DAOS dictionary and write DDict metadata.
        metadata_daos_dd = dcont.dict("metadata", {})
        metadata_daos_dd["data"] = metadata

    @classmethod
    def load_metadata(cls, orc_logger, persist_path, name) -> str:
        pool_name = persist_path
        name = f"ddict_orc_{name}"
        orc_logger.debug(f"about to load ddict metadata to container {name}")
        dcont = pydaos.DCont(pool_name, name)
        orc_logger.debug("connected to ddict metadata container")
        metadata_daos_dd = dcont.get("metadata")
        metadata = metadata_daos_dd["data"].decode('utf-8')
        return metadata

    @classmethod
    def cleanup_metadata(cls, persist_path, name):
        pool_name = persist_path
        name = f"ddict_orc_{name}"
        subprocess.run(
            f"daos cont destroy {pool_name} {name}",
            shell=True,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )

    @property
    def current_chkpt(self):
        return self._current_chkpt_id


class DDictMappingProxy:
    """
    A read-only live copy of the DDict. This mimics the mapping proxy of a dict.
    It should be noted that the DDict does not maintain the order of insertions into
    it like a dict. DDict mapping proxies do not support being reversed.
    """

    def __init__(self, ddict):
        self._ddict = ddict

    def __contains__(self, key):
        return key in self._ddict

    def __getitem__(self, key):
        return self._ddict[key]

    def __iter__(self):
        return iter(self._ddict)

    def __len__(self):
        return len(self._ddict)

    def __reversed__(self):
        raise NotImplementedError("Keys are not ordered on DDicts and cannot be reversed.")

    def __hash__(self):
        return None

    def copy(self):
        return self._ddict.copy()

    def get(self, key, default=None):
        try:
            return self._ddict[key]
        except KeyError:
            return default

    def items(self):
        return self._ddict.items()

    def keys(self):
        return self._ddict.keys()

    def values(self):
        return self._ddict.values()


class DDictKeysView:
    """
    A live view of the keys of a DDict object. This object provides a keys view much like
    the native Python dict has a keys view object which is created by calling the keys method
    on the DDict. The DDictKeysView provides several operations which are efficiently
    implemented including len, iter, and membership.

    Unlike native dict objects, DDict objects do not support maintaining the order of keys
    inserted into the DDict. Hence, reversed is not supported.

    """

    def __init__(self, ddict, managers, local=False):
        self._ddict = ddict
        self._managers = managers
        self._local = local

    def __len__(self):
        if self._local:
            return self._ddict.local_len()
        return len(self._ddict)

    def __iter__(self):
        return self._ddict._keys(self._managers)

    def __contains__(self, key):
        if self._local:
            # Inefficient implementation
            # for k in self:
            #     if k == key:
            #         return True
            # return False
            raise NotImplementedError("Contains method is not supported for local keys.")
        return key in self._ddict

    def __reversed__(self):
        raise NotImplementedError("There is no ordering to DDict keys and therefore they cannot be reversed.")

    @property
    def mapping(self):
        """
            Calling this results in a read-only version of the DDict.

        Returns:
            DDictMappingProxy: A read-only proxy to the original DDict.

        """
        if self._local:
            raise NotImplementedError("Mapping is not supported for local keys.")
        return DDictMappingProxy(self._ddict)


class DDictValuesView:
    """
    A live view of the values of a DDict object. This object provides a values view much like
    the native Python dict has a keys view object which is created by calling the values method
    on the DDict. The DDictValuesView provides several operations which are efficiently
    implemented including len and iter.

    Unlike native dict objects, DDict objects do not support maintaining the order of keys
    inserted into the DDict. Hence, reversed is not supported.

    """

    def __init__(self, ddict, managers, local=False):
        self._ddict = ddict
        self._managers = managers
        self._local = local

    def __len__(self):
        if self._local:
            return self._ddict.local_len()
        return len(self._ddict)

    def __iter__(self):
        return self._ddict._values(self._managers)

    def __contains__(self, key):
        raise NotImplementedError("Membership operation is not supported for DDict values.")

    def __reversed__(self):
        raise NotImplementedError("There is no ordering to DDict keys and therefore they cannot be reversed.")

    @property
    def mapping(self):
        """
            Calling this results in a read-only version of the DDict.

        Returns:
            DDictMappingProxy: A read-only proxy to the original DDict.

        """
        if self._local:
            raise NotImplementedError("Mapping is not supported for local values.")
        return DDictMappingProxy(self._ddict)


class DDictItemsView:
    """
    A live view of the items of a DDict object. This object provides an items view much like
    the native Python dict has a items view object which is created by calling the items method
    on the DDict. The DDictItemsView provides several operations which are efficiently
    implemented including len and iter.

    Unlike native dict objects, DDict objects do not support maintaining the order of keys
    inserted into the DDict. Hence, reversed is not supported.

    """

    def __init__(self, ddict, managers, local=False):
        self._ddict = ddict
        self._managers = managers
        self._local = local

    def __len__(self):
        if self._local:
            return self._ddict.local_len()
        return len(self._ddict)

    def __iter__(self):
        return self._ddict._items(self._managers)

    def __contains__(self, key):
        raise NotImplementedError("Membership operation is not supported for DDict items.")

    def __reversed__(self):
        raise NotImplementedError("There is no ordering to DDict keys and therefore they cannot be reversed.")

    @property
    def mapping(self):
        """
            Calling this results in a read-only version of the DDict.

        Returns:
            DDictMappingProxy: A read-only proxy to the original DDict.

        """
        if self._local:
            raise NotImplementedError("Mapping is not supported for local items.")
        return DDictMappingProxy(self._ddict)


class DDict:
    """

    The Distributed Dictionary provides a key/value store that is distributed
    across a series of managers and on nodes of a Dragon run-time. The goal
    is to evenly distribute data across all managers to provide a scalable
    implementation of a dictionary with a high degree of allowable
    parallelism. Clients attach to the Distributed Dictionary and store
    key/value pairs in it just like accessing a local dictionary in Python.
    However, the Distributed Dictionary goes beyond what the standard Python
    dictionary supports by including support for distributing data,
    checkpointing, and various other optimization opportunities for specific
    applications.

    The example below creates a DDict, specifying wait_for_keys to cause a
    synchronization point between the parent process and the child process
    which is passed a handle to the DDict. Both parent and child processes
    are sharing the same DDict and using wait_for_keys, no additional
    synchronization was necessary between the two processes in this example.

    Example usage:

        .. highlight:: python
        .. code-block:: python

            import dragon
            import multiprocessing as mp
            from dragon.data import DDict

            def client_put(d):
                d["my_key"] = "my_value"
                d.detach()

            def main():
                mp.set_start_method("dragon")

                d = DDict(2, 1, 3000000, wait_for_keys=True, working_set_size=2)

                # non-persistent key
                proc1 = mp.Process(target=client_put, args=(d,))
                proc1.start()
                val = d["my_key"]
                print(val)
                assert(val == "my_value")
                proc1.join()
                assert(0 == proc1.exitcode)
                d.destroy()

            if __name__ == "__main__":
                main()

    """

    def __init__(
        self,
        managers_per_node: int = 1,
        n_nodes: int = 1,
        total_mem: int = DDICT_MIN_SIZE,
        *,
        working_set_size: int = 1,
        wait_for_keys: bool = False,
        wait_for_writers: bool = False,
        policy: policy.Policy or list[policy.Policy] = None,
        managers_per_policy: int = 1,
        orc_policy: policy.Policy = None,
        persist_freq: int = 0,
        name: str = "",
        timeout: float = DDICT_DEFAULT_TIMEOUT,
        trace: bool = False,
        restart: bool = False,
        read_only: bool = False,
        restore_from: int = None,
        persist_count: int = 0,
        persist_path: str = "",
        persister_class: CheckpointPersister = NULLCheckpointPersister,
        streams_per_manager=5,
    ) -> None:
        """

        Construct a Distributed Dictionary to be shared amongst distributed
        processes running in the Dragon Runtime. The distributed
        dictionary creates the specified number of managers and shards
        the data across all managers. The total memory of the dictionary
        is split across all the managers. There are some extra resources
        that are allocated from the space reserved, so you want to
        allocate a little more than enough room. If the DDict fills up
        you will be notified with an exception. While running, you can
        gather stats from the DDict to see exactly how full it is at its
        fullest point. The amount of extra space required will depend on
        your application, but as a general rule of thumb you can start
        with allocating 30 percent more than you think you will need.
        Then you can call stats to determine how close you were on your
        initial calculation and adjust that amount for future runs. See
        Dragon's documentation section on the Distributed Dictionary
        design for more details about creating and using a distributed
        dictionary.

        :param managers_per_node: The number of managers on each node. The
             total_mem is divided up amongst the managers. If a list of policies
             is provided then this is the number of managers per
             policy. Each policy could be used to start more than
             one manager per node, in a potentially heterogeneous
             way. Defaults to 1.

        :param streams_per_manager: A tuning parameter that when non-zero will
             reduce the time needed for a manager to receive requests from
             clients. If there are many clients connecting, then
             some clients will use their own stream channels when
             connecting.

        :param n_nodes: The number of nodes that will have managers
             deployed on them. This must be set to None if a list of policies is
             provided. Defaults to 1.

        :param total_mem: The total memory in bytes that will be
             sharded evenly across all managers. Defaults to DDICT_MIN_SIZE but
             the constant is really a minimum size for a single
             manager. Typically this should be provided by the
             creator of the DDict.

        :param working_set_size: This sets the
             number of checkpoints that can be simultaneously stored in the
             DDict. Defaults to 1. If your application needs to
             evolve the value of certain key/value pairs over time
             at distinct moments, then checkpointing is a natural
             way to achieve this. By having a working set size of more than
             one and specifying "wait_for_keys=True" each client interacting
             with the DDict can be working independently of other
             clients while still relying on other clients for computation
             results.

        :param wait_for_keys: Setting this to
             true means that each manager will keep track of a set of keys at
             each checkpoint and clients advancing to a new
             checkpoint will block until a requested key/value pair
             is available at their checkpoint before continuing. By
             specifying this all clients will remain in sync with
             each other relative to the size of the working set.
             Defaults to False. It is also possible to store
             key/values that are not part of the checkpointing set
             of key/values. Those keys are called persistent keys
             and will not be affected by setting this argument to
             true. Specifying wait_for_keys also means that readers
             will block while waiting for a non-persistent key to be
             written the first time. Setting this to true requires a
             working set size of at least 2.

        :param wait_for_writers: Setting this
             to true means that each manager will wait for the set of
             clients which have previously written keys to a checkpoint
             in a manager to have all advanced their checkpoint id beyond
             the oldest checkpointing id before retiring a checkpoint
             from the working set. Setting this to true will cause
             clients that are advancing rapidly to block while others
             catch up. Defaults to False. Setting this to true
             requires a working set size of at least 2.

        :param policy: A policy
             can be supplied for starting the managers. Please read about policies in the
             Process Group documentation. Managers are started via a
             Process Group and placement of managers and other
             characteristics can be controlled via a policy or list of policies.
             If a list of policies is given then managers_per_node processes are
             started for each policy. Defaults to None which applies a
             Round-Robin strategy for manager placement on nodes of the run-time.

        :param managers_per_policy: The number of managers started with each
             policy when a list of policies is provided. Defaults to 1.

        :param orc_policy: A policy can be supplied for starting the
             orchestrator. The policy defaults to None which applies a
             Round-Robin strategy for placing the orchestrator
             process.

        :param name: This is a
             base file name to be applied to persisted state for the
             dictionary. This base name along with other identifying
             information is used to restore a distributed dictionary from a
             persisted checkpoint. Defaults to "".

        :param timeout: This is a timeout that will be used for
             all timed operations on the creating client and all managers during
             communication between the distributed components of the
             dictionary. New clients wishing to set their own
             timeout can use the attach method to specify their own
             local timeout. Defaults to None which means to wait
             forever.

        :param trace: Defaults to False. If set to true,
             interaction between clients and managers is logged. This results
             in large logs, but may help in debugging.

        :param restart: Restart a DDict from a persisted state. The name must
             match the name given when the state was persisted.

        :param read_only: Restart the DDict in read-only mode. This mode will
             reject any operations that would modify the DDict. The advance
             method can be useful in replaying a persisted DDict so checkpoints
             can be inspected.

        :param restore_from: An integer checkpoint id to restore from. When restart
             is specified, this should be a valid persisted checkpoint id.

        :param persist_freq: This is the frequency that a checkpoint will be
             persisted. This is independent of the working set size and can be
             any frequency desired. Defaults to 0 which means that no
             persisting will be done.

        :param persist_count: The maximum number of persisted checkpoints to
             maintain. A value of 0 indicates to save/maintain all persisted
             checkpoints.

        :param persist_path: A path specifying where persisted checkpoints should
             be stored. The default is the current working directory.

        :param persister_class: A Checkpoint Persister class. One of PosixCheckpointPersister,
             DAOSCheckpointPersister, a user-defined Persister class, or the default
             NULLCheckpointPersister (which does not persist checkpoints) should be
             specified. Each of these persisters is called when a checkpoint is retired
             from the DDict. The NULLCheckpointPersister frees the storage associated with
             the retiring checkpoint. The others persist the retiring checkpoint and then
             free the storage.

        :returns: A new instance of a distributed dictionary.

        :raises AttributeError: If incorrect parameters are supplied.

        :raises RuntimeError: If there was an unexpected error during initialization.

        """
        # Store all input arguments. We need this to create a copy of the dictionary.
        self._input_args = locals()
        del self._input_args["self"]

        self.setup_logging()

        # This is the pattern used in the pydragon_perf.pyx file
        # It works, but may need review if it's the way we want to do it

        # This block turns on client log for the initial client that creates the dictionary
        try:
            # Arguments checks for persisted checkpoints
            if read_only:
                if len(name) == 0:
                    raise AttributeError(
                        "When creating a read-only Dragon Distributed Dict you must provide the name to restore the dictionary."
                    )

                if restore_from is None:
                    raise AttributeError(
                        "When creating a read-only Dragon Distributed Dict you must provide the checkpoint ID to restore from."
                    )

                if wait_for_keys or wait_for_writers:
                    raise AttributeError(
                        "When creating a read-only Dragon Distributed Dict, specifying wait_for_keys or wait_for_writers is invalid."
                    )

                if working_set_size != 1:
                    raise AttributeError(
                        "When creating a read-only Dragon Distributed Dict, working set size should be 1."
                    )

                if persist_count != 0:
                    raise AttributeError(
                        "When creating a read-only Dragon Distributed Dict, specifying a non-zero persist count is invalid."
                    )

            if restore_from is not None:
                if len(name) == 0:
                    raise AttributeError(
                        "When restoring from a checkpoint in Dragon Distributed Dict you must provide the name to restore the dictionary."
                    )

            elif persister_class == NULLCheckpointPersister and persist_freq != 0:
                raise AttributeError(
                    "When using NULLCheckpointPersister, specifying a non-zero persist_freq is invalid."
                )

            if working_set_size < 1:
                raise ValueError("The working set size of a DDict cannot be less than 1.")

            if working_set_size == 1 and (wait_for_keys or wait_for_writers):
                raise ValueError(
                    "The working set size must be greater than one when specifying wait_for_keys or wait_for_writers."
                )

            if persist_freq < 0:
                raise ValueError("Persist frequency should be non-negative.")

            if persist_count < -1:
                raise ValueError("Persist count should be greater or equal to -1.")

            if type(managers_per_node) is not int and type(managers_per_policy) is not int:
                raise AttributeError(
                    "When creating a Dragon Distributed Dict you must provide managers_per_node or managers_per_policy."
                )

            if type(total_mem) is not int:
                raise AttributeError("When creating a Dragon Distributed Dict you must provide total_mem.")

            if type(policy) is not list and type(n_nodes) is not int:
                raise AttributeError(
                    "When creating a Dragon Distributed Dict if you provide a single policy you must provide n_nodes."
                )

            if type(policy) is list and (n_nodes is not None or managers_per_node is not None):
                raise AttributeError(
                    "When creating a Dragon Distributed Dict if you provide a list of policies you must provide n_nodes = None and managers_per_node=None."
                )

            if restart and policy is not None:
                raise AttributeError(
                    "When restarting a Dragon Distributed Dict if you specified restart you must not provide policy."
                )

            # we overwrite n_nodes and managers_per_node so that we get the correct division of the total memory among the managers that are launched as part of the process group.
            if isinstance(policy, list):
                if len(policy) == 0:
                    raise AttributeError(
                        "When creating a Dragon Distributed Dict if you provide a list of policies you must provide at least one policy in the list."
                    )
                n_nodes = len(policy)
                managers_per_node = managers_per_policy

            self._orc_policy = orc_policy
            # Start the Orchestrator and capture its serialized descriptor so we can connect to it.
            self._orc_proc = Popen(
                executable=sys.executable,
                args=[
                    "-c",
                    f"import dragon.data.ddict.orchestrator as orc; orc.start({managers_per_node}, {n_nodes}, {total_mem}, {trace})",
                ],
                stdout=Popen.PIPE,
                policy=self._orc_policy,
            )

            # Read the serialized FLI of the orchestrator.
            ddict = self._orc_proc.stdout.recv().strip()

            self._orc_connector = fli.FLInterface.attach(b64decode(ddict))
            self._args = (
                working_set_size,
                wait_for_keys,
                wait_for_writers,
                policy,
                persist_freq,
                name,
                timeout,
                restart,
                read_only,
                restore_from,
                persist_path,
                persist_count,
                persister_class,
                streams_per_manager,
            )

            self._managers_per_node = managers_per_node
            self._wait_for_keys = wait_for_keys
            self._wait_for_writers = wait_for_writers
            self._init_props((True, ddict, 0, timeout, trace, self._input_args))
        except AttributeError as ex:
            tb = traceback.format_exc()
            log.debug("There is an attribute error while initializing ddict: %s\nTraceback: %s", ex, tb)
            raise
        except ValueError as ex:
            tb = traceback.format_exc()
            log.debug("There is a value error while initializing ddict: %s\nTraceback: %s", ex, tb)
            raise
        except Exception as ex:
            tb = traceback.format_exc()
            log.debug("There is an exception initializing ddict: %s\nTraceback: %s", ex, tb)
            raise RuntimeError(f"There is an exception initializing ddict: {ex}\nTraceback: {tb}\n")

    def __setstate__(self, args):
        self._init_props(args)

    def _init_props(self, args):
        self._creator, serialized_orc, chkpt_id, timeout, trace, self._input_args = args
        # -1 indicates to use the default timeout since the default
        # is sometimes None and None can't be passed through capnproto
        if timeout == -1:
            self._timeout = DDICT_DEFAULT_TIMEOUT
        else:
            self._timeout = timeout

        self.setup_logging()
        random.seed()

        self._managers = dict()
        self._tag = 0
        self._chkpt_id = chkpt_id
        self._destroyed = False
        self._detached = False
        self._timeout = timeout
        self._trace = trace
        self._chosen_manager = None

        # Batch put
        self._batch_put_started = False
        self._batch_put_msg_tags = set()
        self._num_batch_puts = {}
        self._batch_put_stream_channels = {}
        self._opened_send_handles = {}

        # Broadcast put with batch
        self._bput_strm = None
        self._bput_root_manager_sendh = None
        self._bput_resp_strm = None
        self._bput_respFLI = None
        self._bput_tag = None
        self._num_bputs = 0

        # custom pickler
        self._key_pickler = None
        self._value_pickler = None

        try:
            self._traceit("Connecting to ddict.")
            self._return_channel = Channel.make_process_local()
            self._buffered_return_channel = Channel.make_process_local()
            self._main_stream_channel = Channel.make_process_local()

            self._default_pool = self._return_channel.get_pool()

            self._return_connector = fli.FLInterface(main_ch=self._return_channel)
            self._serialized_return_connector = b64encode(self._return_connector.serialize())

            self._buffered_return_connector = fli.FLInterface(
                main_ch=self._buffered_return_channel, use_buffered_protocol=True
            )
            self._serialized_buffered_return_connector = b64encode(self._buffered_return_connector.serialize())

            self._serialized_orc = serialized_orc
            if self._creator:
                self._create(b64encode(cloudpickle.dumps((self._args))))
            else:
                self._orc_connector = fli.FLInterface.attach(b64decode(serialized_orc))

            self._client_id = None

            # if the client has a local manager, it is the main manager
            self._has_local_manager = False
            self._local_manager = None
            self._main_manager = None
            self._manager_nodes = []
            self._local_managers = []
            self._host_id = host_id()

            self._get_main_manager()
            self._register_client_to_main_manager(timeout)
        except DDictUnableToCreateError as ex:
            tb = traceback.format_exc()
            raise DDictUnableToCreateError(
                DragonError.FAILURE, "Failed to create dictionary: %s\n Traceback: %s\n"%(ex, tb)
            )
        except DDictError as ex:
            tb = traceback.format_exc()
            raise
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug("There is an exception __setstate__ of ddict: %s\n Traceback: %s\n"%(ex, tb))
            except:
                pass
            raise RuntimeError(f"There is an exception __setstate__ of ddict.")

    def __getstate__(self):
        return (False, self.serialize(), self._chkpt_id, self._timeout, self._trace, self._input_args)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        return

    def setup_logging(self):
        # This block turns on a client log for each client
        global log
        if log is None:
            fname = f"{dls.DD}_{socket.gethostname()}_client_{str(this_process.my_puid)}.log"
            setup_BE_logging(service=dls.DD, fname=fname)
            log = logging.getLogger(str(dls.DD))

    def __del__(self):
        try:
            self.detach()
            self._free_process_local_channels()
        except Exception as ex:
            try:
                tb = traceback.format_exc()
                log.debug(
                    "There was an exception while terminating the Distributed Dictionary. Exception is %s\n Traceback: %s\n",
                    ex,
                    tb,
                )
            except:
                pass

    def _create(self, pickled_args):
        msg = dmsg.DDCreate(self._tag_inc(), respFLI=self._serialized_buffered_return_connector, args=pickled_args)
        resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector, buffered=True)

        if resp_msg.err == DragonError.FAILURE:
            raise DDictUnableToCreateError(resp_msg.err, resp_msg.errInfo)

        if resp_msg.err != DragonError.SUCCESS:
            self._cleanup()
            raise DDictError(resp_msg.err, f"Failed to create dictionary! {resp_msg.errInfo}")

        if self._input_args["restore_from"] is not None:
            self._chkpt_id = self._input_args["restore_from"]

    def _free_process_local_channels(self):
        try:
            self._return_channel.destroy_process_local()
            self._return_channel = None
        except:
            pass
        try:
            self._buffered_return_channel.destroy_process_local()
            self._buffered_return_channel = None
        except:
            pass
        try:
            self._main_stream_channel.destroy_process_local()
            self._main_stream_channel = None
        except:
            pass

    def _cleanup(self):
        if self._creator:
            try:
                self._orc_proc.wait()
                log.debug("joined orc proc")
            except Exception as ex:
                tb = traceback.format_exc()
                try:
                    log.debug("There was an exception while joining orchestrator process: %s\n Traceback: %s", ex, tb)
                except:
                    pass

    def _traceit(self, *args, **kw_args):
        if self._trace:
            log.log(logging.INFO, *args, **kw_args)

    def _get_main_manager(self):  # SHGetKV
        try:
            serialized_main_manager = get_local_kv(key=self._serialized_orc)
            self._main_manager_connection = fli.FLInterface.attach(b64decode(serialized_main_manager))
            self._has_local_manager = True
        except KeyError as e:
            # no manager on the node, get a random manager from orchestrator
            try:
                log.info(
                    "Got KeyError during bringup, sending get random manager request to orchestrator. Exception was %s",
                    e,
                )
            except:
                pass
            msg = dmsg.DDRandomManager(self._tag_inc(), respFLI=self._serialized_buffered_return_connector)
            resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector, buffered=True)
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, f"Client failed to get manager from orchestrator. {resp_msg.errInfo}")
            self._main_manager_connection = fli.FLInterface.attach(b64decode(resp_msg.manager))

    def _register_client_to_main_manager(self, timeout):  # client ID assigned here
        msg = dmsg.DDRegisterClient(
            self._tag_inc(),
            respFLI=self._serialized_return_connector,
            bufferedRespFLI=self._serialized_buffered_return_connector,
        )  # register client to the manager (same node)
        resp_msg = self._send_receive([(msg, None)], connection=self._main_manager_connection, buffered=True)
        if resp_msg.err != DragonError.SUCCESS:
            raise DDictError(resp_msg.err, f"Client failed to connect to main manager.{resp_msg.errInfo}")
        # -1 indicates to use the default timeout since the default
        # is sometimes None and None can't be passed through capnproto
        if timeout == -1:
            self._timeout = resp_msg.timeout
        self._client_id = resp_msg.clientID
        self._num_managers = resp_msg.numManagers
        for serialized_node in resp_msg.managerNodes:
            self._manager_nodes.append(cloudpickle.loads(b64decode(serialized_node)))
        # local managers is a list of local managers' ID
        for i in range(self._num_managers):
            if self._manager_nodes[i].h_uid == self._host_id:
                self._local_managers.append(i)
        self._name = resp_msg.name
        self._managers[resp_msg.managerID] = self._main_manager_connection
        self._main_manager = resp_msg.managerID
        if self._has_local_manager:
            self._local_manager = resp_msg.managerID

    def _connect_to_manager(self, manager_id):
        msg = dmsg.DDConnectToManager(self._tag_inc(), clientID=self._client_id, managerID=manager_id)
        resp_msg = self._send_receive([(msg, None)], connection=self._main_manager_connection, buffered=True)
        if resp_msg.err != DragonError.SUCCESS:
            raise DDictError(resp_msg.err, resp_msg.errInfo)

        self._managers[manager_id] = fli.FLInterface.attach(b64decode(resp_msg.manager))

    def _register_client_ID_to_manager(self, manager_id):
        try:
            msg = dmsg.DDRegisterClientID(
                self._tag_inc(),
                clientID=self._client_id,
                respFLI=self._serialized_return_connector,
                bufferedRespFLI=self._serialized_buffered_return_connector,
            )
            resp_msg = self._send_receive([(msg, None)], connection=self._managers[manager_id], buffered=True)
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug(
                    "There was an exception registering client ID %s with manager: %s \n Traceback: %s",
                    self._client_id,
                    ex,
                    tb,
                )
            except:
                pass
            raise RuntimeError(
                f"There was an exception registering client ID {self._client_id=} with manager: {ex} \n Traceback: {tb}"
            )

    def _check_manager_connection(self, manager_id=None, all=False):
        if all:
            for manager_id in range(self._num_managers):
                if manager_id not in self._managers:
                    self._connect_to_manager(manager_id)
                    self._register_client_ID_to_manager(manager_id)
        elif not (manager_id in self._managers):
            self._connect_to_manager(manager_id)
            self._register_client_ID_to_manager(manager_id)

    def _choose_manager_pickle_key(self, key):
        # Check to see if there is a user-defined hash function. If so, then
        # assume it is deterministic and the same across all nodes and use it.
        if self._key_pickler is None:
            pickled_key = cloudpickle.dumps(key)
            stripped_key = strip_pickled_bytes(pickled_key)
        else:
            pickled_key = self._key_pickler.dumps(key)
            stripped_key = None  # Not used if the user provided a custom pickler

        if self._chosen_manager is not None:
            return (self._chosen_manager, pickled_key)

        if self._key_pickler is None:
            # We will try this first if no chosen manager. Might not be instance of
            # one of these. If not we fall down to code below which is what we want
            # when a manager is chosen.
            if isinstance(key, int):
                return (dragon_hash(str(key).encode("utf-8")) % self._num_managers, pickled_key)

            if isinstance(key, str):
                return (dragon_hash(key.encode("utf-8")) % self._num_managers, pickled_key)

            hash_val = dragon_hash(stripped_key)
            manager_id = hash_val % self._num_managers
        else:
            manager_id = dragon_hash(pickled_key) % self._num_managers

        return (manager_id, pickled_key)

    def _tag_inc(self):
        tag = self._tag
        self._tag += 1
        return tag

    def _send(self, msglist, connection, buffered=False):
        self._traceit("Opening send handle.")

        if connection.is_buffered or buffered:
            strm = None
        else:
            strm = self._main_stream_channel

        with connection.sendh(stream_channel=strm, use_main_buffered=buffered, timeout=self._timeout) as sendh:
            for msg, arg in msglist:
                self._traceit("Sending msg: %s", msg)
                if arg is None:
                    sendh.send_bytes(msg.serialize(), buffer=buffered, timeout=self._timeout)
                else:
                    # It is a pickled value so don't call serialize.
                    sendh.send_bytes(msg, arg=arg, buffer=buffered, timeout=self._timeout)

    def _recv_resp(self, resp_set, buffered_connector):
        self._traceit("About to open receive handle on fli to receive response.")
        done = False
        with buffered_connector.recvh(timeout=self._timeout) as recvh:
            while not done:
                resp_ser_msg, hint = recvh.recv_bytes(timeout=self._timeout)
                msg = dmsg.parse(resp_ser_msg)
                if msg.ref not in resp_set:
                    log.info("Tossing lost/timed out response message in DDict Client: %s", msg)
                else:
                    resp_set.remove(msg.ref)
                    done = True

        self._traceit("Response: %s", msg)
        return msg

    def _recv_responses(self, resp_set, num_responses, connector=None):
        msglist = []
        if connector is None:
            connector = self._buffered_return_connector
        for _ in range(num_responses):
            if len(resp_set) == 1 and num_responses > 1:
                # Expects multiple responses with the same ref ID. (ex: DDClear, DDLength ... etc.)
                # This happens when client sends request to the root manager who then broadcasts the
                # request to all other managers. All managers then create the responses using the exact
                # same request tag and send it back to client directly. Therefore, the client should
                # expect multiple responses with the same ref ID.
                resp_msg = self._recv_resp(set(resp_set), connector)
            else:
                resp_msg = self._recv_resp(resp_set, connector)
            msglist.append(resp_msg)

        return msglist

    def _recv_responses_and_check_err(self, resp_set, num_responses, connector=None):
        resp_msgs = []
        if connector is None:
            connector = self._buffered_return_connector
        for _ in range(num_responses):
            resp_msg = None
            if len(resp_set) == 1 and num_responses > 1:
                # Expects multiple responses with the same ref ID. (ex: DDClear, DDLength ... etc.)
                # This happens when client sends request to the root manager who then broadcasts the
                # request to all other managers. All managers then create the responses using the exact
                # same request tag and send it back to client directly. Therefore, the client should
                # expect multiple responses with the same ref ID.
                resp_msg = self._recv_resp(set(resp_set), connector)

            else:
                resp_msg = self._recv_resp(resp_set, connector)

            resp_msgs.append(resp_msg)

            if resp_msg.err == DragonError.MEMORY_POOL_FULL:
                log.debug(f"resp err is MEMORY_POOL_FULL, raising exception")
                raise DDictFullError(
                    DragonError.MEMORY_POOL_FULL,
                    f"Distributed Dictionary Manager {resp_msg.managerID} is full. The key/value pair was not stored.",
                )
            elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                raise DDictCheckpointSyncError(resp_msg.err, resp_msg.errInfo)

            elif resp_msg.err == DragonError.DDICT_FUTURE_CHECKPOINT:
                raise DDictFutureCheckpointError(resp_msg.err, resp_msg.errInfo)

            elif resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)

        return resp_msgs

    def _recv_dmsg_and_val(self, req_msg, key, manager_not_local=False):
        self._traceit("About to open receive handle on fli to receive response and value.")
        with self._return_connector.recvh(use_main_as_stream_channel=True, timeout=self._timeout) as recvh:
            ref = -1
            while ref != req_msg.tag:
                resp_ser_msg, hint = recvh.recv_bytes(timeout=self._timeout)
                resp_msg = dmsg.parse(resp_ser_msg)
                self._traceit("Response: %s", resp_msg)
                ref = resp_msg.ref
                if ref != req_msg.tag:
                    log.info("Tossing lost/timed out message in DDict Client: %s", resp_msg)

            if resp_msg.err == DragonError.KEY_NOT_FOUND:
                value = self.__missing__(key, err=resp_msg.err)
            elif resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
            else:
                try:
                    free_mem = resp_msg.freeMem or manager_not_local
                    log.debug(f"{free_mem=}")
                    if self._value_pickler is None:
                        value = cloudpickle.load(
                            file=PickleReadAdapter(
                                recvh=recvh, hint=VALUE_HINT, free_mem=free_mem, timeout=self._timeout
                            )
                        )
                    else:
                        value = self._value_pickler.load(
                            file=PickleReadAdapter(
                                recvh=recvh, hint=VALUE_HINT, free_mem=free_mem, timeout=self._timeout
                            )
                        )
                except Exception as e:
                    tb = traceback.format_exc()
                    try:
                        log.info("Exception caught in cloudpickle load: %s \n Traceback: %s", e, tb)
                    except:
                        pass
                    raise RuntimeError(f"Exception caught in cloudpickle load: {e} \n Traceback: {tb}")

        return value

    def __missing__(self, key, *, err=DragonError.SUCCESS):
        raise DDictKeyError(err, "The key was not found", key)

    def _send_receive(self, msglist, connection, buffered=False):
        try:
            msg = msglist[0][0]
            tag = msg.tag
            self._send(msglist, connection, buffered=buffered)
            resp_msg = self._recv_resp(set([msg.tag]), self._buffered_return_connector)
            return resp_msg

        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug("There was an exception in the _send_receive in ddict: %s\n Traceback: %s", ex, tb)
            except:
                pass
            raise RuntimeError(f"There was an exception in the _send_receive in ddict: {ex} \n Traceback: {tb}")

    def _send_dmsg_and_key_value(self, manager_id, msg, pickled_key, key, value):
        self._check_manager_connection(manager_id)
        try:
            self._traceit("Opening send handle to manager for put: %s", manager_id)
            with self._managers[manager_id].sendh(
                stream_channel=self._main_stream_channel, timeout=self._timeout
            ) as sendh:
                self._traceit("Sending to manager: %s", msg)
                sendh.send_bytes(msg.serialize(), timeout=self._timeout)
                self._traceit("Sending pickled key to manager: %s", key)
                sendh.send_bytes(pickled_key, arg=KEY_HINT, timeout=self._timeout)
                if self._value_pickler is not None:
                    self._value_pickler.dump(
                        value, file=PickleWriteAdapter(sendh=sendh, hint=VALUE_HINT, timeout=self._timeout)
                    )
                else:
                    cloudpickle.dump(
                        value,
                        file=PickleWriteAdapter(sendh=sendh, hint=VALUE_HINT, timeout=self._timeout),
                        protocol=pickle.HIGHEST_PROTOCOL,
                    )

        except TimeoutError as ex:
            raise DDictTimeoutError(
                DragonError.TIMEOUT,
                f"The operation timed out. This could be a network failure or an out of memory condition.\n{str(ex)}",
            )

    def _put(self, msg, key, value):
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._send_dmsg_and_key_value(manager_id, msg, pickled_key, key, value)
        try:
            resp_msg = self._recv_resp(set([msg.tag]), self._buffered_return_connector)
        except TimeoutError as ex:
            raise DDictTimeoutError(
                DragonError.TIMEOUT,
                f"The operation timed out. This could be a network failure or an out of memory condition.\n{str(ex)}",
            )

        if resp_msg.err == DragonError.MEMORY_POOL_FULL:
            raise DDictFullError(
                DragonError.MEMORY_POOL_FULL,
                f"Distributed Dictionary Manager {manager_id} is full. The key/value pair was not stored.",
            )

        elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
            raise DDictCheckpointSyncError(resp_msg.err, resp_msg.errInfo)

        elif resp_msg.err != DragonError.SUCCESS:
            raise DDictError(
                resp_msg.err,
                "Failed to store key in the distributed dictionary.\nAdditional Information: %s" % resp_msg.errInfo,
            )

    def _batch_put(self, key: object, value: object, persist: bool):
        # hash the key and get the target manager ID
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._check_manager_connection(manager_id)
        try:
            # If there is not send handle for the manager yet, we create another stream channel
            # and open the send handle. Otherwise use the existing send handle.
            if manager_id in self._opened_send_handles:
                sendh = self._opened_send_handles[manager_id]
            else:
                strm = Channel.make_process_local()
                # Keep track of stream channels and send handle for cleanup in the end of batch put.
                self._batch_put_stream_channels[manager_id] = strm
                sendh = self._managers[manager_id].sendh(stream_channel=strm, timeout=self._timeout)
                self._opened_send_handles[manager_id] = sendh
                # Send DDBatchPutMsg first to notify the manager that the following puts are batch puts.
                msg = dmsg.DDBatchPut(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, persist=persist)
                self._batch_put_msg_tags.add(msg.tag)
                self._traceit("Sending to manager: %s", msg)
                sendh.send_bytes(msg.serialize(), timeout=self._timeout)

            self._traceit("Sending pickled key to manager: %s", key)
            sendh.send_bytes(pickled_key, arg=KEY_HINT, timeout=self._timeout)
            cloudpickle.dump(
                value,
                file=PickleWriteAdapter(sendh=sendh, hint=VALUE_HINT, timeout=self._timeout),
                protocol=pickle.HIGHEST_PROTOCOL,
            )
            # Keep track of the number of batch put to the manager
            if manager_id not in self._num_batch_puts:
                self._num_batch_puts[manager_id] = 0
            self._num_batch_puts[manager_id] += 1

        except TimeoutError as ex:
            raise DDictTimeoutError(
                DragonError.TIMEOUT,
                f"The operation timed out. This could be a network failure or an out of memory condition.\n{str(ex)}",
            )

    def destroy(self, allow_restart=False) -> None:
        """

        Destroy a Distributed Dictionary instance, freeing all the resources that
        were allocated when it was created. Any clients that are still
        attached to the dictionary and try to do an operation on it will
        experience an exception if attempting subsequent operations.

        """
        if self._destroyed:
            return

        self._traceit("Destroying the ddict.")

        self._destroyed = True
        try:
            msg = dmsg.DDDestroy(
                self._tag_inc(),
                self._client_id,
                respFLI=self._serialized_buffered_return_connector,
                allowRestart=allow_restart,
            )
            resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector, buffered=True)
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug("There was an exception in the destroy: %s\n Traceback: %s", ex, tb)
            except:
                pass

        try:
            self._orc_connector.detach()
        except Exception as ex:
            try:
                tb = traceback.format_exc()
                log.debug("There was an exception while detaching orchestrator channel: %s \n Traceback: %s", ex, tb)
            except:
                pass

        # join on the orchestrator proc
        if self._creator:
            try:
                self._orc_proc.wait()
            except Exception as ex:
                tb = traceback.format_exc()
                try:
                    log.debug("There was an exception while joining orchestrator process: %s \n Traceback: %s", ex, tb)
                except:
                    pass

        try:
            self._free_process_local_channels()
        except:
            pass

    def serialize(self) -> str:
        """

        Returns a serialized, base64 encoded descriptor (i.e. string) that may be
        shared with other processes for attaching. This is especially
        useful when sharing with C or C++ code. Within Python you can
        pass the Distributed Dictionary to another process and it will be
        automatically serialized and attached so using this method is not
        needed when passing to another Python process.

        :returns: A serialized, base64 encoded string that may be used for
            attaching to the dictionary.

        """
        return self._serialized_orc

    @classmethod
    def attach(cls, serialized_dict: str, *, timeout: float = None, trace: bool = False) -> DDict:
        """

        Within Python you typically do not need to call this method explicitly.
        It will be done automatically when you pass a Distributed
        Dictionary from one process to another. However, you can do this
        explicitly if desired/needed.

        :param serialized_dict: A serialized distributed dictionary.

        :param timeout: None or a float or int value. A value of None means to
            wait forever. Otherwise it is the number of seconds to wait while
            an operation is performed. This timeout is applied to all
            subsequent client operations that are performed by the process
            that is attaching this DDict.

        :param trace: If True, specifies that all operations on the distributed
            dictionary should be logged in detail within the client log.

        :returns: An attached serialized dictionary.

        :raises TimeoutError: If the timeout expires.

        :raises Exception: Other exceptions are possible if for instance the
            serialized dictionary no longer exists.

        """
        new_client = cls.__new__(cls)
        new_client._init_props((False, serialized_dict, 0, timeout, trace, None))
        return new_client

    def detach(self) -> None:
        """

        Detach from the Distributed Dictionary and free all local resources of
        this client. But leave in place the DDict for other clients and
        processes.

        """

        try:
            if self._destroyed or self._detached or not self._creator:
                return

            self._traceit("Detaching from ddict.")

            self._detached = True

            for manager_id in self._managers:
                try:
                    msg = dmsg.DDDeregisterClient(
                        self._tag_inc(), clientID=self._client_id, respFLI=self._serialized_buffered_return_connector
                    )
                    resp_msg = self._send_receive([(msg, None)], connection=self._managers[manager_id], buffered=True)
                    if resp_msg.err != DragonError.SUCCESS:
                        log.debug("Error on response to deregister client %s", self._client_id)

                    self._managers[manager_id].detach()
                except:
                    pass

        except Exception as ex:
            try:
                tb = traceback.format_exc()
                log.debug(
                    "There was an exception while detaching the client %s. Exception: %s\n Traceback: %s",
                    self._client_id,
                    ex,
                    tb,
                )
            except:
                pass

    @classmethod
    def synchronize_ddicts(cls, serialized_ddicts: list[str]) -> None:
        """

        Synchronize managers across all parallel dictionaries. This is useful
        when you have two or more identical instances of a DDict and are
        using one to recover other instances. This method will look for
        any empty managers in the list of serialized dictionaries and
        fill them with their parallel counterpart from another non-empty
        dictionary manager.

        :param serialized_ddicts: A list of serialized DDicts to synchronize.

        """

        if len(serialized_ddicts) == 0:
            raise ValueError("The list of serialized ddicts must not be empty.")

        for ser_ddict in serialized_ddicts:
            if not isinstance(ser_ddict, str):
                raise DDictError(DragonError.INVALID_ARGUMENT, "The serialized dictionary must be a string.")

        new_client = cls.__new__(cls)
        new_client.__setstate__((False, serialized_ddicts[0], 0, DDICT_DEFAULT_TIMEOUT, False, None))

        tags = set()
        # send request to every ddict orchestrator
        for ser_ddict in serialized_ddicts:
            current_tag = new_client._tag_inc()
            tags.add(current_tag)
            msg = dmsg.DDGetManagers(current_tag, respFLI=new_client._serialized_buffered_return_connector)
            connection = fli.FLInterface.attach(b64decode(ser_ddict))
            new_client._send([(msg, None)], connection, buffered=True)
            connection.detach()

        # receive responses from all orchestrators
        resp_num = len(serialized_ddicts)
        msglist = new_client._recv_responses(tags, resp_num)
        # check each response
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)

        # build empty and full manager list
        num_managers = len(msglist[0].managers)
        # Check managers with the same manager_id from all parallel dictionaries
        for manager_id in range(num_managers):
            empty_managers = []
            full_managers = []
            for resp_msg in msglist:
                # If the manager is empty, we append its fli to the empty_manager list.
                # Otherwise append its fli to full_managers.
                if resp_msg.emptyManagers[manager_id]:
                    empty_managers.append(resp_msg.managers[manager_id])
                else:
                    full_managers.append(resp_msg.managers[manager_id])

            if len(empty_managers) > 0:  # Sync manager only when there's any empty manager
                if len(full_managers) == 0:
                    raise RuntimeError(f"Failed to synchronize dictionary. No full manager for manager {manager_id}.")

                if len(full_managers) < len(empty_managers):
                    # guarantees the length of full managers is equal to or longer then empty managers.
                    full_managers = full_managers * int(1 + len(empty_managers) / len(full_managers))

                # Send sync request along with empty managers fli to full managers so that full managers can send the data to reconstruct
                # empty managers directly
                tags = set()
                for i in range(len(empty_managers)):
                    empty_fli = empty_managers[i]
                    full_fli = full_managers[i]
                    current_tag = new_client._tag_inc()
                    tags.add(current_tag)
                    msg = dmsg.DDManagerSync(
                        current_tag, emptyManagerFLI=empty_fli, respFLI=new_client._serialized_buffered_return_connector
                    )
                    connection = fli.FLInterface.attach(b64decode(full_fli))
                    new_client._send([(msg, None)], connection, buffered=True)
                    connection.detach()

                # Receive sync responses from all full managers
                resp_num = len(empty_managers)
                manager_sync_msglist = new_client._recv_responses(tags, resp_num)
                for resp_msg in manager_sync_msglist:
                    if resp_msg.err != DragonError.SUCCESS:
                        raise DDictSyncError(resp_msg.err, f"Failed to synchronize dictionary, {resp_msg.errInfo}")

        # unmark all empty managers after recovered successfully
        tags = set()
        managerIDs = [id for id in range(new_client._num_managers)]
        for ser_ddict in serialized_ddicts:
            current_tag = new_client._tag_inc()
            tags.add(current_tag)
            msg = dmsg.DDUnmarkDrainedManagers(
                current_tag, respFLI=new_client._serialized_buffered_return_connector, managerIDs=managerIDs
            )
            connection = fli.FLInterface.attach(b64decode(ser_ddict))
            new_client._send([(msg, None)], connection, buffered=True)
            connection.detach()

        # receive responses from all orchestrators
        resp_num = len(serialized_ddicts)
        msglist = new_client._recv_responses(tags, resp_num)
        # check each response
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)

        new_client.detach()

    def clone(self, clone_list: list[str]) -> None:
        """

        Clone the current dictionary to the list of provided serialized
        dictionaries.

        :param clone_list: A list of serialized DDicts which will then be clones
        of this DDict.

        """
        if len(clone_list) == 0:
            raise ValueError("The list of serialized ddicts must not be empty.")

        for ser_ddict in clone_list:
            if not isinstance(ser_ddict, str):
                raise DDictError(DragonError.INVALID_ARGUMENT, "The serialized dictionary must be a string.")

        # check if ddicts in clone_list has the same number of managers
        ddict_connections = []
        tags = set()
        for ser_ddict in clone_list:
            current_tag = self._tag_inc()
            tags.add(current_tag)
            msg = dmsg.DDGetMetaData(current_tag, respFLI=self._serialized_buffered_return_connector)
            connection = fli.FLInterface.attach(b64decode(ser_ddict))
            ddict_connections.append(connection)
            self._send([(msg, None)], connection, buffered=True)

        # receive responses from all orchestrator and check that the meta data match
        resp_num = len(clone_list)
        msglist = self._recv_responses(tags, resp_num)
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)

            if resp_msg.numManagers != self._num_managers:
                raise RuntimeError("Metadata mismatches. Could not clone dictionary with different metadata.")

        # marks all managers as empty managers for all dictionary
        tags = set()
        managerIDs = [i for i in range(self._num_managers)]
        for ddict_connection in ddict_connections:
            current_tag = self._tag_inc()
            tags.add(current_tag)
            msg = dmsg.DDMarkDrainedManagers(
                current_tag, respFLI=self._serialized_buffered_return_connector, managerIDs=managerIDs
            )
            self._send([(msg, None)], ddict_connection, buffered=True)
            ddict_connection.detach()

        msglist = self._recv_responses(tags, len(ddict_connections))
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)

        del ddict_connections

        # Only current dictionary has full managers. It will be used to reconstruct empty managers from other dictionaries.
        ddicts = [self._serialized_orc]
        ddicts.extend(clone_list)
        # synchronize dictionaries
        self.synchronize_ddicts(ddicts)

    def manager(self, id: int) -> DDict:
        """

        Return a version of the current DDict that will always choose the given
        manager for storing and retrieving data. This is only useful when
        storing and/or retrieving data locally. If you need data to be
        globally available then you should only store data that would be
        globally stored there anyway. One way to accomplish this is to
        store data globally, but then work on locally stored keys. You
        can discover "local_keys" of a manager by calling getting a
        manager-directed handle to the DDict and iterating over its keys.

        :param id: The manager id of the chosen manager.

        :returns: A version of the same DDict which will direct all gets
            and puts to the specified manager.

        :raises Exception: If the manager id is not a valid id.

        """

        if id < 0 or id >= self._num_managers:
            raise DDictError(DragonError.INVALID_ARGUMENT, f"The value {id} is not a valid manager id.")
        mgr_dd = copy.copy(self)
        mgr_dd._chosen_manager = id
        mgr_dd._creator = False
        mgr_dd._key_pickler = self._key_pickler
        mgr_dd._value_pickler = self._value_pickler

        return mgr_dd



    def pickler(self, key_pickler=None, value_pickler=None) -> DDict:
        """

        Create a copy of the DDict which will utilize a specific key and value
        pickler.

        :param key_pickler: A pickler to de/serialize keys. Defaults to None.

        :param value_pickler: A pickler to de/serialize values. Defaults to None.

        :returns: The same DDict with the desired pickling attributes.

        """
        pickler_dd = copy.copy(self)
        pickler_dd._key_pickler = key_pickler
        pickler_dd._value_pickler = value_pickler
        pickler_dd._chosen_manager = self._chosen_manager
        pickler_dd._creator = False

        return pickler_dd

    def which_manager(self, key: object) -> int:
        """

        Return the manager id of the manager to which this key would be sent on a
        put/store operation. This can be useful when wanting to minimize
        the movement of data.

        :param key: A key that might be stored at some future time. It must be serializable.

        :returns: The manager id of the manager where this key would be stored.

        """
        manager_id, _ = self._choose_manager_pickle_key(key)
        return manager_id


    def __setitem__(self, key: object, value: object) -> None:
        """

        Store the key/value pair in the current checkpoint within the Distributed
        Dictionary. Due to the nature of a parallel, distributed
        dictionary, insertion order into the distributed dictionary is
        not maintained.

        :param key: The key of the pair. It must be serializable.

        :param value: the value of the pair. It also must be serializable.

        :raises Exception: Various exceptions can be raised including TimeoutError.

        """
        if self._batch_put_started:
            if self._batch_persist:
                raise DDictError(
                    DragonError.INVALID_OPERATION,
                    "Persistent value mismatch. Could not perform non-persistent put during batch put.",
                )
            self._batch_put(key, value, False)
        else:
            msg = dmsg.DDPut(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, persist=False)
            self._put(msg, key, value)

    def __getitem__(self, key: object) -> object:
        """

        Get the value that is associated with the given key.

        :param key: The key of a stored key/value pair.

        :returns: The value associated with the key.

        :raises Exception: Various exceptions can be raised including TimeoutError and KeyError.

        """
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        msg = dmsg.DDGet(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, key=pickled_key)
        self._check_manager_connection(manager_id)
        self._send([(msg, None)], self._managers[manager_id], buffered=True)
        manager_not_local = manager_id not in self._local_managers
        value = self._recv_dmsg_and_val(msg, key, manager_not_local)

        return value

    def __contains__(self, key: object) -> bool:
        """

        Returns True if key is in the Distributed Dictionary and False otherwise.

        :param key: A possible key stored in the DDict.

        :returns bool: True or False depending on if the key is there or not.

        :raises: Various exceptions can be raised including TimeoutError.

        """
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        tag = self._tag_inc()
        msg = dmsg.DDContains(tag, self._client_id, chkptID=self._chkpt_id, key=pickled_key)
        self._check_manager_connection(manager_id)
        resp_msg = self._send_receive([(msg, None)], self._managers[manager_id], buffered=True)

        if resp_msg.err == DragonError.SUCCESS:
            return True
        elif resp_msg.err == DragonError.KEY_NOT_FOUND:
            return False
        elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
            raise DDictCheckpointSyncError(resp_msg.err, resp_msg.errInfo)

        raise DDictError(resp_msg.err, resp_msg.errInfo)

    def __iter__(self) -> Iterator[DDict]:
        """

        Return an iterator over the keys of the DDict. A key iterator is
        evaluated lazily and efficiently to allow for very large key sets
        to be iterated.

        :returns: An iterator over the keys of the DDict.

        """
        return iter(self.keys())

    def __len__(self) -> int:
        """

        Returns the number of keys stored in the entire Distributed Dictionary or
        just from the selected manager if this DDict Client was directed
        to a specific manager by calling the manager method.

        :returns: The number of stored keys in the current checkpoint plus
            any persistent keys.

        :raises: Various exceptions can be raised including TimeoutError.

        """
        self._traceit("Getting length from ddict")
        tag = self._tag_inc()
        broadcast = self._chosen_manager is None

        if broadcast:
            selected_manager = 0
            resp_num = self._num_managers
        else:
            selected_manager = self._chosen_manager
            resp_num = 1

        self._check_manager_connection(selected_manager)
        msg = dmsg.DDLength(
            tag,
            clientID=self._client_id,
            chkptID=self._chkpt_id,
            respFLI=self._serialized_buffered_return_connector,
            broadcast=broadcast,
        )
        self._send([(msg, None)], self._managers[selected_manager], buffered=True)
        msglist = self._recv_responses(set([tag]), resp_num)
        length = 0
        for resp_msg in msglist:
            if resp_msg.err == DragonError.SUCCESS:
                length += resp_msg.length
            elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                raise DDictCheckpointSyncError(resp_msg.err, resp_msg.errInfo)
            else:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
        return length

    def __delitem__(self, key: object) -> None:
        """

        Deletes a key/value pair from the Distributed Dictionary if it exists.

        :raises: Various exceptions can be raised including TimeoutError and
            KeyError.

        """
        self.pop(key)

    def _mark_as_drained(self, manager_id) -> None:
        """
        Notify orchestrator to mark the manager as drained for internal cross
        ddict synchronization testing.
        """
        msg = dmsg.DDMarkDrainedManagers(
            self._tag_inc(), respFLI=self._serialized_buffered_return_connector, managerIDs=[manager_id]
        )
        resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector, buffered=True)
        if resp_msg.err != DragonError.SUCCESS:
            raise DDictError(resp_msg.err, f"Failed to mark manager {manager_id} as drained! {resp_msg.errInfo}")

    def pput(self, key: object, value: object) -> None:
        """

        Persistently store a key/value pair within the Distributed Dictionary.
        This is useful when checkpointing is employed in the dictionary.
        A persistent put of a key/value pair means that the key/value
        pair persists across checkpoints. Persistent key/value pairs are
        useful when putting constant values or other values that don't
        change across checkpoints.

        :param key: A serializable object that will be stored as the key in the
            DDict.

        :param value: A serializable object that will be stored as the value.

        """
        if self._batch_put_started:
            if not self._batch_persist:
                raise DDictError(
                    DragonError.INVALID_OPERATION,
                    "Persistent value mismatch. Could not perform persistent put during batch put.",
                )
            self._batch_put(key, value, True)
        else:
            msg = dmsg.DDPut(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, persist=True)
            self._put(msg, key, value)

    def bput(self, key: object, value: object) -> None:
        """

        Store a non-persistent key/value pair by brodcasting to all managers
        across the Distributed Dictionary. This is useful when multiple
        clients requesting the same key. This should be used carefully as
        each manager holds a duplicate of the key/value pair, but it can
        be useful when a key/value pair is needed across all the nodes of
        the allocation by all worker processes. The broadcast put
        distributes the key/value pair in a tree fashion to maximize
        store performance and the corresponding bget operation can then
        retrieve it locally when the bgetting process is colocated with a
        manager and otherwise will get it from its randomly assigned main
        manager.

        :param key: A serializable object that will be stored as the key in the DDict.

        :param value: A serializable object that will be stored as the value.

        """
        _, pickled_key = self._choose_manager_pickle_key(key)

        if self._batch_put_started:
            # bput with batch
            if self._batch_persist:
                raise DDictError(
                    DragonError.INVALID_OPERATION,
                    "Persistent value mismatch. Could not perform broadcast put during batch put as bput only writes non-persistent key.",
                )
            try:
                # First call to bput in current batch, select a random root manager and create a new send
                # handle for it. A response channel is created to receive bput response during cleanup of
                # the batch.
                if self._bput_root_manager_sendh is None:
                    # This is the first call to bput in current batch, need to select a random root manager.
                    # For bput without batch put, a random root manager is selected every time.
                    managers = [i for i in range(self._num_managers)]
                    random.shuffle(managers)
                    # Assign the random root manager to the root manager of the bcast put with current batch.
                    root_manager = managers[0]
                    # connect the root manager
                    self._check_manager_connection(root_manager)
                    # create new send handle for the root manager
                    self._bput_strm = Channel.make_process_local()
                    self._bput_root_manager_sendh = self._managers[root_manager].sendh(
                        stream_channel=self._bput_strm, timeout=self._timeout
                    )

                    # There might be other response expected in the main response channel, so create a new
                    # response FLI that only receives the bput response for current batch. In the cleanup
                    # of the broadcast put with batch, we will receive bput responses from every manager
                    # through this response FLI.
                    self._bput_resp_strm = Channel.make_process_local()
                    self._traceit(f"The local channel cuid is {self._bput_resp_strm.cuid}")
                    self._bput_respFLI = fli.FLInterface(main_ch=self._bput_resp_strm, use_buffered_protocol=True)
                    serializedRespFLI = b64encode(self._bput_respFLI.serialize())
                    self._bput_tag = self._tag_inc()
                    msg = dmsg.DDBPut(
                        self._bput_tag, self._client_id, self._chkpt_id, serializedRespFLI, managers, True
                    )

                    self._traceit("Sending to manager: %s", msg)
                    self._bput_root_manager_sendh.send_bytes(msg.serialize(), timeout=self._timeout)

                # send key and value
                self._traceit("Sending pickled key to manager: %s", key)
                self._bput_root_manager_sendh.send_bytes(pickled_key, arg=KEY_HINT, timeout=self._timeout)
                cloudpickle.dump(
                    value,
                    file=PickleWriteAdapter(
                        sendh=self._bput_root_manager_sendh, hint=VALUE_HINT, timeout=self._timeout
                    ),
                    protocol=pickle.HIGHEST_PROTOCOL,
                )
                self._num_bputs += 1

            except TimeoutError as ex:
                raise DDictTimeoutError(
                    DragonError.TIMEOUT,
                    f"The operation timed out. This could be a network failure or an out of memory condition.\n{str(ex)}",
                )

            except Exception as ex:
                tb = traceback.format_exc()
                try:
                    log.debug(
                        "There was an exception performing broadcast put with batch put: %s\nTraceback:: %s", ex, tb
                    )
                except:
                    pass
                raise
        else:
            # For bput without batch put, a random root manager is selected every time.
            managers = [i for i in range(self._num_managers)]
            random.shuffle(managers)

            tag = self._tag_inc()
            msg = dmsg.DDBPut(
                tag, self._client_id, self._chkpt_id, self._serialized_buffered_return_connector, managers, False
            )
            self._send_dmsg_and_key_value(managers[0], msg, pickled_key, key, value)
            resp_num = self._num_managers
            resp_msgs = self._recv_responses_and_check_err(set([tag]), resp_num)
            for resp_msg in resp_msgs:
                if resp_msg.numPuts != 1:
                    raise DDictError(
                        DragonError.FAILURE,
                        f"Failed to store all keys in manager {resp_msg.managerID} in the distributed dictionary. Expected number of keys to be written: {self._num_bputs}, number of successful writes: {resp_msg.numPuts}",
                    )

    def _end_bput_with_batch(self):
        self._bput_root_manager_sendh.close()
        self._bput_root_manager_sendh = None

        # receive response
        resp_num = self._num_managers
        resp_msgs = self._recv_responses_and_check_err(set([self._bput_tag]), resp_num, self._bput_respFLI)
        for resp_msg in resp_msgs:
            if resp_msg.numPuts != self._num_bputs:
                raise DDictError(
                    DragonError.FAILURE,
                    f"Failed to store all keys in manager {resp_msg.managerID} in the distributed dictionary. Expected number of keys to be written: {self._num_bputs}, number of successful writes: {resp_msg.numPuts}",
                )

        # cleanup
        self._bput_tag = None
        self._num_bputs = 0

        try:
            self._bput_respFLI.destroy()
            self._traceit(f"Local channel cuid to be destroyed is {self._bput_resp_strm.cuid}")
            self._bput_resp_strm.destroy_process_local()
            self._traceit(f"Local channel cuid to be destroyed is {self._bput_strm.cuid}")
            self._bput_strm.destroy_process_local()
        except:
            pass

        self._bput_respFLI = None
        self._bput_resp_strm = None
        self._bput_strm = None

    def bget(self, key: object) -> object:
        """

        Read the key written through bput. Each manager has a copy of the key,
        the client should be able to request the key from its main
        manager. Clients request the key from the chosen manager if one
        has been set. Otherwise the client requests the key from its main
        manager.

        :param key: The key of a stored key/value pair.

        :returns: The value associated with the key.

        :raises Exception: Various exceptions can be raised including
            TimeoutError and KeyError.

        """
        _, pickled_key = self._choose_manager_pickle_key(key)
        msg = dmsg.DDGet(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, key=pickled_key)
        if self._chosen_manager is not None:
            self._check_manager_connection(self._chosen_manager)
            manager = self._managers[self._chosen_manager]
            local_manager = self._chosen_manager in self._local_managers
        else:
            manager = self._main_manager_connection
            local_manager = self._has_local_manager

        self._send([(msg, None)], manager, buffered=True)

        manager_not_local = not local_manager  # TBD: OR DDGetResponse has free_mem in it (from manager)

        value = self._recv_dmsg_and_val(msg, key, manager_not_local)
        return value

    def _keys(self, managers: set[int]) -> Iterator[DDict]:
        try:
            # Since there could be multiple iterators in the same process over a DDict,
            # each iterator gets its own response channel/stream for keys to be streamed to it.
            # The main response FLI is not used because there may be other operations a user
            # wishes to do to interact with the DDict while iterating over it.
            iter_strm = Channel.make_process_local()
            self._traceit(f"The local channel cuid is {iter_strm.cuid}")
            respFLI = fli.FLInterface(main_ch=iter_strm)
            serializedRespFLI = b64encode(respFLI.serialize())

            # Shuffling the managers list helps if many clients all call keys at the same time.
            # It helps to distribute the communication to managers more evenly.
            manager_list = list(managers)
            random.shuffle(manager_list)
            managers = set(manager_list)

            for manager_id in managers:
                msg = dmsg.DDKeys(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, respFLI=serializedRespFLI)
                self._send([(msg, None)], self._managers[manager_id], buffered=True)
                self._traceit("About to open recv handle to retrieve keys from %s", manager_id)
                with respFLI.recvh(use_main_as_stream_channel=True, timeout=self._timeout) as recvh:
                    # Any data that is left in stream channel by an early break in the iterator is
                    # automatically discarded by the FLI close of the receive handle.
                    resp_ser_msg, _ = recvh.recv_bytes(timeout=self._timeout)
                    self._traceit("Got keys from %s", manager_id)
                    resp_msg = dmsg.parse(resp_ser_msg)
                    if resp_msg.ref == msg.tag:
                        if resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                            raise DDictError(resp_msg.err, resp_msg.errInfo)
                        elif resp_msg.err != DragonError.SUCCESS:
                            raise DDictError(resp_msg.err, "Failed to get key list in the distributed dictionary.")
                        done = False
                        while not done:
                            try:
                                key_bytes, _ = recvh.recv_bytes(timeout=self._timeout)
                                if self._key_pickler is None:
                                    key = cloudpickle.loads(key_bytes)
                                else:
                                    key = self._key_pickler.loads(key_bytes)
                                yield key
                            except EOFError:
                                done = True
                    # If the tag did not match, exiting the context manager will flush the rest of the response.

            if self._chosen_manager is not None:
                self._traceit("Keys was called on a manager directed ddict")
            else:
                self._traceit("Keys was called on all managers")
        except Exception as ex:
            tb = traceback.format_exc()
            log.debug("There was an exception iterating on the DDict keys: %s\nTraceback: %s", ex, tb)
            raise
        finally:
            try:
                respFLI.destroy()
                self._traceit(f"Local channel cuid to be destroyed is {iter_strm.cuid}")
                iter_strm.destroy_process_local()
            except:
                pass

    def _values(self, managers: set[int]) -> Iterator[DDict]:
        try:
            iter_strm = Channel.make_process_local()
            self._traceit(f"The local channel cuid is {iter_strm.cuid}")
            respFLI = fli.FLInterface(main_ch=iter_strm)
            serializedRespFLI = b64encode(respFLI.serialize())

            manager_list = list(managers)
            random.shuffle(manager_list)
            managers = set(manager_list)

            for manager_id in managers:
                local_manager = manager_id in self._local_managers
                msg = dmsg.DDValues(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, respFLI=serializedRespFLI)
                self._send([(msg, None)], self._managers[manager_id], buffered=True)
                self._traceit("About to open recv handle to retrieve values from %s", manager_id)
                with respFLI.recvh(use_main_as_stream_channel=True, timeout=self._timeout) as recvh:
                    resp_ser_msg, _ = recvh.recv_bytes(timeout=self._timeout)
                    self._traceit("Got values from %s", manager_id)
                    resp_msg = dmsg.parse(resp_ser_msg)
                    if resp_msg.ref == msg.tag:
                        if resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                            raise DDictError(resp_msg.err, resp_msg.errInfo)
                        elif resp_msg.err != DragonError.SUCCESS:
                            raise DDictError(resp_msg.err, "Failed to get value list in the distributed dictionary. ")
                        done = False
                        free_mem = not local_manager or resp_msg.freeMem
                        while not done:
                            try:
                                if self._value_pickler is None:
                                    value = cloudpickle.load(
                                        file=PickleReadAdapter(
                                            recvh=recvh, hint=VALUE_HINT, free_mem=free_mem, timeout=self._timeout)
                                    )
                                else:
                                    value = self._value_pickler.load(
                                        file=PickleReadAdapter(
                                            recvh=recvh, hint=VALUE_HINT, free_mem=free_mem, timeout=self._timeout)
                                    )
                                yield value
                            except EOFError:
                                done = True
                            except Exception as e:
                                tb = traceback.format_exc()
                                try:
                                    log.info("Exception caught in cloudpickle load: %s \n Traceback: %s", e, tb)
                                except:
                                    pass
                                raise RuntimeError(f"Exception caught in cloudpickle load: {e} \n Traceback: {tb}")

            if self._chosen_manager is not None:
                self._traceit("Values was called on a manager directed ddict")
            else:
                self._traceit("Values was called on all managers")
        except Exception as ex:
            tb = traceback.format_exc()
            log.debug("There was an exception iterating on the DDict values: %s\nTraceback: %s", ex, tb)
            raise
        finally:
            try:
                respFLI.destroy()
                self._traceit(f"Local channel cuid to be destroyed is {iter_strm.cuid}")
                iter_strm.destroy_process_local()
            except:
                pass

    def _items(self, managers: set[int]) -> Iterator[DDict]:
        try:
            iter_strm = Channel.make_process_local()
            self._traceit(f"The local channel cuid is {iter_strm.cuid}")
            respFLI = fli.FLInterface(main_ch=iter_strm)
            serializedRespFLI = b64encode(respFLI.serialize())

            manager_list = list(managers)
            random.shuffle(manager_list)
            managers = set(manager_list)

            for manager_id in managers:
                local_manager = manager_id in self._local_managers
                msg = dmsg.DDItems(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, respFLI=serializedRespFLI)
                self._send([(msg, None)], self._managers[manager_id], buffered=True)
                self._traceit("About to open recv handle to retrieve items from %s", manager_id)
                with respFLI.recvh(use_main_as_stream_channel=True, timeout=self._timeout) as recvh:
                    resp_ser_msg, _ = recvh.recv_bytes(timeout=self._timeout)
                    self._traceit("Got items from %s", manager_id)
                    resp_msg = dmsg.parse(resp_ser_msg)
                    if resp_msg.ref == msg.tag:
                        if resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                            raise DDictError(resp_msg.err, resp_msg.errInfo)
                        elif resp_msg.err != DragonError.SUCCESS:
                            raise DDictError(resp_msg.err, "Failed to get item list in the distributed dictionary. ")
                        done = False
                        free_mem = not local_manager or resp_msg.freeMem
                        while not done:
                            try:
                                # receive key
                                key_bytes, hint = recvh.recv_bytes(timeout=self._timeout)
                                assert hint == KEY_HINT
                                if self._key_pickler is None:
                                    key = cloudpickle.loads(key_bytes)
                                else:
                                    key = self._key_pickler.loads(key_bytes)
                            except EOFError:
                                done = True
                                break  # No more stuff to receive, exit while loop immediately.
                            except Exception as ex:
                                tb = traceback.format_exc()
                                try:
                                    log.info("Exception caught while loading key: %s \n Traceback: %s", e, tb)
                                except:
                                    pass
                                raise RuntimeError(f"Exception caught while loading key: {e} \n Traceback: {tb}")

                            try:
                                # receive value
                                if self._value_pickler is None:
                                    value = cloudpickle.load(
                                        file=PickleReadAdapter(
                                            recvh=recvh, hint=VALUE_HINT, free_mem=free_mem, timeout=self._timeout)
                                    )
                                else:
                                    value = self._value_pickler.load(
                                        file=PickleReadAdapter(
                                            recvh=recvh, hint=VALUE_HINT, free_mem=free_mem, timeout=self._timeout)
                                    )
                                yield (key, value)
                            except Exception as e:
                                tb = traceback.format_exc()
                                try:
                                    log.info("Exception caught while loading value: %s \n Traceback: %s", e, tb)
                                except:
                                    pass
                                raise RuntimeError(f"Exception caught while loading value: {e} \n Traceback: {tb}")

            if self._chosen_manager is not None:
                self._traceit("Items was called on a manager directed ddict")
            else:
                self._traceit("Items was called on all managers")
        except Exception as ex:
            tb = traceback.format_exc()
            log.debug("There was an exception iterating on the DDict items: %s\nTraceback: %s", ex, tb)
            raise
        finally:
            try:
                respFLI.destroy()
                self._traceit(f"Local channel cuid to be destroyed is {iter_strm.cuid}")
                iter_strm.destroy_process_local()
            except:
                pass

    def _chkpt_avail(self, chkptID: int):
        """

        Check the availability of the checkpoint in the DDict.

        """
        tag = self._tag_inc()
        msg = dmsg.DDChkptAvail(
            tag, chkptID=chkptID, respFLI=self._serialized_buffered_return_connector,
        )
        self._check_manager_connection(0)
        self._send([(msg, None)], self._managers[0], buffered=True)

        msglist = self._recv_responses(set([tag]), num_responses=self._num_managers)
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
            elif not resp_msg.available:
                raise DDictError(
                    DragonError.INVALID_OPERATION,
                    f"Unable to access checkpoint {chkptID} from manager {resp_msg.managerID}. The checkpoint is not available",
                )

    def _persisted_chkpt_avail(self, chkptID: int):
        """

        Check the availability of the persisted checkpoint.

        """
        tag = self._tag_inc()
        msg = dmsg.DDPersistedChkptAvail(
            tag, chkptID=chkptID, respFLI=self._serialized_buffered_return_connector,
        )
        self._check_manager_connection(0)
        self._send([(msg, None)], self._managers[0], buffered=True)

        msglist = self._recv_responses(set([tag]), num_responses=self._num_managers)
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
            elif not resp_msg.available:
                raise DDictError(
                    DragonError.INVALID_OPERATION,
                    f"Unable to access persisted checkpoint {chkptID} from manager {resp_msg.managerID}. The checkpoint is not available",
                )

    def _restore(self, chkpt: int):
        # Checkpoint is available across all managers, proceed to restore it.
        tag = self._tag_inc()
        msg = dmsg.DDRestore(
            tag, chkptID=chkpt, clientID=self._client_id, respFLI=self._serialized_buffered_return_connector
        )
        self._check_manager_connection(0)
        self._send([(msg, None)], self._managers[0], buffered=True)

        msglist = self._recv_responses(set([tag]), num_responses=self._num_managers)
        for resp_msg in msglist:
            if resp_msg.err == DragonError.DDICT_PERSIST_CHECKPOINT_UNAVAILABLE:
                raise DDictPersistCheckpointError(resp_msg.err, resp_msg.errInfo)
            elif resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)

        # After restoring complete, every client should sync to newest checkpoint ID
        self._chkpt_id = chkpt

    def local_len(self) -> int:
        """

        Return the number of keys that are stored on managers that are colocated
        with this client.

        :returns: The number of keys stored on this node of the Dragon run-time.

        :raises DDictCheckpointSyncError: If the checkpoint the client is at has been retired.

        :raises RuntimeError: Other errors are possible including TimeoutError.

        """
        tags = set()
        for i in self._local_managers:
            tag = self._tag_inc()
            tags.add(tag)
            msg = dmsg.DDLength(
                tag, clientID=self._client_id, respFLI=self._serialized_buffered_return_connector, broadcast=False
            )
            self._send([(msg, None)], self._managers[i], buffered=True)
        resp_num = len(self._local_managers)
        msglist = self._recv_responses(tags, resp_num)
        length = 0
        for resp_msg in msglist:
            if resp_msg.err == DragonError.SUCCESS:
                length += resp_msg.length
            elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                raise DDictCheckpointSyncError(resp_msg.err, resp_msg.errInfo)
            else:
                raise RuntimeError(resp_msg.err)
        return length

    def local_keys(self) -> DDictKeysView:
        """

        Returns a DDictKeysView of the keys that are local to the process
        invoking this method. This is useful when a local process wants
        to work with data stored locally that will be transformed and
        then later requested by other processes globally.

        :returns: A DDictKeysView of the current DDict which has only the
            co-located node local keys of the DDict in it.

        """
        # connect to all local managers
        for i in self._local_managers:
            self._check_manager_connection(i)

        return DDictKeysView(self, self._local_managers)

    def keys(self) -> DDictKeysView:
        """

        Returns a keys view of the distributed dictionary. From this view you can
        iterate over the keys or get the number of keys (i.e. length
        operation). See dict view objects for the methods available on a
        ddict keys view. The keys view returned here provides an
        efficient implementation of various dict keys view operations.

        :returns: A DDictKeysView object which is a live view of the DDict.

        """

        if self._chosen_manager is not None:
            self._check_manager_connection(self._chosen_manager)
            managers = [self._chosen_manager]
        else:
            self._check_manager_connection(all=True)
            managers = range(self._num_managers)

        return DDictKeysView(self, managers)

    def pop(self, key: object, default: object = None) -> object:
        """

        Pop the given key and its value from the distributed dictionary and
        return the associated value. If the given key is not found in the
        dictionary, then KeyError is raised unless a default value is
        provided, in which case the default value is returned if the key
        is not found in the dictionary.

        :param key: A key to be popped from the distributed dictionary.

        :param default: A default value to be returned if the key is not in the
            distributed dictionary.

        :returns: The associated value if key is popped and the default value otherwise.

        """

        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        msg = dmsg.DDPop(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, key=pickled_key)
        self._check_manager_connection(manager_id)
        self._send([(msg, None)], self._managers[manager_id], buffered=True)
        try:
            manager_not_local = manager_id not in self._local_managers
            return self._recv_dmsg_and_val(msg, key, manager_not_local)
        except KeyError as ex:
            if default is None:
                raise ex

            return default

    def clear(self) -> None:
        """

        Empty the distributed dictionary of all keys and values.

        """
        self._traceit("clearing dictionary for checkpoint %s", self._chkpt_id)
        tag = self._tag_inc()

        broadcast = self._chosen_manager is None

        if broadcast:
            selected_manager = 0
            resp_num = self._num_managers
        else:
            selected_manager = self._chosen_manager
            resp_num = 1

        self._check_manager_connection(selected_manager)
        msg = dmsg.DDClear(
            tag, self._client_id, self._chkpt_id, self._serialized_buffered_return_connector, broadcast=broadcast
        )
        self._send([(msg, None)], self._managers[selected_manager], buffered=True)
        self._recv_responses_and_check_err(set([tag]), resp_num)

    def get_name(self) -> str:
        return self._name

    def start_batch_put(self, persist=False) -> None:
        """

        Start a Batch Put operation. This allows efficient data loading from a
        process or processes while multiple put operations are being
        performed. A start_batch_put should be followed by a series of
        put operations (i.e. __setitem__ or pput) and then concluded by a
        call to end_batch_put. The advantage of a batch put is the
        elimination of confirmation of each put operation thereby
        reducing the amount of communication and time spent waiting for
        put operations to complete. With batch put the put operations are
        streamed to each manager.

        :param persist: If True, then the put operations should be
            persistent pput operations. Defaults to False.

        """
        self._batch_put_started = True
        self._batch_persist = persist

    def end_batch_put(self) -> None:
        if self._batch_put_started:
            self._batch_put_started = False
            # If broadcast put (bput) is called during batch put, cleanup resrouces claimed for bput.
            if self._bput_root_manager_sendh is not None:
                self._end_bput_with_batch()
            # close all send handles created for batch put
            for i in self._opened_send_handles:
                self._opened_send_handles[i].close()
            self._opened_send_handles.clear()

            expected_num_responses = len(self._num_batch_puts)
            resp_msgs = self._recv_responses(self._batch_put_msg_tags, expected_num_responses)
            self._batch_put_msg_tags.clear()

            for resp_msg in resp_msgs:
                if resp_msg.err == DragonError.MEMORY_POOL_FULL:
                    raise DDictFullError(
                        DragonError.MEMORY_POOL_FULL,
                        f"Distributed Dictionary Manager {resp_msg.managerID} is full. The key/value pair was not stored.",
                    )

                elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                    raise DDictCheckpointSyncError(resp_msg.err, resp_msg.errInfo)

                elif resp_msg.err == DragonError.DDICT_FUTURE_CHECKPOINT:
                    raise DDictFutureCheckpointError(resp_msg.err, resp_msg.errInfo)

                elif resp_msg.err != DragonError.SUCCESS:
                    raise DDictError(resp_msg.err, resp_msg.errInfo)

                elif resp_msg.numPuts != self._num_batch_puts[resp_msg.managerID]:
                    raise DDictError(
                        DragonError.FAILURE,
                        f"Failed to store all keys in the distributed dictionary. Expected number of keys to be written: {self._num_batch_puts[resp_msg.managerID]}, number of successful writes: {resp_msg.numPuts}",
                    )

            self._num_batch_puts.clear()

            # Destroy stream channels only after all managers are done with batch put.
            try:
                for i in self._batch_put_stream_channels:
                    strm_ch = self._batch_put_stream_channels[i]
                    strm_ch.destroy_process_local()
            except:
                pass
            self._batch_put_stream_channels.clear()
        else:
            raise DDictError(DragonError.INVALID_OPERATION, "Could not end batch put without starting.")

    def values(self) -> DDictValuesView:
        """

        When called this returns a view of all values in the Distributed Dictionary that can be iterated
        or otherwise inspected (i.e. for len) in an efficient manner.

        :returns: An view of the values in the DDict.

        """

        if self._chosen_manager is not None:
            self._check_manager_connection(self._chosen_manager)
            managers = [self._chosen_manager]
        else:
            self._check_manager_connection(all=True)
            managers = range(self._num_managers)

        return DDictValuesView(self, managers)

    def local_values(self) -> DDictValuesView:
        """

        Returns a DDictValuesView of the values that are local to the process
        invoking this method.

        :returns: A view of the current DDict which has only the
            co-located values of the DDict in it.

        """
        # connect to all local managers
        for i in self._local_managers:
            self._check_manager_connection(i)

        return DDictValuesView(self, self._local_managers, True)

    def items(self) -> DDictItemsView:
        """
        Returns a view of all key/value pairs in the Distributed Dictionary.

        :returns: A view of all key/value pairs.
        """
        if self._chosen_manager is not None:
            self._check_manager_connection(self._chosen_manager)
            managers = [self._chosen_manager]
        else:
            self._check_manager_connection(all=True)
            managers = range(self._num_managers)

        return DDictItemsView(self, managers)

    def local_items(self) -> DDictItemsView:
        """

        Returns a DDictItemsView of the key/value pairs that are local to the
        process invoking this method.

        :returns: A view of the current DDict which has only the
            co-located node local items of the DDict in it.

        """
        # connect to all local managers
        for i in self._local_managers:
            self._check_manager_connection(i)

        return DDictItemsView(self, self._local_managers, True)

    def update(self, dict2: DDict) -> None:
        """

        Adds all key/value pairs from dict2 into this Distributed Dictionary.

        :param dict2: Another distributed dictionary.

        :raises NotImplementedError: Not implemented.
        """
        raise NotImplementedError("Not implemented on Dragon Distributed Dictionaries.")

    def popitem(self) -> tuple[object, object]:
        """

        Returns a random key/value pair from the Distributed Dictionary.

        :returns: A random key/value pair.

        :raises NotImplementedError: Not implemented.

        """
        raise NotImplementedError("Not implemented on Dragon Distributed Dictionaries.")

    def copy(self, name: str = "") -> DDict:
        """

        Returns a copy of the Distributed Dictionary.

        :returns: A second DDict that is a copy of the first assuming that no
            other processes were concurrently using this DDict.

        """
        if not self._creator:
            raise DDictError(
                DragonError.INVALID_ARGUMENT,
                "Could not copy from a dictionary without input arguments. The original dictionary is needed to perform copy.",
            )
        input_args = self._input_args.copy()
        input_args["name"] = name
        copy_dd = DDict(**input_args)
        self.clone([copy_dd.serialize()])
        return copy_dd

    @property
    def stats(self) -> list[DDictManagerStats]:
        """

        Returns a list of manager stats, one for each manager of the distributed
        dictionary. See the DDictManagerStats structure for a description
        of its contents.

        """
        self._traceit("Getting stats from ddict")
        return list(self.dstats.values())

    @property
    def dstats(self) -> dict[int, DDictManagerStats]:
        """

        Returns a dict of manager stats, one for each manager of the distributed
        dictionary. See the DDictManagerStats structure for a description
        of its contents.

        """
        self._traceit("Getting stats from ddict")
        tag = self._tag_inc()

        broadcast = self._chosen_manager is None

        if broadcast:
            selected_manager = 0
            resp_num = self._num_managers
        else:
            selected_manager = self._chosen_manager
            resp_num = 1

        self._check_manager_connection(selected_manager)
        msg = dmsg.DDManagerStats(tag, respFLI=self._serialized_buffered_return_connector, broadcast=broadcast)
        self._send([(msg, None)], self._managers[selected_manager], buffered=True)
        msglist = self._recv_responses(set([tag]), resp_num)
        data = {}
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
            item = cloudpickle.loads(b64decode(resp_msg.data))
            data[item.manager_id] = item

        return data

    def checkpoint(self) -> None:
        """

        Calling checkpoint advances the checkpoint for this client. In subsequent
        calls to the distributed dictionary, like gets or puts, if the
        chosen manager does not have the current checkpoint in its
        working set, the get/put operations will advance the manager's
        working set to the given checkpoint or block until the checkpoint
        becomes available. Calling this operation itself does not block.

        """

        if self._batch_put_started:
            raise DDictError(DragonError.INVALID_OPERATION, "Could not proceed checkpoint during batch put.")
        self._chkpt_id += 1

    def rollback(self) -> None:
        """

        Calling rollback decrements the checkpoint id to its previous value.
        Again this call does not block. If rollback causes the checkpoint
        id to roll back to a checkpoint that a chosen manager no longer
        has in its working set, then subsequent operations may fail with
        a exception indicating the Checkpoint is no longer available,
        raising a DDictCheckpointSyncError exception.

        """

        if self._batch_put_started:
            raise DDictError(DragonError.INVALID_OPERATION, "Could not rollback checkpoint during batch put.")

        self._chkpt_id -= 1
        if self._chkpt_id < 0:
            try:
                log.debug("Reset checkpoint to 0 as the checkpoint rollback result %s is less than 0.", self._chkpt_id)
            except:
                pass
            self._chkpt_id = 0

    def sync_to_newest_checkpoint(self) -> None:
        """

        Advance the checkpoint identifier of this client to the newest checkpoint
        across all managers. This does not guarantee that all managers
        have advanced to the same checkpoint. It does guarantee that the
        client that calls this will have advanced to the newest
        checkpoint across all the mangerrs. See the
        ddict_checkpoint_pi.py demo in :example_data:`ddict/ddict_checkpoint_pi.py` for an
        example of an application that uses this method.

        """
        self._traceit("Syncing to newest checkpoint")
        tag = self._tag_inc()

        broadcast = self._chosen_manager is None

        if broadcast:
            selected_manager = 0
            resp_num = self._num_managers
        else:
            selected_manager = self._chosen_manager
            resp_num = 1

        chkpt_id = 0
        self._check_manager_connection(selected_manager)
        msg = dmsg.DDManagerNewestChkptID(tag, respFLI=self._serialized_buffered_return_connector, broadcast=broadcast)
        self._send([(msg, None)], self._managers[selected_manager], buffered=True)
        msglist = self._recv_responses(set([tag]), resp_num)
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
            chkpt_id = max(chkpt_id, resp_msg.chkptID)

        self._chkpt_id = chkpt_id

    def advance(self) -> None:
        """

        Advance to next available persisted checkpoint. This operation is for
        read only mode and directs the DDict to load a next available
        persisted checkpoint. This can be useful in replaying checkpoints
        for provenance (i.e. watching how you arrived at a given
        checkpoint state).

        """
        # read_only is only allowed when restore_from is specified
        # advance is only allowed when read_only is true
        tag = self._tag_inc()
        # connect to manager 0
        self._check_manager_connection(0)
        msg = dmsg.DDAdvance(tag, clientID=self._client_id, respFLI=self._serialized_buffered_return_connector)
        self._send([(msg, None)], self._managers[0], buffered=True)

        msglist = self._recv_responses(set([tag]), num_responses=self._num_managers)
        min_newest_chkpt_id = sys.maxsize
        for resp_msg in msglist:
            if resp_msg.err == DragonError.DDICT_PERSIST_CHECKPOINT_UNAVAILABLE:
                raise DDictPersistCheckpointError(resp_msg.err, resp_msg.errInfo)
            elif resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
            else:
                min_newest_chkpt_id = min(min_newest_chkpt_id, resp_msg.chkptID)
        self._chkpt_id = min_newest_chkpt_id
        self._restore(self._chkpt_id)

    def persist(self) -> None:
        """

        Immediately persist the current checkpoint using the provided persister
        backend. Normally persistence occurs automatically when a
        checkpoint falls out of the working set. Calling this will cause
        a checkpoint to persist immediately.

        """
        self._chkpt_avail(self._chkpt_id)

        tag = self._tag_inc()
        msg = dmsg.DDPersist(tag, chkptID=self._chkpt_id, respFLI=self._serialized_buffered_return_connector)
        self._check_manager_connection(0)
        self._send([(msg, None)], self._managers[0], buffered=True)

        msglist = self._recv_responses(set([tag]), num_responses=self._num_managers)
        for resp_msg in msglist:
            if resp_msg.err == DragonError.DDICT_PERSIST_CHECKPOINT_UNAVAILABLE:
                raise DDictPersistCheckpointError(resp_msg.err, resp_msg.errInfo)
            elif resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)

    def restore(self, chkpt: int) -> None:
        """

        Restore a persisted checkpoint to the provided checkpoint ID.

        :param chkpt: The checkpoint ID which should be restored.

        """
        if self._batch_put_started:
            raise DDictError(DragonError.INVALID_OPERATION, "Restoring checkpoint during batch put is invalid.")

        # check persisted checkpoint availability across all managers
        self._persisted_chkpt_avail(chkpt)
        self._restore(chkpt)

    def persisted_ids(self) -> list[int]:
        """

        Get a list of persisted checkpoint IDs.

        :returns: The list of persisted checkpoint IDs.

        """
        tag = self._tag_inc()
        msg = dmsg.DDPersistChkpts(tag, clientID=self._client_id, respFLI=self._serialized_buffered_return_connector)
        self._check_manager_connection(0)
        self._send([(msg, None)], self._managers[0], buffered=True)
        msglist = self._recv_responses(set([tag]), num_responses=self._num_managers)

        # initialize available chkpts with the chkpt IDs from the first responses
        if msglist[0].err != DragonError.SUCCESS:
            raise DDictError(msglist[0].err, msglist[0].errInfo)
        available_chkpts = set(msglist[0].chkptIDs)

        for resp_msg in msglist[1:]:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
            available_chkpts = available_chkpts & set(resp_msg.chkptIDs)
        ret_list = list(available_chkpts)
        ret_list.sort()
        return ret_list

    def filter(self, mgr_code: FunctionType, mgr_code_args: tuple, comparator: FunctionType, branching_factor: int = 5) -> FilterContextManager:
        """

        Calling this instantiates a tree of process groups where mgr_code is
        expected to be a function that is invoked as mgr_code(args) where
        args are (dd, out_queue)+mgr_code_args. For instance, if
        mgr_code_args are (x,) then mgr_code(dd, outqueue, x) is how
        mgr_code is invoked.

        The dd of the mgr_code arguments is this distributed dictionary directed
        toward one manager in the collection of dd managers. In other
        words, dd is as if the manager method had been invoked on this
        distributed dictionary so mgr_code only interacts with the
        manager it was provided. In addition, mgr_code is executed on the
        same node where the manager it is directed toward is running.
        This means that mgr code will get the best possible performance
        while filtering data that is associated with its manager. The
        mgr_code can do whatever computation is desired, but its chosen
        output is put into the outqueue.

        All data written to outqueue is aggregated with data coming from each
        manager in a tree-like fashion so as to be scalable to tens of
        thousands of nodes. All data put in the outqueue by mgr_code is
        assumed to be ordered from best to worst. When data is aggregated
        for sending up the tree, it is aggregated according to some kind
        of ordering which is determined by the comparator function. The
        comparator will be called as comparator(x,y) and should return
        True if x is better than y and False otherwise. If there is no
        ordering, or the ordering is not relevant to the filtering, then
        comparator(x,y) may return a constant value of False or True and
        there will be no ordering of the data.

        The branching_factor of the filtering tree has a default value, but may
        be provided by the user to create a tree of whatever width is
        desired. Note that branching_factor is the max branching factor.
        Depending on the number of managers, some nodes in the tree
        may/will have smaller numbers of children.

        The filter function returns a Context Manager that supplies an Iterator
        over which you can iterate on the filtered values. So you can write
        `with dd.filter(...) as candidates:` and then iterate over `candidates`
        inside the context to read the filtered values.

        Assuming your distributed dictionary is called dd, this will get
        num_needed elements from the result of filtering the distributed
        dictionary by calling the function get_largest on each
        distributed dictionary manager.

        :param mgr_code: A function taking arguments as described above that will
        run on the same node as a distributed dictionary manager and will
        be directed toward that manager.

        :param mgr_code_args: A tuple of arguments to pass to the mgr_code as described
            above.

        :param comparator: A function taking two arguments that should return
            True if the first argument of the values being filtered is
            "better" than the second and False otherwise. Note that returning
            a constant value of True or False will result in the filtering
            imposing no order which may be fine in some use cases.

        :param branching_factor: The maximum branching factor of any interior
            node in the filtering tree (i.e. any aggregator).

        :returns: A Context Manager that supplies an iterator which you be used to
            iterate over the filtered values.

        """

        stats = self.dstats
        nodes = query_all()
        if len(nodes) == 1:
            managers_hosts = [(manager_id, nodes[0].name) for manager_id in stats]
        else:
            managers_hosts = [(manager_id, stats[manager_id].hostname) for manager_id in stats]

        # Order by hostname so aggregators are close to other processes they will control.
        managers_hosts.sort(key=lambda tup: tup[1])
        filter_queue = SentinelQueue()
        pickled_mgr_code = cloudpickle.dumps(mgr_code)
        pickled_mgr_args = cloudpickle.dumps(mgr_code_args)
        pickled_comparator = cloudpickle.dumps(comparator)

        filter_proc = Process(
            target=filter_aggregator,
            args=(
                self,
                managers_hosts,
                branching_factor,
                pickled_mgr_code,
                pickled_mgr_args,
                pickled_comparator,
                filter_queue,
            ),
        )
        filter_proc.start()

        cm = FilterContextManager(filter_proc, filter_queue)

        return cm

    @property
    def is_frozen(self) -> bool:
        """

        Return a True of False value depending on the state of the DDict.

        :returns: True or False to indicate if the DDict is currently frozen.

        :raises DDictError: If the DDict cannot get this status from its main manager.

        """
        tag = self._tag_inc()
        msg = dmsg.DDGetFreeze(tag, self._client_id)
        resp_msg = self._send_receive([(msg, None)], self._main_manager_connection)
        if resp_msg.err != DragonError.SUCCESS:
            raise DDictError(resp_msg.err, resp_msg.errInfo)
        return resp_msg.freeze

    def freeze(self) -> None:
        """

        Freeze the DDict by placing it into read-only mode.

        :raises DDictError: If the DDict could not be frozen for some reason.

        """
        tag = self._tag_inc()
        msg = dmsg.DDFreeze(tag, self._serialized_buffered_return_connector)
        self._check_manager_connection(0)
        self._send([(msg, None)], self._managers[0])
        resp_msgs = self._recv_responses(set([tag]), self._num_managers)
        for resp in resp_msgs:
            if resp.err != DragonError.SUCCESS:
                raise DDictError(resp.err, resp.errInfo)

    def unfreeze(self) -> None:
        """

        Unfreeze the DDict by resetting the read-only state to False.

        :raises DDictError: If the DDict could not be unfrozen for some reason.

        """
        tag = self._tag_inc()
        msg = dmsg.DDUnFreeze(tag, self._serialized_buffered_return_connector)
        self._check_manager_connection(0)
        self._send([(msg, None)], self._managers[0])
        resps = self._recv_responses(set([tag]), self._num_managers)
        for resp in resps:
            if resp.err != DragonError.SUCCESS:
                raise DDictError("Failed to unfreeze DDict.")

    @property
    def checkpoint_id(self) -> int:
        """

        Returns the client's current checkpoint id.

        :returns: The current checkpoint id of the client.

        """
        return self._chkpt_id

    @checkpoint_id.setter
    def checkpoint_id(self, chkpt_id) -> None:
        """

        Set the checkpoint id of the client. The checkpoint id must be an
        integer, greater than or equal to 0.

        :param chkpt_id: A non-negative integer. Note that while you can
            set it to any value, if a manager does not have the checkpoint
            in its working set yet, then operations on the DDict may block.
            Similarly, if the given checkpoint id has already been retired from
            the working set of a manger then trying to use it will result in
            a DDictCheckpointSyncError exception being raised.

        :raises ValueError: The chkpt_id must be non-negative.

        """
        if chkpt_id < 0:
            raise ValueError("The checkpoint_id cannot be negative.")
        self._chkpt_id = chkpt_id

    @property
    def local_managers(self) -> list[int]:
        """

        Returns manager ids of all managers that are local to this node.

        """
        return self._local_managers

    @property
    def local_manager(self) -> int:
        """

        Returns a local manager id if one exists. The manager designated as the
        main manager for the client if it is on the same node as its
        local manager. Otherwise, if no local manager exists, then None
        is returned.

        """
        return self._local_manager

    @property
    def main_manager(self) -> int:
        """

        Returns the main manager id. This will always exist and will be the same
        as the local manager id if a local manager exists. Otherwise, it
        will be the id of a random manager from another node.

        """
        return self._main_manager

    @property
    def manager_nodes(self) -> list[Node]:
        """

        For each manager, a dragon.native.machine.Node object where the manager
        resides is returned.

        """
        return self._manager_nodes

    @property
    def empty_managers(self) -> list[int]:
        """

        Return a list of manager IDs that after a restart were empty because
        their persisted state could not be retrieved.

        """
        msg = dmsg.DDEmptyManagers(self._tag_inc(), respFLI=self._serialized_buffered_return_connector)
        resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)

        if resp_msg.err != DragonError.SUCCESS:
            self._cleanup()
            raise DDictError(resp_msg.err, f"Failed to create dictionary! {resp_msg.errInfo}")

        return resp_msg.managers