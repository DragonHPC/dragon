"""
The Distributed Dictionary is a performant and distributed key-value store
that is available to applications and workflows written for the Dragon ecosystem.

This is Dragon's specialized implementation based on the Dragon file-like interface
which relies on Dragon Channels. The Distributed Dictionary works like a standard
Python dictionary except that the data that it holds may span multiple nodes and be
larger than any one node can hold.

The internals of the distributed dictionary rely on several processes include a
single orchestrator process and one or more manager processes. Each client attaches
to managers on an as-needed basis. Clients discover managers by attaching to the
serialized descriptor of a Distributed Dictionary. When using a Distributed Dictionary
in Python, the dictionary will be automitically pickled/serialized and sent to new
processes in the same way a Queue or other objects can be passed as parameters in
multiprocessing.

While the Distributed Dictionary does its best to evenly distributed data across all
managers, a localized wrapper class can be used to direct key/value pairs to user
chosen managers. See the Distributed Dictionary documentation for more details.

"""

import sys
import logging
import traceback
import cloudpickle
import time
import socket
import builtins
import os


from ...utils import b64decode, b64encode, hash as dragon_hash, get_local_kv
from ...infrastructure import parameters
from ...infrastructure import messages as dmsg
from ...infrastructure import policy
from ...channels import Channel
from ...native.process import Popen
from ...dlogging.util import setup_BE_logging, DragonLoggingServices as dls
from ...dlogging.logger import DragonLoggingError

from ... import fli
from ...rc import DragonError

log = None
KEY_HINT = 1
VALUE_HINT = 2
builtin_types = frozenset(dir(builtins))

# This is a default timeout value that is used for send/receive operations.
# Longer timeouts can be specified if needed by passing in a timeout on the
# distributed dictionary creation. The timeout applies to all operations that
# could timeout in the distributed dictionary. Likely causes of timeouts are
# a manager being overfilled, but some care is taken that does not occur.
DDICT_DEFAULT_TIMEOUT = 10

# This is the default size of a distributed dictionary which would normally be
# overridden.
DDICT_MIN_SIZE = 3*1024**2 # 3 MB

# This is the generic error that all other Distributed Dictionary specific
# errors inherit from. Other types of exceptions my be raised while using the
# Distributed Dictionary, but specific errors generated by this code
# are provided here.
class DDictError(DragonLoggingError):
    def __str__(self):
        return f"DDict Exception: {self.msg}\n*** Dragon C-level Traceback: ***\n{self.lib_msg}\n*** End C-level Traceback: ***\nDragon Error Code: {self.lib_err}"


# This will be raised when a Distributed Dictionary manager has filled to
# capacity. To rectify this you may need to increase the overall size of the
# dictionary and/or devise a better distributed hashing function.
class DDictManagerFull(DDictError):
    pass

# Timeout errors that occur may be either the generic TimeoutError or
# some exception that inherits from TimeoutError, including the
# DDictTimeoutError given below. If catching these errors in your program
# it is probably best to catch the generic TimeoutError so you catch
# all types of timeout errors.
class DDictTimeoutError(DDictError, TimeoutError):
    pass

class DDict:
    # TODO: reorganize __iter__ to use DDictIterator
    # class DDictIterator:
    #     def __init__(self, ddict):
    #         self._ddict = ddict
    #         self._manager_idx = 0
    #         self._current_iter_id = -1

    #     def __next__(self):
    #         # send msg(self._current_iter_id) to manager[self._manager_idx]
    #         # recv resp for the next key
    #         # check hint -> if EOF or not
    #         # if EOF: advance to the next manager, send msg to the manager, store iter id, send msg to manager
    #         #       if no more manager: raise StopIteration
    #         # else: return key
    #         pass

    def __init__(self, managers_per_node:int=1, n_nodes:int=1, total_mem:int=DDICT_MIN_SIZE, *,
                 working_set_size:int=1, wait_for_keys:bool=False, wait_for_writers:bool=False,
                 policy:policy.Policy=None, persist_freq:int=0, persist_base_name:str="",
                 timeout:float=DDICT_DEFAULT_TIMEOUT) -> None:
        """Construct a Distributed Dictionary to be shared amongst distributed processes running
           in the Dragon Runtime. The distributed dictionary creates the specified number of managers
           and shards the data across all managers. The total memory of the dictionary is split
           across all the managers, so you want to allocate more space than is required by perhaps
           30 percent, but that should be determined via some experimentation and depends on the
           application being developed. See the Dragon documentation's section on the Distributed
           Dictionary design for more details about creating and using a distributed dictionary.

        Args:
            managers_per_node (int, optional): The number of managers on each
            node. The total_mem is divided up amongst the managers.
            Defaults to 1.

            n_nodes (int, optional): The number of nodes that will have managers
            deployed on them. Defaults to 1.

            total_mem (int, optional): The total memory in bytes that will be
            sharded across all managers. Defaults to DDICT_MIN_SIZE
            but this is really a minimum size for a single manager
            and should be specified by the user.

            working_set_size (int, optional): Not implemented yet. This sets the
            size of the checkpoint, in memory, working set. This
            determines how much state each manager will keep
            internally. This is the number of different, simultaneous
            checkpoints that may be active at any point in time.
            Defaults to 1.

            wait_for_keys (bool, optional): Not implemented yet. Setting this to
            true means that each manager will keep track of a set of
            keys at each checkpoint level and clients advancing to a
            new checkpoint level will block until the set of keys at
            the oldest, retiring working set checkpoint are all
            written. By specifying this all clients will remain in
            sync with each other relative to the size of the working
            set. Defaults to False. It is also possible to store
            key/values that are not part of the checkpointing set of
            key/values. Those keys are called persistent keys and
            will not be affected by setting this argument to true.

            wait_for_writers (bool, optional): Not implemented yet. Setting this
            to true means that each manager will wait for a set of
            clients to have all advanced their checkpoint id beyond
            the oldest checkpointing id before retiring a checkpoint
            from the working set. Setting this to true will cause
            clients that are advancing rapidly to block while others
            catch up. Defaults to False.

            policy (policy.Policy, optional): A policy can be supplied for
            starting the managers. Please read about policies in the
            Process Group documentation. Managers are started via a
            Process Group and placement of managers and other
            characteristics can be controlled via a policy. Defaults
            to None which applies a Round-Robin policy.

            persist_freq (int, optional): Not implemented yet. This is the
            frequency that a checkpoint will be persisted to disk.
            This is independent of the working set size and can be
            any frequency desired. Defaults to 0 which means that no
            persisting will be done.

            persist_base_name (str, optional): Not implemented yet. This is a
            base file name to be applied to persisted state for the
            dictionary. This base name along with a checkpoint number
            is used to restore a distributed dictionary from a
            persisted checkpoint. Defaults to "".

            timeout (float, optional): This is a timeout that will be used for
            all timeouts on the creating client and all managers
            during communication between the distributed components
            of the dictionary. New clients wishing to set their own
            timeout can use the attach method to specify their own
            local timeout. Defaults to DDICT_DEFAULT_TIMEOUT.

        Raises:
            AttributeError: If incorrect parameters are supplied.
            RuntimeError: If there was an unexpected error during initialization.
        """

        # This is the pattern used in the pydragon_perf.pyx file
        # It works, but may need review if it's the way we want to do it

        # This block turns on client log for the initial client that creates the dictionary
        global log
        if log == None:
            fname = f'{dls.DD}_{socket.gethostname()}_client_{str(parameters.this_process.my_puid)}.log'
            setup_BE_logging(service=dls.DD, fname=fname)
            log = logging.getLogger(str(dls.DD))

        try:
            # Start the Orchestrator and capture its serialized descriptor so we can connect to it.
            if type(managers_per_node) is not int or type(n_nodes) is not int or type(total_mem) is not int:
                raise AttributeError('When creating a Dragon Distributed Dict you must provide managers_per_node, n_nodes, and total_mem')

            proc = Popen(executable=sys.executable, args=['-c',
                f'import dragon.data.ddict.orchestrator as orc; orc.start({managers_per_node}, {n_nodes}, {total_mem})'],
                stdout=Popen.PIPE)

            # Read the serialized FLI of the orchestrator.
            ddict = proc.stdout.recv().strip()

            self._orc_connector = fli.FLInterface.attach(b64decode(ddict))
            self._args = (working_set_size, wait_for_keys, wait_for_writers, policy, persist_freq, persist_base_name, timeout)

            self.__setstate__((ddict, timeout))
        except Exception as ex:
            tb = traceback.format_exc()
            log.debug(f'There is an exception initializing ddict: {ex}\n Traceback: {tb}\n')
            raise RuntimeError(f'There is an exception initializing ddict: {ex}\n Traceback: {tb}\n')

    def __setstate__(self, args):
        serialized_orc, timeout = args

        # This block turns on a client log for each client
        # global log
        # if log == None:
        #     fname = f'{dls.DD}_{socket.gethostname()}_client_{str(parameters.this_process.my_puid)}.log'
        #     setup_BE_logging(service=dls.DD, fname=fname)
        #     log = logging.getLogger(str(dls.DD))

        self._managers = dict()
        self._tag = 0
        self._destroyed = False
        self._detached = False
        self._timeout = timeout

        try:
            return_channel = Channel.make_process_local()
            buffered_return_channel = Channel.make_process_local()

            self._default_pool = return_channel.get_pool()

            self._return_connector = fli.FLInterface(main_ch=return_channel)
            self._serialized_return_connector = b64encode(self._return_connector.serialize())

            self._buffered_return_connector = fli.FLInterface(main_ch=buffered_return_channel, use_buffered_protocol=True)
            self._serialized_buffered_return_connector = b64encode(self._buffered_return_connector.serialize())

            self._serialized_orc = serialized_orc
            try:
                self._create(b64encode(cloudpickle.dumps((self._args))))
            except AttributeError:
                self._orc_connector = fli.FLInterface.attach(b64decode(serialized_orc))

            self._client_id = None

            self._get_main_manager()
            self._register_client_to_main_manager()
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug(f'There is an exception __setstate__ of ddict: {ex}\n Traceback: {tb}\n')
            except:
                pass
            raise RuntimeError(f'There is an exception __setstate__ of ddict: {ex}\n Traceback: {tb}\n')

    def __getstate__(self):
        return (self.serialize(), self._timeout)

    def __del__(self):
        try:
            self.detach()
        except Exception as ex:
            try:
                tb = traceback.format_exc()
                log.debug(f'There was an exception while terminating the Distributed Dictionary. Exception is {ex}\n Traceback: {tb}\n')
            except:
                pass

    def _create(self, pickled_args):
        msg = dmsg.DDCreate(self._tag_inc(), respFLI=self._serialized_buffered_return_connector, args=pickled_args)
        resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)
        if resp_msg.err != DragonError.SUCCESS:
            raise RuntimeError('Failed to create dictionary!')

    def _get_main_manager(self): # SHGetKV
        try:
            serialized_main_manager = get_local_kv(key=self._serialized_orc)
            self._main_manager_connection = fli.FLInterface.attach(b64decode(serialized_main_manager))
        except KeyError as e:
            # no manager on the node, get a random manager from orchestrator
            try:
                log.info(f'Got KeyError {e} during bringup, sending get random manager request to orchestrator')
            except:
                pass
            msg = dmsg.DDGetRandomManager(self._tag_inc(), respFLI=self._serialized_buffered_return_connector)
            resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)
            if resp_msg.err != DragonError.SUCCESS:
                raise RuntimeError('Client failed to get manager from orchestrator')
            self._main_manager_connection = fli.FLInterface.attach(b64decode(resp_msg.manager))

    def _register_client_to_main_manager(self): # client ID assigned here
        msg = dmsg.DDRegisterClient(self._tag_inc(), respFLI=self._serialized_return_connector, bufferedRespFLI=self._serialized_buffered_return_connector) # register client to the manager (same node)
        resp_msg = self._send_receive([(msg, None)], connection=self._main_manager_connection)
        if resp_msg.err != DragonError.SUCCESS:
            raise RuntimeError('Client failed to connect to main manager.')
        self._client_id = resp_msg.clientID
        self._num_managers = resp_msg.numManagers

    def _connect_to_manager(self, manager_id):
        msg = dmsg.DDConnectToManager(self._tag_inc(), clientID=self._client_id, managerID=manager_id)
        resp_msg = self._send_receive([(msg, None)], connection=self._main_manager_connection)
        if resp_msg.err != DragonError.SUCCESS:
            raise RuntimeError(f'Client {self._client_id} failed to coonect to manager {manager_id}')

        self._managers[manager_id] = fli.FLInterface.attach(b64decode(resp_msg.manager))

    def _register_client_ID_to_manager(self, manager_id):
        try:
            msg = dmsg.DDRegisterClientID(self._tag_inc(), clientID=self._client_id, respFLI=self._serialized_return_connector, bufferedRespFLI=self._serialized_buffered_return_connector)
            resp_msg = self._send_receive([(msg, None)], connection=self._managers[manager_id])
            if resp_msg.err != DragonError.SUCCESS:
                raise Exception(f'Failed to register client {self._client_id} to manager {manager_id}')
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug(f'There was an exception registering client ID {self._client_id=} with manager: {ex} \n Traceback: {tb}')
            except:
                pass
            raise RuntimeError(f'There was an exception registering client ID {self._client_id=} with manager: {ex} \n Traceback: {tb}')

    def _check_manager_connection(self, manager_id=None, all=False):
        if all:
            for manager_id in range(self._num_managers):
                if manager_id not in self._managers:
                    self._connect_to_manager(manager_id)
                    self._register_client_ID_to_manager(manager_id)
        elif not (manager_id in self._managers):
            self._connect_to_manager(manager_id)
            self._register_client_ID_to_manager(manager_id)

    def _choose_manager_pickle_key(self, key):
        # Check to see if there is a user-defined hash function. If so, then
        # assume it is deterministic and the same across all nodes and use it.
        pickled_key = cloudpickle.dumps(key)

        try:
            if key.__hash__.__class__.__name__ == 'method':
                return (hash(key) % self._num_managers, pickled_key)
        except:
            pass

        hash_val = dragon_hash(pickled_key)
        return (hash_val % self._num_managers, pickled_key)


    def _tag_inc(self):
        tag = self._tag
        self._tag += 1
        return tag

    def _send(self, msglist, connection):
        with connection.sendh(timeout=self._timeout) as sendh:
            for (msg, arg) in msglist:
                if arg is None:
                    sendh.send_bytes(msg.serialize(), timeout=self._timeout)
                else:
                    # It is a pickled value so don't call serialize.
                    sendh.send_bytes(msg, arg=arg, timeout=self._timeout)

    def _recv_resp(self):
        with self._buffered_return_connector.recvh(timeout=self._timeout) as recvh:
            resp_ser_msg, hint = recvh.recv_bytes(timeout=self._timeout)
        return dmsg.parse(resp_ser_msg)

    def _recv_dmsg_and_val(self, key):
        with self._return_connector.recvh(use_main_as_stream_channel=True, timeout=self._timeout) as recvh:
            (resp_ser_msg, hint) = recvh.recv_bytes(timeout=self._timeout)
            resp_msg = dmsg.parse(resp_ser_msg)
            if resp_msg.err != DragonError.SUCCESS:
                raise KeyError(key)
            try:
                value = cloudpickle.load(file=PickleReadAdapter(recvh=recvh, hint=VALUE_HINT, timeout=self._timeout))
            except Exception as e:
                tb = traceback.format_exc()
                try:
                    log.info(f'Exception caught in cloudpickle load: {e} \n Traceback: {tb}')
                except:
                    pass
                raise RuntimeError(f'Exception caught in cloudpickle load: {e} \n Traceback: {tb}')

        return value

    def _send_receive(self, msglist, connection):
        try:
            self._send(msglist, connection)
            resp_msg = self._recv_resp()
            return resp_msg

        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug(f'There was an exception in the _send_receive in ddict: {ex} \n Traceback: {tb}')
            except:
                pass
            raise RuntimeError(f'There was an exception in the _send_receive in ddict: {ex} \n Traceback: {tb}')

    def destroy(self):
        if self._destroyed:
            return

        self._destroyed = True
        try:
            msg = dmsg.DDDestroy(self._tag_inc(), self._client_id, respFLI=self._serialized_buffered_return_connector)
            resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug(f'There was an exception in the destroy: {ex} \n Traceback: {tb}')
            except:
                pass

        try:
            self._orc_connector.detach()
        except Exception as ex:
            try:
                tb = traceback.format_exc()
                log.debug(f'There was an exception while detaching orchestrator channel: {ex} \n Traceback: {tb}')
            except:
                pass

    def serialize(self):
        return self._serialized_orc

    @classmethod
    def attach(cls, serialized_dict, timeout=0):
        new_client = cls.__new__(cls)
        new_client.__setstate__((serialized_dict, timeout))
        return new_client

    def __setitem__(self, key, value):
        msg = dmsg.DDPut(self._tag_inc(), self._client_id)
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._check_manager_connection(manager_id)
        try:
            with self._managers[manager_id].sendh(timeout=self._timeout) as sendh:
                sendh.send_bytes(msg.serialize(), timeout=self._timeout)
                sendh.send_bytes(pickled_key, arg=KEY_HINT, timeout=self._timeout)
                cloudpickle.dump(value, file=PickleWriteAdapter(sendh=sendh, hint=VALUE_HINT, timeout=self._timeout))

        except TimeoutError as ex:
            raise DDictTimeoutError(DragonError.TIMEOUT, f'The operation timed out. This could be a network failure or an out of memory condition.\n{str(ex)}')

        try:
            resp_msg = self._recv_resp()
        except TimeoutError as ex:
            raise DDictTimeoutError(DragonError.TIMEOUT, f'The operation timed out. This could be a network failure or an out of memory condition.\n{str(ex)}')

        if resp_msg.err == DragonError.MEMORY_POOL_FULL:
            raise DDictManagerFull(DragonError.MEMORY_POOL_FULL, f"Distributed Dictionary Manager {manager_id} is full. The key/value pair was not stored.", )

        if resp_msg.err != DragonError.SUCCESS:
            raise DDictError(resp_msg.err, 'Failed to store key in the distributed dictionary.')

    def __getitem__(self, key):
        msg = dmsg.DDGet(self._tag_inc(), self._client_id)
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._check_manager_connection(manager_id)
        self._send([(msg, None),
                    (pickled_key, KEY_HINT)], self._managers[manager_id])

        value = self._recv_dmsg_and_val(key)
        return value

    def keys(self):

        keys = []
        self._check_manager_connection(all=True)
        for manager_id in range(self._num_managers):
            msg = dmsg.DDKeys(self._tag_inc(), self._client_id)
            self._send([(msg, None)], self._managers[manager_id])
            with self._return_connector.recvh(use_main_as_stream_channel=True, timeout=self._timeout) as recvh:
                resp_ser_msg, _ = recvh.recv_bytes(timeout=self._timeout)
                resp_msg = dmsg.parse(resp_ser_msg)
                if resp_msg.err != DragonError.SUCCESS:
                    raise RuntimeError(f'{resp_msg.err}')
                done = False
                while not done:
                    try:
                        key = cloudpickle.load(file=PickleReadAdapter(recvh=recvh, hint=KEY_HINT, timeout=self._timeout))
                        keys.append(key)
                    except EOFError:
                        done = True
                        break
        return keys

    def values(self):
        raise NotImplementedError('Not implemented on Dragon Distributed Dictionaries.')

    def items(self):
        raise NotImplementedError('Not implemented on Dragon Distributed Dictionaries.')

    def pop(self, key):
        msg = dmsg.DDPop(self._tag_inc(), self._client_id)
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._check_manager_connection(manager_id)
        self._send([(msg, None),
                    (pickled_key, KEY_HINT)], self._managers[manager_id])
        return self._recv_dmsg_and_val(key)

    def clear(self):

        self._check_manager_connection(all=True)

        for manager_id in range(self._num_managers):
            msg = dmsg.DDClear(self._tag_inc(), self._client_id)
            self._send([(msg, None)], self._managers[manager_id])

        for _ in range(self._num_managers):
            with self._buffered_return_connector.recvh(timeout=self._timeout) as recvh:
                (resp_ser_msg, hint) = recvh.recv_bytes(timeout=self._timeout)
                resp_msg = dmsg.parse(resp_ser_msg)
                if resp_msg.err != DragonError.SUCCESS:
                    raise RuntimeError(resp_msg.err)

    def update(self, dict2):
        raise NotImplementedError('Not implemented on Dragon Distributed Dictionaries.')

    def popitem(self):
        raise NotImplementedError('Not implemented on Dragon Distributed Dictionaries.')

    def copy(self):
        raise NotImplementedError('Not implemented on Dragon Distributed Dictionaries.')

    def __contains__(self, key):
        msg = dmsg.DDContains(self._tag_inc(), self._client_id)
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._check_manager_connection(manager_id)
        resp_msg = self._send_receive([(msg, None), (pickled_key, KEY_HINT)],
                                      connection=self._managers[manager_id])

        if resp_msg.err == DragonError.SUCCESS:
            return True

        if resp_msg.err == DragonError.KEY_NOT_FOUND:
            return False

        raise RuntimeError(resp_msg.err)

    def __len__(self):
        self._check_manager_connection(all=True)

        for manager_id in range(self._num_managers):
            msg = dmsg.DDGetLength(self._tag_inc(), self._client_id)
            sendh = self._send([(msg, None)], self._managers[manager_id])

        length = 0
        for _ in range(self._num_managers):
            resp_msg = self._recv_resp()
            if resp_msg.err == DragonError.SUCCESS:
                length += resp_msg.length
            else:
                raise RuntimeError(resp_msg.err)
        return length

    def __delitem__(self, key):
        self.pop(key)

    # Not yet implemented.
    # def __iter__(self):
    #     """
    #     not safe to iterate over dictionary while other clients are modifying
    #     """
    #     try:
    #         self._check_manager_connection(all=True)

    #         for manager_id in range(self._num_managers):
    #             msg = dmsg.DDGetIterator(self._tag_inc(), self._client_id)

    #             resp_msg = self._send_receive([(msg, None)], connection=self._managers[manager_id])
    #             if resp_msg.err != DragonError.SUCCESS:
    #                 raise RuntimeError('Fail to get iterator from manager')
    #             iter_id = resp_msg.iterID
    #             done = False
    #             while not done:
    #                 msg = dmsg.DDIteratorNext(tag=self._tag_inc(), clientID=self._client_id, iterID=iter_id)
    #                 self._send([(msg, None)], connection=self._managers[manager_id])
    #                 with self._return_connector.recvh(use_main_as_stream_channel=True, timeout=self._timeout) as recvh:
    #                     resp_ser_msg, hint = recvh.recv_bytes(timeout=self._timeout)
    #                     resp_msg = dmsg.parse(resp_ser_msg)
    #                     if resp_msg.err == DragonError.NO_MORE_KEYS:
    #                         done = True
    #                     elif resp_msg.err != DragonError.SUCCESS:
    #                         raise RuntimeError('Unable to iterate the next key.')
    #                     else:
    #                         try:
    #                             key = cloudpickle.load(file=PickleReadAdapter(recvh=recvh, hint=KEY_HINT, timeout=self._timeout))
    #                             yield key
    #                         except Exception as e:
    #                             tb = traceback.format_exc()
    #                             raise e
    #     except Exception as e:
    #         tb = traceback.format_exc()
    #         log.debug(f'Got exception in client iter: {e}\n Traceback: {tb}')
    #         raise RuntimeError(f'Got exception in client iter: {e}\n Traceback: {tb}')

    def __hash__(self):
        raise NotImplementedError('Not implemented on Dragon Distributed Dictionaries.')

    def __equal__(self):
        raise NotImplementedError('Not implemented on Dragon Distributed Dictionaries.')

    def __str__(self): # return a iterator for an object
        raise NotImplementedError('Not implemented on Dragon Distributed Dictionaries.')

    def dump_state(self):
        pass

    def detach(self):
        try:
            if self._destroyed or self._detached:
                try:
                    log.debug(f'Cannot detach client {self._client_id} from a destroyed/detached dictionary.')
                except:
                    pass
                return

            self._detached = True

            for manager_id in self._managers:
                try:
                    msg = dmsg.DDDeregisterClient(self._tag_inc(), clientID=self._client_id, respFLI=self._serialized_buffered_return_connector)
                    resp_msg = self._send_receive([(msg, None)], connection=self._managers[manager_id])
                    if resp_msg.err != DragonError.SUCCESS:
                        log.debug(f'Error on response to deregister client {self._client_id}')

                    self._managers[manager_id].detach()
                except:
                    pass

        except Exception as ex:
            try:
                tb = traceback.format_exc()
                log.debug(f'There was an exception while detaching the client {self._client_id}. Exception: {ex}\n Traceback: {tb}')
            except:
                pass

class PickleWriteAdapter:

    def __init__(self, sendh, timeout=None, hint=None):
        self._sendh = sendh
        self._timeout = timeout
        self._hint = hint

    def write(self, b):
        try:
            self._sendh.send_bytes(b, timeout=self._timeout, arg=self._hint)
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug(f'Caught exception in pickle write: {ex}\n {tb}')
            except:
                pass

class PickleReadAdapter:

    def __init__(self, recvh, timeout=None, hint=None):
        self._recvh = recvh
        self._timeout = timeout
        self._hint = hint

    def read(self, size=-1):
        try:
            data, arg = self._recvh.recv_bytes(size=size, timeout=self._timeout)
            assert arg == self._hint
            return data
        except EOFError:
            return b''
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug(f'Caught exception in pickle read: {ex}\n {tb}')
            except:
                pass

    def readline(self):
        return self.read()
