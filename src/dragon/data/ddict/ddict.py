"""
The Distributed Dictionary is a performant and distributed key-value store
that is available to applications and workflows written for the Dragon ecosystem.

This is Dragon's specialized implementation based on the Dragon file-like interface
which relies on Dragon Channels. The Distributed Dictionary works like a standard
Python dictionary except that the data that it holds may span multiple nodes and be
larger than any one node can hold.

The internals of the distributed dictionary rely on several processes include a
single orchestrator process and one or more manager processes. Each client attaches
to managers on an as-needed basis. Clients discover managers by attaching to the
serialized descriptor of a Distributed Dictionary. When using a Distributed Dictionary
in Python, the dictionary will be automatically pickled/serialized and sent to new
processes in the same way a Queue or other objects can be passed as parameters in
multiprocessing.

While the Distributed Dictionary does its best to evenly distributed data across all
managers, a localized wrapper class can be used to direct key/value pairs to user
chosen managers. See the Distributed Dictionary documentation for more details.

"""

import sys
import math
import logging
import traceback
import cloudpickle
import pickle
import time
import socket
import os
import copy
from dataclasses import dataclass, field
import multiprocessing as mp
import heapq
from types import FunctionType
from collections.abc import Iterator
import random

from ...utils import b64decode, b64encode, hash as dragon_hash, get_local_kv, host_id
from ...infrastructure.parameters import this_process
from ...infrastructure import messages as dmsg
from ...infrastructure import policy
from ...channels import Channel
from ...native.process import Popen
from ...dlogging.util import setup_BE_logging, DragonLoggingServices as dls
from ...dlogging.logger import DragonLoggingError
from ...native.machine import Node
from dragon.native.process_group import ProcessGroup
from dragon.infrastructure.policy import Policy
from dragon.native.process import ProcessTemplate
from dragon.globalservices.node import query_all

from ... import fli
from dragon.fli import PickleWriteAdapter, PickleReadAdapter
from ...rc import DragonError

log = None
KEY_HINT = 1
VALUE_HINT = 2

# This is a default timeout value that is used for send/receive operations.
# Timeouts can be specified if needed by passing in a timeout on the
# distributed dictionary creation. The timeout applies to all operations that
# could timeout in the distributed dictionary. Likely causes of timeouts are
# a manager being overfilled, but some care is taken that does not occur.
# A timeout of None indicates to wait forever. Otherwise, positive timeout
# values are in seconds.
DDICT_DEFAULT_TIMEOUT = None

# This is the default size of a distributed dictionary which would normally be
# overridden.
DDICT_MIN_SIZE = 3 * 1024**2  # 3 MB


# This is the generic error that all other Distributed Dictionary specific
# errors inherit from. Other types of exceptions my be raised while using the
# Distributed Dictionary, but specific errors generated by this code
# are provided here.
class DDictError(DragonLoggingError):
    def __str__(self):
        if not self.lib_msg.strip():
            return f"DDict Exception: {self.msg}\nDragon Error Code: {self.lib_err}"
        return f"DDict Exception: {self.msg}\n*** Dragon C-level Traceback: ***\n{self.lib_msg}\n*** End C-level Traceback: ***\nDragon Error Code: {self.lib_err}"


# This will be raised when a Distributed Dictionary manager failed to reconstruct
# empty managers.
class DDictSyncError(DDictError):
    pass


# This will be raised when a Distributed Dictionary manager has filled to
# capacity. To rectify this you may need to increase the overall size of the
# dictionary and/or devise a better distributed hashing function.
class DDictManagerFull(DDictError):
    pass


# Timeout errors that occur may be either the generic TimeoutError or
# some exception that inherits from TimeoutError, including the
# DDictTimeoutError given below. If catching these errors in your program
# it is probably best to catch the generic TimeoutError so you catch
# all types of timeout errors.
class DDictTimeoutError(DDictError, TimeoutError):
    pass


# KeyErrors that can also be caught as DDictErrors.
class DDictKeyError(DDictError, KeyError):
    def __init__(self, err, msg, key):
        super().__init__(err, msg)
        super(KeyError, self).__init__(key)
        self.key = key

    def __str__(self):
        err_str = super().__str__()
        return err_str + ": " + self.key


# A Checkpoint Sync error can occur when a client is trying to work with
# a dictionary checkpoint that has now been retired from the working set
# of checkpoints.
class DDictCheckpointSync(DDictError):
    pass


class DDictUnableToCreateDDict(DDictError):
    pass


# A Future Checkpoint error can occur when a client tries to perform batch
# put on the checkpoint that has not existed yet.
class DDictFutureCheckpoint(DDictError):
    pass


# Make the key a little more precise by stripping off pickled data.
def strip_pickled_bytes(byte_str):
    if byte_str[-2:] == b"\x94.":
        byte_str = byte_str[:-2]

    if byte_str[-1:] == b".":
        byte_str = byte_str[:-1]

    if byte_str[:3] == b"\x80\x05\x95":
        byte_str = byte_str[3:]

    return byte_str


@dataclass
class DDictManagerStats:
    """
    Included in manager stats are the manager identifier (0 to num_managers-1), the
    total number of bytes in the manager's pool, the total used bytes in the manager's pool,
    the number of key/value pairs stored in the manager, and the dictionary of free blocks. The
    free blocks has the block size and the number of free blocks of that size. If the pool is
    empty, then there will be one free block of the same size as the total number of bytes of the
    manager's pool. Otherwise, free blocks can be used to see the amount of fragmentation within
    the pool by looking at the various block sizes and number of blocks available. NOTE: Any
    larger block (except the smallest block size) can be split into two smaller blocks for
    smaller allocations.
    """

    manager_id: int
    hostname: str
    total_bytes: int
    total_used_bytes: int
    pool_free_space: int
    pool_utilization: float
    num_keys: int
    free_blocks: dict
    max_pool_allocations: int
    max_pool_allocations_used: int
    current_pool_allocations_used: int


# A SentinelQueue is a queue that raise EOFError when end of
# file is reached. It knows to do this by writing a sentinel
# into the queue when the queue is closed. A SentinelQueue
# contains a multiprocessing Queue. Other methods could
# be implemented for SentinelQueue, but are not needed in
# this example.
class SentinelQueue:
    _SENTINEL = "sentinel_queue_sentinel"

    def __init__(self):
        self._queue = mp.Queue()
        self._put_called = False
        self._get_called = False
        self._sentinel_sent = False
        self._closed = False
        self._sentinel_received = False

    def get(self):
        if self._closed:
            raise RuntimeError("Cannot get an item from a closed SentinelQueue")

        if not self._get_called:
            self._shutdown_event = self._queue.get()
            self._get_called = True

        item = self._queue.get()
        if item == SentinelQueue._SENTINEL:
            self._sentinel_received = True
            raise EOFError("SentinelQueue EOF")

        return item

    def put(self, item):
        if self._closed:
            raise RuntimeError("Cannot put an item on a closed SentinelQueue")

        if not self._put_called:
            self._shutdown_event = mp.Event()
            self._queue.put(self._shutdown_event)
            self._put_called = True

        if self._shutdown_event.is_set():
            raise EOFError("SentinelQueue receiving end closed.")

        self._queue.put(item)

    def close(self):
        if self._closed:
            return

        if self._put_called and not self._sentinel_sent:
            self._sentinel_sent = True
            self._queue.put(SentinelQueue._SENTINEL)

        if self._get_called and not self._sentinel_received:
            # This is a receiving end of a SentinelQueue,
            # so set the event and empty the queue.
            self._shutdown_event.set()
            try:
                while True:
                    self.get()
            except:
                pass

        try:
            self._queue.close()
        except:
            pass

        self._closed = True


# The PQEntry class is needed by the priority
# queue which is used to always know which queue
# to get the next value from. The __lt__ orders
# the priority queue elements by their original
# values. But the queue index of where the value
# came from is carried along with the value in the
# priority queue so the merging algorithm knows where
# to get the next value. In this way, the total number
# of entries in the priority queue is never more than the
# fanin value of the MergePool.
class PQEntry:
    def __init__(self, value, queue_index, comparator):
        self._value = value
        self._queue_index = queue_index
        self._comparator = comparator

    def __lt__(self, other):
        # The comparator is supplied by the user and must take
        # to arguments to be compared and return True or False.
        return self._comparator(self._value, other._value)

    @property
    def queue_index(self):
        return self._queue_index

    @property
    def value(self):
        return self._value

    def __repr__(self):
        return "PQEntry(" + str(self.value) + "," + str(self.queue_index) + "," + str(self._comparator) + ")"

    def __str__(self):
        return repr(self)


# This is here for type hinting below. It is redefined right below.
class DDict:
    pass


def filter_manager(dd, pickled_src_code, pickled_src_args, out_queue, manager_id):
    try:
        src_code = cloudpickle.loads(pickled_src_code)
        src_args = cloudpickle.loads(pickled_src_args)
        my_manager = dd.manager(manager_id)
        args = (my_manager, out_queue) + src_args
        src_code(*args)
    except Exception as ex:
        tb = traceback.format_exc()
        print(
            "There was an exception in filter_manager: %s\n Traceback: %s" % (ex, tb),
            flush=True,
            file=sys.stderr,
        )
        raise ex
    finally:
        out_queue.close()


def filter_aggregator(
    dd: DDict, managers_hosts, branching_factor, pickled_src_code, pickled_src_args, pickled_comparator, out_queue
):
    try:
        comparator = cloudpickle.loads(pickled_comparator)
        # managers_hosts is a list of tuples (manager_id, hostname)
        if len(managers_hosts) == 1:
            # It was already a in a process started on the manager node, so just call
            # the filter_manager code.
            return filter_manager(dd, pickled_src_code, pickled_src_args, out_queue, managers_hosts[0][0])

        # At larger scales, it is useful create more than one of these processes,
        # themselves in a process group where each of them is given an output queue and
        # a set of managers to merge (instead of merging all managers in one process).
        # Then the second-level merging would merge their managers, writing the merged
        # values to their output queues while a third level merge is done to merge all the
        # second level merges together. In this way, this algorithm can scale to whatever
        # size is necessary.

        filter_queues = []
        shutdown_events = []

        if len(managers_hosts) > branching_factor:

            num_divisions = len(managers_hosts) // branching_factor

            if num_divisions == 1:
                num_divisions = 2

            if num_divisions > branching_factor:
                num_divisions = branching_factor

            division_sz = len(managers_hosts) // num_divisions

            if division_sz * num_divisions < len(managers_hosts):
                division_sz += 1

            grp = ProcessGroup(restart=False)

            while len(managers_hosts) > 0:
                managers_subset = managers_hosts[:division_sz]
                managers_hosts = managers_hosts[division_sz:]
                filter_queue = SentinelQueue()
                filter_queues.append(filter_queue)

                node_name = managers_subset[0][1]
                local_policy = Policy(
                    placement=Policy.Placement.HOST_NAME,
                    host_name=node_name,
                )
                grp.add_process(
                    nproc=1,
                    template=ProcessTemplate(
                        target=filter_aggregator,
                        args=(
                            dd,
                            managers_subset,
                            branching_factor,
                            pickled_src_code,
                            pickled_src_args,
                            pickled_comparator,
                            filter_queue,
                        ),
                        policy=local_policy,
                    ),
                )
        else:

            grp = ProcessGroup(restart=False)

            for manager_id, node_name in managers_hosts:
                filter_queue = SentinelQueue()
                filter_queues.append(filter_queue)

                local_policy = Policy(
                    placement=Policy.Placement.HOST_NAME,
                    host_name=node_name,
                )
                grp.add_process(
                    nproc=1,
                    template=ProcessTemplate(
                        target=filter_manager,
                        args=(dd, pickled_src_code, pickled_src_args, filter_queue, manager_id),
                        policy=local_policy,
                    ),
                )

        grp.init()
        grp.start()
        # prime the priority queue. The values coming from the
        priority_queue = []

        try:
            for i in range(len(filter_queues)):
                try:
                    item = filter_queues[i].get()
                    heapq.heappush(priority_queue, PQEntry(item, i, comparator))
                except EOFError:
                    pass

            # merge the values from different procs

            while len(priority_queue) > 0:
                # If items are not in strictly decreasing order for values, then
                # you need to reverse the < to a > in the PQEntry __lt__ method.
                item = heapq.heappop(priority_queue)
                out_queue.put(item.value)

                try:
                    next = filter_queues[item.queue_index].get()
                    heapq.heappush(priority_queue, PQEntry(next, item.queue_index, comparator))
                except EOFError:
                    pass
        except EOFError:
            # This could happen if the outqueue is closed before this proc is done.
            pass
        except Exception as ex:
            tb = traceback.format_exc()
            print(
                "There was an exception in filter_aggregator priming: %s\n Traceback: %s" % (ex, tb),
                flush=True,
                file=sys.stderr,
            )
            raise ex

        out_queue.close()

        while len(filter_queues) > 0:
            filter_queue = filter_queues.pop()
            try:
                filter_queue.close()
                del filter_queue
            except:
                pass

        grp.join()
        grp.close()

    except Exception as ex:
        tb = traceback.format_exc()
        print(
            "There was an exception in filter_aggregator: %s\n Traceback: %s" % (ex, tb),
            flush=True,
            file=sys.stderr,
        )
        raise ex


class FilterIterator:
    def __init__(self, out_queue):
        self._queue = out_queue

    def __iter__(self):
        return self

    def __next__(self):
        try:
            value = self._queue.get()
            return value
        except EOFError:
            raise StopIteration("End of filter stream")

    def close(self):
        self._queue.close()


class FilterContextManager:
    def __init__(self, filter_proc, filter_queue):
        self._proc = filter_proc
        self._queue = filter_queue

    def __enter__(self):
        # Simulate resource acquisition
        self._iter = FilterIterator(self._queue)
        return self._iter

    def __exit__(self, exc_type, exc_val, exc_tb):
        # Simulate resource release
        self._iter.close()
        self._proc.join()
        return False


class DDictMappingProxy:
    """
    A read-only live copy of the DDict. This mimics the mapping proxy of a dict.
    It should be noted that the DDict does not maintain the order of insertions into
    it like a dict. DDict mapping proxies also do not support being reversed.
    """

    def __init__(self, ddict):
        self._ddict = ddict

    def __contains__(self, key):
        return key in self._ddict

    def __getitem__(self, key):
        return self._ddict[key]

    def __iter__(self):
        return iter(self._ddict)

    def __len__(self):
        return len(self._ddict)

    def __reversed__(self):
        raise NotImplementedError("Keys are not ordered on DDicts and cannot be reversed.")

    def __hash__(self):
        return None

    def copy(self):
        return self._ddict.copy()

    def get(self, key, default=None):
        try:
            return self._ddict[key]
        except KeyError:
            return default

    def items(self):
        return self._ddict.items()

    def keys(self):
        return self._ddict.keys()

    def values(self):
        return self._ddict.values()


class DDictKeysView:
    """
    A live view of the keys of a DDict object. This object provides a keys view much like
    the native Python dict has a keys view object which is created by calling the keys method
    on the DDict. The DDictKeysView provides several operations which are efficiently
    implemented including len, iter, and membership.

    Unlike native dict objects, DDict objects do not support maintaining the order of keys
    inserted into the DDict. Hence, reversed is also not supported.

    """

    def __init__(self, ddict, managers, local=False):
        self._ddict = ddict
        self._managers = managers
        self._local = local

    def __len__(self):
        if self._local:
            return self._ddict.local_len()
        return len(self._ddict)

    def __iter__(self):
        return self._ddict._keys(self._managers)

    def __contains__(self, key):
        if self._local:
            # Inefficient implementation
            # for k in self:
            #     if k == key:
            #         return True
            # return False
            raise NotImplementedError("Contains method is not supported for local keys.")
        return key in self._ddict

    def __reversed__(self):
        raise NotImplementedError("There is no ordering to DDict keys and therefore they cannot be reversed.")

    @property
    def mapping(self):
        """
            Calling this results in a read-only version of the DDict.

        Returns:
            DDictMappingProxy: A read-only proxy to the original DDict.

        """
        if self._local:
            raise NotImplementedError("Mapping is not supported for local keys.")
        return DDictMappingProxy(self._ddict)


class DDictValuesView:
    """
    A live view of the values of a DDict object. This object provides a values view much like
    the native Python dict has a keys view object which is created by calling the values method
    on the DDict. The DDictValuesView provides several operations which are efficiently
    implemented including len and iter.

    Unlike native dict objects, DDict objects do not support maintaining the order of keys
    inserted into the DDict. Hence, reversed is also not supported.

    """

    def __init__(self, ddict, managers, local=False):
        self._ddict = ddict
        self._managers = managers
        self._local = local

    def __len__(self):
        if self._local:
            return self._ddict.local_len()
        return len(self._ddict)

    def __iter__(self):
        return self._ddict._values(self._managers)

    def __contains__(self, key):
        raise NotImplementedError("Membership operation is not supported for DDict values.")

    def __reversed__(self):
        raise NotImplementedError("There is no ordering to DDict keys and therefore they cannot be reversed.")

    @property
    def mapping(self):
        """
            Calling this results in a read-only version of the DDict.

        Returns:
            DDictMappingProxy: A read-only proxy to the original DDict.

        """
        if self._local:
            raise NotImplementedError("Mapping is not supported for local values.")
        return DDictMappingProxy(self._ddict)


class DDictItemsView:
    """
    A live view of the items of a DDict object. This object provides an items view much like
    the native Python dict has a items view object which is created by calling the items method
    on the DDict. The DDictItemsView provides several operations which are efficiently
    implemented including len and iter.

    Unlike native dict objects, DDict objects do not support maintaining the order of keys
    inserted into the DDict. Hence, reversed is also not supported.

    """

    def __init__(self, ddict, managers, local=False):
        self._ddict = ddict
        self._managers = managers
        self._local = local

    def __len__(self):
        if self._local:
            return self._ddict.local_len()
        return len(self._ddict)

    def __iter__(self):
        return self._ddict._items(self._managers)

    def __contains__(self, key):
        raise NotImplementedError("Membership operation is not supported for DDict items.")

    def __reversed__(self):
        raise NotImplementedError("There is no ordering to DDict keys and therefore they cannot be reversed.")

    @property
    def mapping(self):
        """
            Calling this results in a read-only version of the DDict.

        Returns:
            DDictMappingProxy: A read-only proxy to the original DDict.

        """
        if self._local:
            raise NotImplementedError("Mapping is not supported for local items.")
        return DDictMappingProxy(self._ddict)


class DDict:
    """
    The Distributed Dictionary provides a key/value store that is distributed across a series
    of managers and one nodes of a Dragon run-time.
    The goal is to evenly distribute data across all managers to provide a scalable
    implementation of a dictionary with a high degree of allowable parallelism. Clients attach
    to the Distributed Dictionary and store key/value pairs in it just like accessing a local
    dictionary in Python. However, the Distributed Dictionary goes beyond what the standard Python
    dictionary supports by including support for distributing data, checkpointing, and various
    other optimization opportunities for specific applications.
    """

    def __init__(
        self,
        managers_per_node: int = 1,
        n_nodes: int = 1,
        total_mem: int = DDICT_MIN_SIZE,
        *,
        working_set_size: int = 1,
        wait_for_keys: bool = False,
        wait_for_writers: bool = False,
        policy: policy.Policy or list[policy.Policy] = None,
        managers_per_policy: int = 1,
        persist_freq: int = 0,
        name: str = "",
        timeout: float = DDICT_DEFAULT_TIMEOUT,
        trace: bool = False,
        restart: bool = False,
    ) -> None:
        """
        Construct a Distributed Dictionary to be shared amongst distributed processes running
        in the Dragon Runtime. The distributed dictionary creates the specified number of managers
        and shards the data across all managers. The total memory of the dictionary is split
        across all the managers, so you want to allocate more space than is required by perhaps
        30 percent, but that should be determined via some experimentation and depends on the
        application being developed. See the Dragon documentation's section on the Distributed
        Dictionary design for more details about creating and using a distributed dictionary.

        :param managers_per_node: The number of managers on each node. The
             total_mem is divided up amongst the managers. If a list of
             policies is provided then this is the number of managers
             per policy. Each policy could be used to start more than
             one manager per node, in a potentially heterogeneous way.
             Defaults to 1.

        :param n_nodes: The number of nodes that will have managers
             deployed on them. This must be set to None if a list of policies is
             provided. Defaults to 1.

        :param total_mem: The total memory in bytes that will be
             sharded evenly across all managers. Defaults to DDICT_MIN_SIZE
             but this is really a minimum size for a single manager
             and should be specified by the user.

        :param working_set_size: Not implemented yet. This sets the
             size of the checkpoint, in memory, working set. This
             determines how much state each manager will keep
             internally. This is the number of different, simultaneous
             checkpoints that may be active at any point in time.
             Defaults to 1.

        :param wait_for_keys: Not implemented yet. Setting this to
             true means that each manager will keep track of a set of
             keys at each checkpoint level and clients advancing to a
             new checkpoint level will block until the set of keys at
             the oldest, retiring working set checkpoint are all
             written. By specifying this all clients will remain in
             sync with each other relative to the size of the working
             set. Defaults to False. It is also possible to store
             key/values that are not part of the checkpointing set of
             key/values. Those keys are called persistent keys and
             will not be affected by setting this argument to true.
             Specifying wait_for_keys also means that readers will block while
             waiting for a non-persistent key to be written until the
             key is found or a timeout occurs.

        :param wait_for_writers: Not implemented yet. Setting this
             to true means that each manager will wait for a set of
             clients to have all advanced their checkpoint id beyond
             the oldest checkpointing id before retiring a checkpoint
             from the working set. Setting this to true will cause
             clients that are advancing rapidly to block while others
             catch up. Defaults to False.

        :param policy: A policy
             can be supplied for starting the managers. Please read about policies in the
             Process Group documentation. Managers are started via a
             Process Group and placement of managers and other
             characteristics can be controlled via a policy or list of policies.
             If a list of policies is given then managers_per_node processes are
             started for each policy. Defaults to None which applies a
             Round-Robin policy.

        :param managers_per_policy: The number of managers started
             with each policy when a list of policies is provided. The total_mem
             is divided up evenly amongst the managers. This is the Defaults to
             1.

        :param persist_freq: Not implemented yet. This is the
             frequency that a checkpoint will be persisted to disk.
             This is independent of the working set size and can be
             any frequency desired. Defaults to 0 which means that no
             persisting will be done.

        :param name: Not implemented yet. This is a
             base file name to be applied to persisted state for the
             dictionary. This base name along with a checkpoint number
             is used to restore a distributed dictionary from a
             persisted checkpoint. Defaults to "".

        :param timeout: This is a timeout that will be used for
             all timeouts on the creating client and all managers
             during communication between the distributed components
             of the dictionary. New clients wishing to set their own
             timeout can use the attach method to specify their own
             local timeout. Defaults to None (block).

        :param trace: Defaults to False. If set to true, all
             interaction between clients and managers is logged. This results
             in large logs, but may help in debugging.

        :returns: None and a new instance of a distributed dictionary is initialized.

        :raises AttributeError: If incorrect parameters are supplied.
        :raises RuntimeError: If there was an unexpected error during initialization.
        """
        # Store all input arguments. We need this to create a copy of the dictionary.
        self._input_args = locals()
        del self._input_args["self"]

        self.setup_logging()

        # This is the pattern used in the pydragon_perf.pyx file
        # It works, but may need review if it's the way we want to do it

        # This block turns on client log for the initial client that creates the dictionary
        try:
            if type(managers_per_node) is not int and type(managers_per_policy) is not int:
                raise AttributeError(
                    "When creating a Dragon Distributed Dict you must provide managers_per_node or managers_per_policy"
                )

            if type(total_mem) is not int:
                raise AttributeError("When creating a Dragon Distributed Dict you must provide total_mem")

            if type(policy) is not list and type(n_nodes) is not int:
                raise AttributeError(
                    "When creating a Dragon Distributed Dict if you provide a single policy you must provide n_nodes"
                )

            if type(policy) is list and (n_nodes is not None or managers_per_node is not None):
                raise AttributeError(
                    "When creating a Dragon Distributed Dict if you provide a list of policies you must provide n_nodes = None and managers_per_node=None"
                )

            if restart and policy is not None:
                raise AttributeError(
                    "When restarting a Dragon Distributed Dict if you specified restart you must not provide policy."
                )

            # we overwrite n_nodes and managers_per_node so that we get the correct division of the total memory among the managers that are launched as part of the process group.
            if isinstance(policy, list):
                if len(policy) == 0:
                    raise AttributeError(
                        "When creating a Dragon Distributed Dict if you provide a list of policies you must provide at least one policy in the list"
                    )
                n_nodes = len(policy)
                managers_per_node = managers_per_policy

            # Start the Orchestrator and capture its serialized descriptor so we can connect to it.
            self._orc_proc = Popen(
                executable=sys.executable,
                args=[
                    "-c",
                    f"import dragon.data.ddict.orchestrator as orc; orc.start({managers_per_node}, {n_nodes}, {total_mem}, {trace})",
                ],
                stdout=Popen.PIPE,
            )

            # Read the serialized FLI of the orchestrator.
            ddict = self._orc_proc.stdout.recv().strip()

            self._orc_connector = fli.FLInterface.attach(b64decode(ddict))
            self._args = (
                working_set_size,
                wait_for_keys,
                wait_for_writers,
                policy,
                persist_freq,
                name,
                timeout,
                restart,
            )

            self._managers_per_node = managers_per_node
            self._wait_for_keys = wait_for_keys
            self._wait_for_writers = wait_for_writers
            self._init_props((True, ddict, timeout, trace, self._input_args))
        except Exception as ex:
            tb = traceback.format_exc()
            log.debug("There is an exception initializing ddict: %s\nTraceback: %s", ex, tb)
            raise RuntimeError(f"There is an exception initializing ddict: {ex}\nTraceback: {tb}\n")

    def __setstate__(self, args):
        self._init_props(args)

    def _init_props(self, args):
        self._creator, serialized_orc, timeout, trace, self._input_args = args
        # -1 indicates to use the default timeout since the default
        # is sometimes None and None can't be passed through capnproto
        if timeout == -1:
            self._timeout = DDICT_DEFAULT_TIMEOUT
        else:
            self._timeout = timeout

        self.setup_logging()
        random.seed()

        self._managers = dict()
        self._tag = 0
        self._chkpt_id = 0
        self._destroyed = False
        self._detached = False
        self._timeout = timeout
        self._trace = trace
        self._chosen_manager = None

        # Batch put
        self._batch_put_started = False
        self._batch_put_msg_tags = set()
        self._num_batch_puts = {}
        self._batch_put_stream_channels = {}
        self._opened_send_handles = {}

        # Broadcast put with batch
        self._bput_strm = None
        self._bput_root_manager_sendh = None
        self._bput_resp_strm = None
        self._bput_respFLI = None
        self._bput_tag = None
        self._num_bputs = 0

        # custom pickler
        self._key_pickler = None
        self._value_pickler = None

        try:
            self._traceit("Connecting to ddict.")
            self._return_channel = Channel.make_process_local()
            self._buffered_return_channel = Channel.make_process_local()
            self._main_stream_channel = Channel.make_process_local()

            self._default_pool = self._return_channel.get_pool()

            self._return_connector = fli.FLInterface(main_ch=self._return_channel)
            self._serialized_return_connector = b64encode(self._return_connector.serialize())

            self._buffered_return_connector = fli.FLInterface(
                main_ch=self._buffered_return_channel, use_buffered_protocol=True
            )
            self._serialized_buffered_return_connector = b64encode(self._buffered_return_connector.serialize())

            self._serialized_orc = serialized_orc
            if self._creator:
                self._create(b64encode(cloudpickle.dumps((self._args))))
            else:
                self._orc_connector = fli.FLInterface.attach(b64decode(serialized_orc))

            self._client_id = None

            # if the client has a local manager, it is the main manager
            self._has_local_manager = False
            self._local_manager = None
            self._main_manager = None
            self._manager_nodes = []
            self._local_managers = []
            self._host_id = host_id()

            self._get_main_manager()
            self._register_client_to_main_manager(timeout)
        except DDictUnableToCreateDDict as ex:
            tb = traceback.format_exc()
            raise DDictUnableToCreateDDict(DragonError.FAILURE, "Failed to create dictionary.")
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug("There is an exception __setstate__ of ddict: %s\n Traceback: %s\n", ex, tb)
            except:
                pass
            raise RuntimeError(f"There is an exception __setstate__ of ddict.")

    def __getstate__(self):
        return (False, self.serialize(), self._timeout, self._trace, self._input_args)

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        return

    def setup_logging(self):
        # This block turns on a client log for each client
        global log
        if log is None:
            fname = f"{dls.DD}_{socket.gethostname()}_client_{str(this_process.my_puid)}.log"
            setup_BE_logging(service=dls.DD, fname=fname)
            log = logging.getLogger(str(dls.DD))

    def __del__(self):
        try:
            self.detach()
            self._free_process_local_channels()
        except Exception as ex:
            try:
                tb = traceback.format_exc()
                log.debug(
                    "There was an exception while terminating the Distributed Dictionary. Exception is %s\n Traceback: %s\n",
                    ex,
                    tb,
                )
            except:
                pass

    def _create(self, pickled_args):
        msg = dmsg.DDCreate(self._tag_inc(), respFLI=self._serialized_buffered_return_connector, args=pickled_args)
        resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)

        if resp_msg.err == DragonError.FAILURE:
            raise DDictUnableToCreateDDict(resp_msg.err, resp_msg.errInfo)

        if resp_msg.err != DragonError.SUCCESS:
            self._cleanup()
            raise DDictError(resp_msg.err, f"Failed to create dictionary! {resp_msg.errInfo}")

    def _free_process_local_channels(self):
        try:
            self._return_channel.destroy_process_local()
            self._return_channel = None
        except:
            pass
        try:
            self._buffered_return_channel.destroy_process_local()
            self._buffered_return_channel = None
        except:
            pass
        try:
            self._main_stream_channel.destroy_process_local()
            self._main_stream_channel = None
        except:
            pass

    def _cleanup(self):
        if self._creator:
            try:
                self._orc_proc.wait()
                log.debug("joined orc proc")
            except Exception as ex:
                tb = traceback.format_exc()
                try:
                    log.debug("There was an exception while joining orchestrator process: %s\n Traceback: %s", ex, tb)
                except:
                    pass

    def _traceit(self, *args, **kw_args):
        if self._trace:
            log.log(logging.INFO, *args, **kw_args)

    def _get_main_manager(self):  # SHGetKV
        try:
            serialized_main_manager = get_local_kv(key=self._serialized_orc)
            self._main_manager_connection = fli.FLInterface.attach(b64decode(serialized_main_manager))
            self._has_local_manager = True
        except KeyError as e:
            # no manager on the node, get a random manager from orchestrator
            try:
                log.info(
                    "Got KeyError during bringup, sending get random manager request to orchestrator. Exception was %s",
                    e,
                )
            except:
                pass
            msg = dmsg.DDRandomManager(self._tag_inc(), respFLI=self._serialized_buffered_return_connector)
            resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, f"Client failed to get manager from orchestrator. {resp_msg.errInfo}")
            self._main_manager_connection = fli.FLInterface.attach(b64decode(resp_msg.manager))

    def _register_client_to_main_manager(self, timeout):  # client ID assigned here
        msg = dmsg.DDRegisterClient(
            self._tag_inc(),
            respFLI=self._serialized_return_connector,
            bufferedRespFLI=self._serialized_buffered_return_connector,
        )  # register client to the manager (same node)
        resp_msg = self._send_receive([(msg, None)], connection=self._main_manager_connection)
        if resp_msg.err != DragonError.SUCCESS:
            raise DDictError(resp_msg.err, f"Client failed to connect to main manager.{resp_msg.errInfo}")
        # -1 indicates to use the default timeout since the default
        # is sometimes None and None can't be passed through capnproto
        if timeout == -1:
            self._timeout = resp_msg.timeout
        self._client_id = resp_msg.clientID
        self._num_managers = resp_msg.numManagers
        for serialized_node in resp_msg.managerNodes:
            self._manager_nodes.append(cloudpickle.loads(b64decode(serialized_node)))
        # local managers is a list of local managers' ID
        for i in range(self._num_managers):
            if self._manager_nodes[i].h_uid == self._host_id:
                self._local_managers.append(i)
        self._name = resp_msg.name
        self._managers[resp_msg.managerID] = self._main_manager_connection
        self._main_manager = resp_msg.managerID
        if self._has_local_manager:
            self._local_manager = resp_msg.managerID

    def _connect_to_manager(self, manager_id):
        msg = dmsg.DDConnectToManager(self._tag_inc(), clientID=self._client_id, managerID=manager_id)
        resp_msg = self._send_receive([(msg, None)], connection=self._main_manager_connection)
        if resp_msg.err != DragonError.SUCCESS:
            raise DDictError(resp_msg.err, resp_msg.errInfo)

        self._managers[manager_id] = fli.FLInterface.attach(b64decode(resp_msg.manager))

    def _register_client_ID_to_manager(self, manager_id):
        try:
            msg = dmsg.DDRegisterClientID(
                self._tag_inc(),
                clientID=self._client_id,
                respFLI=self._serialized_return_connector,
                bufferedRespFLI=self._serialized_buffered_return_connector,
            )
            resp_msg = self._send_receive([(msg, None)], connection=self._managers[manager_id])
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug(
                    "There was an exception registering client ID %s with manager: %s \n Traceback: %s",
                    self._client_id,
                    ex,
                    tb,
                )
            except:
                pass
            raise RuntimeError(
                f"There was an exception registering client ID {self._client_id=} with manager: {ex} \n Traceback: {tb}"
            )

    def _check_manager_connection(self, manager_id=None, all=False):
        if all:
            for manager_id in range(self._num_managers):
                if manager_id not in self._managers:
                    self._connect_to_manager(manager_id)
                    self._register_client_ID_to_manager(manager_id)
        elif not (manager_id in self._managers):
            self._connect_to_manager(manager_id)
            self._register_client_ID_to_manager(manager_id)

    def _choose_manager_pickle_key(self, key):
        # Check to see if there is a user-defined hash function. If so, then
        # assume it is deterministic and the same across all nodes and use it.
        if self._key_pickler is None:
            pickled_key = cloudpickle.dumps(key)
            stripped_key = strip_pickled_bytes(pickled_key)
        else:
            pickled_key = self._key_pickler.dumps(key)
            stripped_key = None  # Not used if the user provided a custom pickler

        if self._chosen_manager is not None:
            return (self._chosen_manager, pickled_key)

        if self._key_pickler is None:
            # We will try this first if no chosen manager. Might not be instance of
            # one of these. If not we fall down to code below which is what we want
            # when a manager is chosen.
            if isinstance(key, int):
                return (dragon_hash(str(key).encode("utf-8")) % self._num_managers, pickled_key)

            if isinstance(key, str):
                return (dragon_hash(key.encode("utf-8")) % self._num_managers, pickled_key)

            hash_val = dragon_hash(stripped_key)
            manager_id = hash_val % self._num_managers
        else:
            manager_id = dragon_hash(pickled_key) % self._num_managers

        return (manager_id, pickled_key)

    def _tag_inc(self):
        tag = self._tag
        self._tag += 1
        return tag

    def _send(self, msglist, connection):
        self._traceit("Opening send handle.")

        if connection.is_buffered:
            strm = None
        else:
            strm = self._main_stream_channel

        with connection.sendh(stream_channel=strm, timeout=self._timeout) as sendh:
            for msg, arg in msglist:
                self._traceit("Sending msg: %s", msg)
                if arg is None:
                    sendh.send_bytes(msg.serialize(), timeout=self._timeout)
                else:
                    # It is a pickled value so don't call serialize.
                    sendh.send_bytes(msg, arg=arg, timeout=self._timeout)

    def _recv_resp(self, resp_set, buffered_connector):
        self._traceit("About to open receive handle on fli to receive response.")
        done = False
        with buffered_connector.recvh(timeout=self._timeout) as recvh:
            while not done:
                resp_ser_msg, hint = recvh.recv_bytes(timeout=self._timeout)
                msg = dmsg.parse(resp_ser_msg)
                if msg.ref not in resp_set:
                    log.info("Tossing lost/timed out response message in DDict Client: %s", msg)
                else:
                    resp_set.remove(msg.ref)
                    done = True

        self._traceit("Response: %s", msg)
        return msg

    def _recv_responses(self, resp_set, num_responses, connector=None):
        msglist = []
        if connector is None:
            connector = self._buffered_return_connector
        for _ in range(num_responses):
            if len(resp_set) == 1 and num_responses > 1:
                # Expects multiple responses with the same ref ID. (ex: DDClear, DDLength ... etc.)
                # This happens when client sends request to the root manager who then broadcasts the
                # request to all other managers. All managers then create the responses using the exact
                # same request tag and send it back to client directly. Therefore, the client should
                # expect multiple responses with the same ref ID.
                resp_msg = self._recv_resp(set(resp_set), connector)
            else:
                resp_msg = self._recv_resp(resp_set, connector)
            msglist.append(resp_msg)

        return msglist

    def _bput_recv_responses_and_check(self, resp_set, num_responses, expected_num_puts=1, connector=None):
        msglist = []
        if connector is None:
            connector = self._buffered_return_connector
        for _ in range(num_responses):
            resp_msg = None
            if len(resp_set) == 1 and num_responses > 1:
                # Expects multiple responses with the same ref ID. (ex: DDClear, DDLength ... etc.)
                # This happens when client sends request to the root manager who then broadcasts the
                # request to all other managers. All managers then create the responses using the exact
                # same request tag and send it back to client directly. Therefore, the client should
                # expect multiple responses with the same ref ID.
                resp_msg = self._recv_resp(set(resp_set), connector)

            else:
                resp_msg = self._recv_resp(resp_set, connector)

            if resp_msg.err == DragonError.MEMORY_POOL_FULL:
                log.debug(f"resp err is MEMORY_POOL_FULL, raising exception")
                raise DDictManagerFull(
                    DragonError.MEMORY_POOL_FULL,
                    f"Distributed Dictionary Manager {resp_msg.managerID} is full. The key/value pair was not stored.",
                )
            elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                raise DDictCheckpointSync(resp_msg.err, resp_msg.errInfo)

            elif resp_msg.err == DragonError.DDICT_FUTURE_CHECKPOINT:
                raise DDictFutureCheckpoint(resp_msg.err, resp_msg.errInfo)

            elif resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)

            elif resp_msg.numPuts != expected_num_puts:
                raise DDictError(
                    DragonError.FAILURE,
                    f"Failed to store all keys in manager {resp_msg.managerID} in the distributed dictionary. Expected number of keys to be written: {self._num_bputs}, number of successful writes: {resp_msg.numPuts}",
                )

    def _recv_dmsg_and_val(self, req_msg, key):
        self._traceit("About to open receive handle on fli to receive response and value.")
        with self._return_connector.recvh(use_main_as_stream_channel=True, timeout=self._timeout) as recvh:
            ref = -1
            while ref != req_msg.tag:
                resp_ser_msg, hint = recvh.recv_bytes(timeout=self._timeout)
                resp_msg = dmsg.parse(resp_ser_msg)
                self._traceit("Response: %s", resp_msg)
                ref = resp_msg.ref
                if ref != req_msg.tag:
                    log.info("Tossing lost/timed out message in DDict Client: %s", resp_msg)

            if resp_msg.err == DragonError.KEY_NOT_FOUND:
                value = self.__missing__(key, err=resp_msg.err)
            elif resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
            else:
                try:
                    if self._value_pickler is None:
                        value = cloudpickle.load(
                            file=PickleReadAdapter(recvh=recvh, hint=VALUE_HINT, timeout=self._timeout)
                        )
                    else:
                        value = self._value_pickler.load(
                            file=PickleReadAdapter(recvh=recvh, hint=VALUE_HINT, timeout=self._timeout)
                        )
                except Exception as e:
                    tb = traceback.format_exc()
                    try:
                        log.info("Exception caught in cloudpickle load: %s \n Traceback: %s", e, tb)
                    except:
                        pass
                    raise RuntimeError(f"Exception caught in cloudpickle load: {e} \n Traceback: {tb}")

        return value

    def __missing__(self, key, *, err=DragonError.SUCCESS):
        raise DDictKeyError(err, "The key was not found", key)

    def _send_receive(self, msglist, connection):
        try:
            msg = msglist[0][0]
            tag = msg.tag
            self._send(msglist, connection)
            resp_msg = self._recv_resp(set([msg.tag]), self._buffered_return_connector)
            return resp_msg

        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug("There was an exception in the _send_receive in ddict: %s\n Traceback: %s", ex, tb)
            except:
                pass
            raise RuntimeError(f"There was an exception in the _send_receive in ddict: {ex} \n Traceback: {tb}")

    def _send_dmsg_and_key_value(self, manager_id, msg, pickled_key, key, value):
        self._check_manager_connection(manager_id)
        try:
            self._traceit("Opening send handle to manager for put: %s", manager_id)
            with self._managers[manager_id].sendh(
                stream_channel=self._main_stream_channel, timeout=self._timeout
            ) as sendh:
                self._traceit("Sending to manager: %s", msg)
                sendh.send_bytes(msg.serialize(), timeout=self._timeout)
                self._traceit("Sending pickled key to manager: %s", key)
                sendh.send_bytes(pickled_key, arg=KEY_HINT, timeout=self._timeout)
                if self._value_pickler is not None:
                    self._value_pickler.dump(
                        value, file=PickleWriteAdapter(sendh=sendh, hint=VALUE_HINT, timeout=self._timeout)
                    )
                else:
                    cloudpickle.dump(
                        value,
                        file=PickleWriteAdapter(sendh=sendh, hint=VALUE_HINT, timeout=self._timeout),
                        protocol=pickle.HIGHEST_PROTOCOL,
                    )

        except TimeoutError as ex:
            raise DDictTimeoutError(
                DragonError.TIMEOUT,
                f"The operation timed out. This could be a network failure or an out of memory condition.\n{str(ex)}",
            )

    def _put(self, msg, key, value):
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._send_dmsg_and_key_value(manager_id, msg, pickled_key, key, value)
        try:
            resp_msg = self._recv_resp(set([msg.tag]), self._buffered_return_connector)
        except TimeoutError as ex:
            raise DDictTimeoutError(
                DragonError.TIMEOUT,
                f"The operation timed out. This could be a network failure or an out of memory condition.\n{str(ex)}",
            )

        if resp_msg.err == DragonError.MEMORY_POOL_FULL:
            raise DDictManagerFull(
                DragonError.MEMORY_POOL_FULL,
                f"Distributed Dictionary Manager {manager_id} is full. The key/value pair was not stored.",
            )

        elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
            raise DDictCheckpointSync(resp_msg.err, resp_msg.errInfo)

        elif resp_msg.err != DragonError.SUCCESS:
            raise DDictError(
                resp_msg.err,
                "Failed to store key in the distributed dictionary.\nAdditional Information: %s" % resp_msg.errInfo,
            )

    def _batch_put(self, key: object, value: object, persist: bool):
        # hash the key and get the target manager ID
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._check_manager_connection(manager_id)
        try:
            # If there is not send handle for the manager yet, we create another stream channel
            # and open the send handle. Otherwise use the existing send handle.
            if manager_id in self._opened_send_handles:
                sendh = self._opened_send_handles[manager_id]
            else:
                strm = Channel.make_process_local()
                # Keep track of stream channels and send handle for cleanup in the end of batch put.
                self._batch_put_stream_channels[manager_id] = strm
                sendh = self._managers[manager_id].sendh(stream_channel=strm, timeout=self._timeout)
                self._opened_send_handles[manager_id] = sendh
                # Send DDBatchPutMsg first to notify the manager that the following puts are batch puts.
                msg = dmsg.DDBatchPut(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, persist=persist)
                self._batch_put_msg_tags.add(msg.tag)
                self._traceit("Sending to manager: %s", msg)
                sendh.send_bytes(msg.serialize(), timeout=self._timeout)

            self._traceit("Sending pickled key to manager: %s", key)
            sendh.send_bytes(pickled_key, arg=KEY_HINT, timeout=self._timeout)
            cloudpickle.dump(
                value,
                file=PickleWriteAdapter(sendh=sendh, hint=VALUE_HINT, timeout=self._timeout),
                protocol=pickle.HIGHEST_PROTOCOL,
            )
            # Keep track of the number of batch put to the manager
            if manager_id not in self._num_batch_puts:
                self._num_batch_puts[manager_id] = 0
            self._num_batch_puts[manager_id] += 1

        except TimeoutError as ex:
            raise DDictTimeoutError(
                DragonError.TIMEOUT,
                f"The operation timed out. This could be a network failure or an out of memory condition.\n{str(ex)}",
            )

    def destroy(self, allow_restart=False) -> None:
        """
        Destroy a Distributed Dictionary instance, freeing all the resources that were allocated
        when it was created. Any clients that are still attached to the dictionary and try to do
        an operation on it will experience an exception when attempting subsequent operations.
        """
        if self._destroyed:
            return

        self._traceit("Destroying the ddict.")

        self._destroyed = True
        try:
            msg = dmsg.DDDestroy(
                self._tag_inc(),
                self._client_id,
                respFLI=self._serialized_buffered_return_connector,
                allowRestart=allow_restart,
            )
            resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)
        except Exception as ex:
            tb = traceback.format_exc()
            try:
                log.debug("There was an exception in the destroy: %s\n Traceback: %s", ex, tb)
            except:
                pass

        try:
            self._orc_connector.detach()
        except Exception as ex:
            try:
                tb = traceback.format_exc()
                log.debug("There was an exception while detaching orchestrator channel: %s \n Traceback: %s", ex, tb)
            except:
                pass

        # join on the orchestrator proc
        if self._creator:
            try:
                self._orc_proc.wait()
            except Exception as ex:
                tb = traceback.format_exc()
                try:
                    log.debug("There was an exception while joining orchestrator process: %s \n Traceback: %s", ex, tb)
                except:
                    pass

        try:
            self._free_process_local_channels()
        except:
            pass

    def serialize(self) -> str:
        """
        Returns a serialized, base64 encoded descriptor (i.e. string) that may be shared with other
        processes for attaching. This is especially useful when sharing with C/C++ or Fortran code
        though not all clients are available yet. Within Python you can pass the Distributed Dictionary
        to another process and it will be automatically serialized and attached so using this method
        is not needed when passing to another Python process.

        :returns: A serialized, base64 encoded string that may be used for attaching to the dictionary.
        """
        return self._serialized_orc

    @classmethod
    def attach(cls, serialized_dict: str, *, timeout: float = None, trace: bool = False) -> DDict:
        """
        Within Python you typically do not need to call this method explicitly. It will be done
        automatically when you pass a Distributed Dictionary from one process to another. However,
        you can do this explicitly if desired/needed.

        :param serialized_dict: A serialized distributed dictionary.

        :param timeout: None or a float or int value. A value of None means to wait forever. Otherwise
            it is the number of seconds to wait while an operation is performed. This timeout is applied
            to all subsequent client operations that are performed by the process that is attaching
            this DDict.

        :param trace: If True, specifies that all operations on the distributed dictionary should be
            logged in detail within the client log.

        :returns: An attached serialized dictionary.
        :rtype: DDict

        :raises TimeoutError: If the timeout expires.

        :raises Exception: Other exceptions are possible if for instance the serialized dictionary
            no longer exists.

        """
        new_client = cls.__new__(cls)
        new_client._init_props((False, serialized_dict, timeout, trace, None))
        return new_client

    def detach(self) -> None:
        """
        Detach from the Distributed Dictionary and free all local resources of this client. But leave
        in place the DDict for other clients and processes.
        """

        try:
            if self._destroyed or self._detached or not self._creator:
                return

            self._traceit("Detaching from ddict.")

            self._detached = True

            for manager_id in self._managers:
                try:
                    msg = dmsg.DDDeregisterClient(
                        self._tag_inc(), clientID=self._client_id, respFLI=self._serialized_buffered_return_connector
                    )
                    resp_msg = self._send_receive([(msg, None)], connection=self._managers[manager_id])
                    if resp_msg.err != DragonError.SUCCESS:
                        log.debug("Error on response to deregister client %s", self._client_id)

                    self._managers[manager_id].detach()
                except:
                    pass

        except Exception as ex:
            try:
                tb = traceback.format_exc()
                log.debug(
                    "There was an exception while detaching the client %s. Exception: %s\n Traceback: %s",
                    self._client_id,
                    ex,
                    tb,
                )
            except:
                pass

    @classmethod
    def synchronize_ddicts(cls, serialized_ddicts: list[str]) -> None:
        """
        Synchronize managers across all parallel dictionaries.

        :param serialized_ddicts: a list of serialized dictionaries.
        """

        if len(serialized_ddicts) == 0:
            raise ValueError("The list of serialized ddicts must not be empty.")

        for ser_ddict in serialized_ddicts:
            if not isinstance(ser_ddict, str):
                raise DDictError(DragonError.INVALID_ARGUMENT, "The serialized dictionary must be a string.")

        new_client = cls.__new__(cls)
        new_client.__setstate__((False, serialized_ddicts[0], DDICT_DEFAULT_TIMEOUT, False, None))

        tags = set()
        # send request to every ddict orchestrator
        for ser_ddict in serialized_ddicts:
            current_tag = new_client._tag_inc()
            tags.add(current_tag)
            msg = dmsg.DDGetManagers(current_tag, respFLI=new_client._serialized_buffered_return_connector)
            connection = fli.FLInterface.attach(b64decode(ser_ddict))
            new_client._send([(msg, None)], connection)
            connection.detach()

        # receive responses from all orchestrators
        resp_num = len(serialized_ddicts)
        msglist = new_client._recv_responses(tags, resp_num)
        # check each response
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)

        # build empty and full manager list
        num_managers = len(msglist[0].managers)
        # Check managers with the same manager_id from all parallel dictionaries
        for manager_id in range(num_managers):
            empty_managers = []
            full_managers = []
            for resp_msg in msglist:
                # If the manager is empty, we append its fli to the empty_manager list.
                # Otherwise append its fli to full_managers.
                if resp_msg.emptyManagers[manager_id]:
                    empty_managers.append(resp_msg.managers[manager_id])
                else:
                    full_managers.append(resp_msg.managers[manager_id])

            if len(empty_managers) > 0:  # Sync manager only when there's any empty manager
                if len(full_managers) == 0:
                    raise RuntimeError(f"Failed to synchronize dictionary. No full manager for manager {manager_id}.")

                if len(full_managers) < len(empty_managers):
                    # guarantees the length of full managers is equal to or longer then empty managers.
                    full_managers = full_managers * int(1 + len(empty_managers) / len(full_managers))

                # Send sync request along with empty managers fli to full managers so that full managers can send the data to reconstruct
                # empty managers directly
                tags = set()
                for i in range(len(empty_managers)):
                    empty_fli = empty_managers[i]
                    full_fli = full_managers[i]
                    current_tag = new_client._tag_inc()
                    tags.add(current_tag)
                    msg = dmsg.DDManagerSync(
                        current_tag, emptyManagerFLI=empty_fli, respFLI=new_client._serialized_buffered_return_connector
                    )
                    connection = fli.FLInterface.attach(b64decode(full_fli))
                    new_client._send([(msg, None)], connection)
                    connection.detach()

                # Receive sync responses from all full managers
                resp_num = len(empty_managers)
                manager_sync_msglist = new_client._recv_responses(tags, resp_num)
                for resp_msg in manager_sync_msglist:
                    if resp_msg.err != DragonError.SUCCESS:
                        raise DDictSyncError(resp_msg.err, f"Failed to synchronize dictionary, {resp_msg.errInfo}")

        # unmark all empty managers after recovered successfully
        tags = set()
        managerIDs = [id for id in range(new_client._num_managers)]
        for ser_ddict in serialized_ddicts:
            current_tag = new_client._tag_inc()
            tags.add(current_tag)
            msg = dmsg.DDUnmarkDrainedManagers(
                current_tag, respFLI=new_client._serialized_buffered_return_connector, managerIDs=managerIDs
            )
            connection = fli.FLInterface.attach(b64decode(ser_ddict))
            new_client._send([(msg, None)], connection)
            connection.detach()

        # receive responses from all orchestrators
        resp_num = len(serialized_ddicts)
        msglist = new_client._recv_responses(tags, resp_num)
        # check each response
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)

        new_client.detach()

    def clone(self, clone_list: list[str]) -> None:
        """
        Clone dictionary to the list of dictionaries.

        :param clone_list: a list of serialized dictionaries.
        """
        if len(clone_list) == 0:
            raise ValueError("The list of serialized ddicts must not be empty.")

        for ser_ddict in clone_list:
            if not isinstance(ser_ddict, str):
                raise DDictError(DragonError.INVALID_ARGUMENT, "The serialized dictionary must be a string.")

        # check if ddicts in clone_list has the same number of managers
        ddict_connections = []
        tags = set()
        for ser_ddict in clone_list:
            current_tag = self._tag_inc()
            tags.add(current_tag)
            msg = dmsg.DDGetMetaData(current_tag, respFLI=self._serialized_buffered_return_connector)
            connection = fli.FLInterface.attach(b64decode(ser_ddict))
            ddict_connections.append(connection)
            self._send([(msg, None)], connection)

        # receive responses from all orchestrator and check that the meta data match
        resp_num = len(clone_list)
        msglist = self._recv_responses(tags, resp_num)
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)

            if resp_msg.numManagers != self._num_managers:
                raise RuntimeError("Metadata mismatches. Could not clone dictionary with different metadata.")

        # marks all managers as empty managers for all dictionary
        tags = set()
        managerIDs = [i for i in range(self._num_managers)]
        for ddict_connection in ddict_connections:
            current_tag = self._tag_inc()
            tags.add(current_tag)
            msg = dmsg.DDMarkDrainedManagers(
                current_tag, respFLI=self._serialized_buffered_return_connector, managerIDs=managerIDs
            )
            self._send([(msg, None)], ddict_connection)
            ddict_connection.detach()

        msglist = self._recv_responses(tags, len(ddict_connections))
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)

        del ddict_connections

        # Only current dictionary has full managers. It will be used to reconstruct empty managers from other dictionaries.
        ddicts = [self._serialized_orc]
        ddicts.extend(clone_list)
        # synchronize dictionaries
        self.synchronize_ddicts(ddicts)

    def manager(self, id: int) -> DDict:
        """
        Return a version of the current ddict that will always choose the given manager for storing and retrieving data.

        :param id: The manager id of the chosen manager.
        :raises Exception: If the manager id is not a valid id.
        """

        if id < 0 or id >= self._num_managers:
            raise DDictError(DragonError.INVALID_ARGUMENT, f"The value {id} is not a valid manager id.")
        mgr_dd = copy.copy(self)
        mgr_dd._chosen_manager = id
        mgr_dd._creator = False
        mgr_dd._key_pickler = self._key_pickler
        mgr_dd._value_pickler = self._value_pickler

        return mgr_dd

    def pickler(self, key_pickler=None, value_pickler=None) -> DDict:
        pickler_dd = copy.copy(self)
        pickler_dd._key_pickler = key_pickler
        pickler_dd._value_pickler = value_pickler
        pickler_dd._chosen_manager = self._chosen_manager
        pickler_dd._creator = False

        return pickler_dd

    def __setitem__(self, key: object, value: object) -> None:
        """
        Store the key/value pair in the current checkpoint within the Distributed Dictionary.
        Due to the nature of a parallel, distributed dictionary, insertion order into the
        distributed dictionary is not maintained.

        :param key: The key of the pair. It must be serializable.
        :param value: the value of the pair. It also must be serializable.
        :raises Exception: Various exceptions can be raised including TimeoutError.
        """
        if self._batch_put_started:
            if self._batch_persist:
                raise DDictError(
                    DragonError.INVALID_OPERATION,
                    "Persistent value mismatch. Could not perform non-persistent put during batch put.",
                )
            self._batch_put(key, value, False)
        else:
            msg = dmsg.DDPut(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, persist=False)
            self._put(msg, key, value)

    def __getitem__(self, key: object) -> object:
        """
        Get the value that is associated with the given key.

        :param key: The key of a stored key/value pair.
        :returns: The value associated with the key.
        :raises Exception: Various exceptions can be raised including TimeoutError and KeyError.
        """
        msg = dmsg.DDGet(self._tag_inc(), self._client_id, chkptID=self._chkpt_id)
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._check_manager_connection(manager_id)
        self._send([(msg, None), (pickled_key, KEY_HINT)], self._managers[manager_id])

        value = self._recv_dmsg_and_val(msg, key)
        return value

    def __contains__(self, key: object) -> bool:
        """
        Returns True if key is in the Distributed Dictionary and False otherwise.

        :param key: A possible key stored in the DDict.
        :returns bool: True or False depending on if the key is there or not.
        :raises: Various exceptions can be raised including TimeoutError.
        """
        msg = dmsg.DDContains(self._tag_inc(), self._client_id, chkptID=self._chkpt_id)
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._check_manager_connection(manager_id)
        resp_msg = self._send_receive([(msg, None), (pickled_key, KEY_HINT)], connection=self._managers[manager_id])

        if resp_msg.err == DragonError.SUCCESS:
            return True
        elif resp_msg.err == DragonError.KEY_NOT_FOUND:
            return False
        elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
            raise DDictCheckpointSync(resp_msg.err, resp_msg.errInfo)

        raise DDictError(resp_msg.err, resp_msg.errInfo)

    def __iter__(self) -> Iterator[DDict]:
        return iter(self.keys())

    def __len__(self) -> int:
        """
        Returns the number of keys stored in the entire Distributed Dictionary.

        :returns int: The number of stored keys in the current checkpoint plus any persistent keys.
        :raises: Various exceptions can be raised including TimeoutError.
        """
        self._traceit("Getting length from ddict")
        tag = self._tag_inc()
        broadcast = self._chosen_manager is None

        if broadcast:
            selected_manager = 0
            resp_num = self._num_managers
        else:
            selected_manager = self._chosen_manager
            resp_num = 1

        self._check_manager_connection(selected_manager)
        msg = dmsg.DDLength(
            tag,
            clientID=self._client_id,
            chkptID=self._chkpt_id,
            respFLI=self._serialized_buffered_return_connector,
            broadcast=broadcast,
        )
        self._send([(msg, None)], self._managers[selected_manager])
        msglist = self._recv_responses(set([tag]), resp_num)
        length = 0
        for resp_msg in msglist:
            if resp_msg.err == DragonError.SUCCESS:
                length += resp_msg.length
            elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                raise DDictCheckpointSync(resp_msg.err, resp_msg.errInfo)
            else:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
        return length

    def __delitem__(self, key: object) -> None:
        """
        Deletes a key/value pair from the Distributed Dictionary if it exists.

        :raises: Various exceptions can be raised including TimeoutError and KeyError.
        """
        self.pop(key)

    def _mark_as_drained(self, manager_id) -> None:
        """
        Notify orchestrator to mark the manager as drained for internal cross ddict synchronization testing.
        """
        msg = dmsg.DDMarkDrainedManagers(
            self._tag_inc(), respFLI=self._serialized_buffered_return_connector, managerIDs=[manager_id]
        )
        resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)
        if resp_msg.err != DragonError.SUCCESS:
            raise DDictError(resp_msg.err, f"Failed to mark manager {manager_id} as drained! {resp_msg.errInfo}")

    def pput(self, key: object, value: object) -> None:
        """
        Persistently store a key/value pair within the Distributed Dictionary. This is
        useful when checkpointing is employed in the dictionary. A persistent put of a
        key/value pair means that the key/value pair persists across checkpoints. Persistent
        key/value pairs are useful when putting constant values or other values that don't
        change across checkpoints.

        :param key: A serializable object that will be stored as the key in the DDict.
        :param value: A serializable object that will be stored as the value.
        """
        if self._batch_put_started:
            if not self._batch_persist:
                raise DDictError(
                    DragonError.INVALID_OPERATION,
                    "Persistent value mismatch. Could not perform persistent put during batch put.",
                )
            self._batch_put(key, value, True)
        else:
            msg = dmsg.DDPut(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, persist=True)
            self._put(msg, key, value)

    def bput(self, key: object, value: object) -> None:
        """
        Store a non-persistent key/value pair by brodcasting to all managers across the Distributed Dictionary.
        This is useful when multiple clients requesting the same key. This should be used carefully as each manager
        holds a duplicate of the key/value pair.

        :param key: A serializable object that will be stored as the key in the DDict.
        :param value: A serializable object that will be stored as the value.
        """
        _, pickled_key = self._choose_manager_pickle_key(key)

        if self._batch_put_started:
            # bput with batch
            if self._batch_persist:
                raise DDictError(
                    DragonError.INVALID_OPERATION,
                    "Persistent value mismatch. Could not perform broadcast put during batch put as bput only writes non-persistent key.",
                )
            try:
                # First call to bput in current batch, select a random root manager and create a new send
                # handle for it. A response channel is created to receive bput response during cleanup of
                # the batch.
                if self._bput_root_manager_sendh is None:

                    # This is the first call to bput in current batch, need to select a random root manager.
                    # For bput without batch put, a random root manager is selected every time.
                    managers = [i for i in range(self._num_managers)]
                    random.shuffle(managers)
                    # Assign the random root manager to the root manager of the bcast put with current batch.
                    root_manager = managers[0]
                    # connect the root manager
                    self._check_manager_connection(root_manager)
                    # create new send handle for the root manager
                    self._bput_strm = Channel.make_process_local()
                    self._bput_root_manager_sendh = self._managers[root_manager].sendh(
                        stream_channel=self._bput_strm, timeout=self._timeout
                    )

                    # There might be other response expected in the main response channel, so create a new
                    # response FLI that only receives the bput response for current batch. In the cleanup
                    # of the brocast put with batch, we will receive bput responses from every manager
                    # through this response FLI.
                    self._bput_resp_strm = Channel.make_process_local()
                    self._traceit(f"The local channel cuid is {self._bput_resp_strm.cuid}")
                    self._bput_respFLI = fli.FLInterface(main_ch=self._bput_resp_strm, use_buffered_protocol=True)
                    serializedRespFLI = b64encode(self._bput_respFLI.serialize())
                    self._bput_tag = self._tag_inc()
                    msg = dmsg.DDBPut(
                        self._bput_tag,
                        self._client_id,
                        self._chkpt_id,
                        serializedRespFLI,
                        managers,
                        True,
                    )

                    self._traceit("Sending to manager: %s", msg)
                    self._bput_root_manager_sendh.send_bytes(msg.serialize(), timeout=self._timeout)

                # send key and value
                self._traceit("Sending pickled key to manager: %s", key)
                self._bput_root_manager_sendh.send_bytes(pickled_key, arg=KEY_HINT, timeout=self._timeout)
                cloudpickle.dump(
                    value,
                    file=PickleWriteAdapter(
                        sendh=self._bput_root_manager_sendh, hint=VALUE_HINT, timeout=self._timeout
                    ),
                    protocol=pickle.HIGHEST_PROTOCOL,
                )
                self._num_bputs += 1

            except TimeoutError as ex:
                raise DDictTimeoutError(
                    DragonError.TIMEOUT,
                    f"The operation timed out. This could be a network failure or an out of memory condition.\n{str(ex)}",
                )

            except Exception as ex:
                tb = traceback.format_exc()
                try:
                    log.debug(
                        "There was an exception performing broadcast put with batch put: %s\nTraceback:: %s", ex, tb
                    )
                except:
                    pass
                raise
        else:
            # For bput without batch put, a random root manager is selected every time.
            managers = [i for i in range(self._num_managers)]
            random.shuffle(managers)

            tag = self._tag_inc()
            msg = dmsg.DDBPut(
                tag,
                self._client_id,
                self._chkpt_id,
                self._serialized_buffered_return_connector,
                managers,
                False,
            )
            self._send_dmsg_and_key_value(managers[0], msg, pickled_key, key, value)
            resp_num = self._num_managers
            self._bput_recv_responses_and_check(set([tag]), resp_num, 1)

    def _end_bput_with_batch(self):
        self._bput_root_manager_sendh.close()
        self._bput_root_manager_sendh = None

        # receive response
        resp_num = self._num_managers
        self._bput_recv_responses_and_check(set([self._bput_tag]), resp_num, self._num_bputs, self._bput_respFLI)

        # cleanup
        self._bput_tag = None
        self._num_bputs = 0

        try:
            self._bput_respFLI.destroy()
            self._trace_it(f"Local channel cuid to be destroyed is {self._bput_resp_strm.cuid}")
            self._bput_resp_strm.destroy_process_local()
            self._trace_it(f"Local channel cuid to be destroyed is {self._bput_strm.cuid}")
            self._bput_strm.destroy_process_local()
        except:
            pass

        self._bput_respFLI = None
        self._bput_resp_strm = None
        self._bput_strm = None

    def bget(self, key: object) -> object:
        """
        Read the key written through bput. Each manager has a copy of the key, the client should be able to request
        the key from its main manager. Client request the key from chosen manager if there is one. Otherwise the
        client request key from its main manager.

        :param key: The key of a stored key/value pair.
        :returns: The value associated with the key.
        :raises Exception: Various exceptions can be raised including TimeoutError and KeyError.
        """
        msg = dmsg.DDGet(self._tag_inc(), self._client_id, chkptID=self._chkpt_id)
        _, pickled_key = self._choose_manager_pickle_key(key)
        if self._chosen_manager is not None:
            self._check_manager_connection(self._chosen_manager)
            manager = self._managers[self._chosen_manager]
        else:
            manager = self._main_manager_connection
        self._send([(msg, None), (pickled_key, KEY_HINT)], manager)

        value = self._recv_dmsg_and_val(msg, key)
        return value

    def _keys(self, managers: set[int]) -> Iterator[DDict]:
        try:
            # Since there could be multiple iterators in the same process over a DDict,
            # each iterator gets its own response channel/stream for keys to be streamed to it.
            # The main response FLI is not used because there may be other operations a user
            # wishes to do to interact with the DDict while iterating over it.
            iter_strm = Channel.make_process_local()
            self._traceit(f"The local channel cuid is {iter_strm.cuid}")
            respFLI = fli.FLInterface(main_ch=iter_strm)
            serializedRespFLI = b64encode(respFLI.serialize())

            # Shuffling the managers list helps if many clients all call keys at the same time.
            # It helps to distribute the communication to managers more evenly.
            manager_list = list(managers)
            random.shuffle(manager_list)
            managers = set(manager_list)

            for manager_id in managers:
                msg = dmsg.DDKeys(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, respFLI=serializedRespFLI)
                self._send([(msg, None)], self._managers[manager_id])
                self._traceit("About to open recv handle to retrieve keys from %s", manager_id)
                with respFLI.recvh(use_main_as_stream_channel=True, timeout=self._timeout) as recvh:
                    # Any data that is left in stream channel by an early break in the iterator is
                    # automatically discarded by the FLI close of the receive handle.
                    resp_ser_msg, _ = recvh.recv_bytes(timeout=self._timeout)
                    self._traceit("Got keys from %s", manager_id)
                    resp_msg = dmsg.parse(resp_ser_msg)
                    if resp_msg.ref == msg.tag:
                        if resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                            raise DDictError(resp_msg.err, resp_msg.errInfo)
                        elif resp_msg.err != DragonError.SUCCESS:
                            raise DDictError(resp_msg.err, "Failed to get key list in the distributed dictionary.")
                        done = False
                        while not done:
                            try:
                                key_bytes, _ = recvh.recv_bytes(timeout=self._timeout)
                                if self._key_pickler is None:
                                    key = cloudpickle.loads(key_bytes)
                                else:
                                    key = self._key_pickler.loads(key_bytes)
                                yield key
                            except EOFError:
                                done = True
                    # If the tag did not match, exiting the context manager will flush the rest of the response.

            if self._chosen_manager is not None:
                self._traceit("Keys was called on a manager directed ddict")
            else:
                self._traceit("Keys was called on all managers")
        except Exception as ex:
            tb = traceback.format_exc()
            log.debug("There was an exception iterating on the DDict keys: %s\nTraceback: %s", ex, tb)
            raise
        finally:
            try:
                respFLI.destroy()
                self._traceit(f"Local channel cuid to be destroyed is {iter_strm.cuid}")
                iter_strm.destroy_process_local()
            except:
                pass

    def _values(self, managers: set[int]) -> Iterator[DDict]:
        try:

            iter_strm = Channel.make_process_local()
            self._traceit(f"The local channel cuid is {iter_strm.cuid}")
            respFLI = fli.FLInterface(main_ch=iter_strm)
            serializedRespFLI = b64encode(respFLI.serialize())

            manager_list = list(managers)
            random.shuffle(manager_list)
            managers = set(manager_list)

            for manager_id in managers:
                msg = dmsg.DDValues(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, respFLI=serializedRespFLI)
                self._send([(msg, None)], self._managers[manager_id])
                self._traceit("About to open recv handle to retrieve values from %s", manager_id)
                with respFLI.recvh(use_main_as_stream_channel=True, timeout=self._timeout) as recvh:
                    resp_ser_msg, _ = recvh.recv_bytes(timeout=self._timeout)
                    self._traceit("Got values from %s", manager_id)
                    resp_msg = dmsg.parse(resp_ser_msg)
                    if resp_msg.ref == msg.tag:
                        if resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                            raise DDictError(resp_msg.err, resp_msg.errInfo)
                        elif resp_msg.err != DragonError.SUCCESS:
                            raise DDictError(resp_msg.err, "Failed to get value list in the distributed dictionary. ")
                        done = False
                        while not done:
                            try:
                                if self._value_pickler is None:
                                    value = cloudpickle.load(
                                        file=PickleReadAdapter(recvh=recvh, hint=VALUE_HINT, timeout=self._timeout)
                                    )
                                else:
                                    value = self._value_pickler.load(
                                        file=PickleReadAdapter(recvh=recvh, hint=VALUE_HINT, timeout=self._timeout)
                                    )
                                yield value
                            except EOFError:
                                done = True
                            except Exception as e:
                                tb = traceback.format_exc()
                                try:
                                    log.info("Exception caught in cloudpickle load: %s \n Traceback: %s", e, tb)
                                except:
                                    pass
                                raise RuntimeError(f"Exception caught in cloudpickle load: {e} \n Traceback: {tb}")

            if self._chosen_manager is not None:
                self._traceit("Values was called on a manager directed ddict")
            else:
                self._traceit("Values was called on all managers")
        except Exception as ex:
            tb = traceback.format_exc()
            log.debug("There was an exception iterating on the DDict values: %s\nTraceback: %s", ex, tb)
            raise
        finally:
            try:
                respFLI.destroy()
                self._traceit(f"Local channel cuid to be destroyed is {iter_strm.cuid}")
                iter_strm.destroy_process_local()
            except:
                pass

    def _items(self, managers: set[int]) -> Iterator[DDict]:
        try:

            iter_strm = Channel.make_process_local()
            self._traceit(f"The local channel cuid is {iter_strm.cuid}")
            respFLI = fli.FLInterface(main_ch=iter_strm)
            serializedRespFLI = b64encode(respFLI.serialize())

            manager_list = list(managers)
            random.shuffle(manager_list)
            managers = set(manager_list)

            for manager_id in managers:
                msg = dmsg.DDItems(self._tag_inc(), self._client_id, chkptID=self._chkpt_id, respFLI=serializedRespFLI)
                self._send([(msg, None)], self._managers[manager_id])
                self._traceit("About to open recv handle to retrieve items from %s", manager_id)
                with respFLI.recvh(use_main_as_stream_channel=True, timeout=self._timeout) as recvh:
                    resp_ser_msg, _ = recvh.recv_bytes(timeout=self._timeout)
                    self._traceit("Got items from %s", manager_id)
                    resp_msg = dmsg.parse(resp_ser_msg)
                    if resp_msg.ref == msg.tag:
                        if resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                            raise DDictError(resp_msg.err, resp_msg.errInfo)
                        elif resp_msg.err != DragonError.SUCCESS:
                            raise DDictError(resp_msg.err, "Failed to get item list in the distributed dictionary. ")
                        done = False
                        while not done:
                            try:
                                # receive key
                                key_bytes, hint = recvh.recv_bytes(timeout=self._timeout)
                                assert hint == KEY_HINT
                                if self._key_pickler is None:
                                    key = cloudpickle.loads(key_bytes)
                                else:
                                    key = self._key_pickler.loads(key_bytes)
                            except EOFError:
                                done = True
                                break  # No more stuff to receive, exit while loop immediately.
                            except Exception as ex:
                                tb = traceback.format_exc()
                                try:
                                    log.info("Exception caught while loading key: %s \n Traceback: %s", e, tb)
                                except:
                                    pass
                                raise RuntimeError(f"Exception caught while loading key: {e} \n Traceback: {tb}")

                            try:
                                # receive value
                                if self._value_pickler is None:
                                    value = cloudpickle.load(
                                        file=PickleReadAdapter(recvh=recvh, hint=VALUE_HINT, timeout=self._timeout)
                                    )
                                else:
                                    value = self._value_pickler.load(
                                        file=PickleReadAdapter(recvh=recvh, hint=VALUE_HINT, timeout=self._timeout)
                                    )
                                yield (key, value)
                            except Exception as e:
                                tb = traceback.format_exc()
                                try:
                                    log.info("Exception caught while loading value: %s \n Traceback: %s", e, tb)
                                except:
                                    pass
                                raise RuntimeError(f"Exception caught while loading value: {e} \n Traceback: {tb}")

            if self._chosen_manager is not None:
                self._traceit("Items was called on a manager directed ddict")
            else:
                self._traceit("Items was called on all managers")
        except Exception as ex:
            tb = traceback.format_exc()
            log.debug("There was an exception iterating on the DDict items: %s\nTraceback: %s", ex, tb)
            raise
        finally:
            try:
                respFLI.destroy()
                self._traceit(f"Local channel cuid to be destroyed is {iter_strm.cuid}")
                iter_strm.destroy_process_local()
            except:
                pass

    def local_len(self) -> int:
        tags = set()
        for i in self._local_managers:
            tag = self._tag_inc()
            tags.add(tag)
            msg = dmsg.DDLength(
                tag, clientID=self._client_id, respFLI=self._serialized_buffered_return_connector, broadcast=False
            )
            self._send([(msg, None)], self._managers[i])
        resp_num = len(self._local_managers)
        msglist = self._recv_responses(tags, resp_num)
        length = 0
        for resp_msg in msglist:
            if resp_msg.err == DragonError.SUCCESS:
                length += resp_msg.length
            elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                raise DDictCheckpointSync(resp_msg.err, resp_msg.errInfo)
            else:
                raise RuntimeError(resp_msg.err)
        return length

    def local_keys(self) -> list[object]:
        """

        Returns a DDictKeysView of the keys that are local to the process
        invoking this method.

        :returns: A DDictKeysView of the current DDict which has only the
        co-located node local keys of the DDict in it.

        """
        # connect to all local managers
        for i in self._local_managers:
            self._check_manager_connection(i)

        return DDictKeysView(self, self._local_managers)

    def keys(self) -> list[object]:
        """
        Returns a keys view of the distributed dictionary. From this view you can iterate over the
        keys or get the number of keys (i.e. length operation). See dict view objects for the
        methods available on a ddict keys view.

        :returns: A DDictKeysView object which is a live view of the DDict.
        """

        if self._chosen_manager is not None:
            self._check_manager_connection(self._chosen_manager)
            managers = [self._chosen_manager]
        else:
            self._check_manager_connection(all=True)
            managers = range(self._num_managers)

        return DDictKeysView(self, managers)

    def pop(self, key: object, default: object = None) -> object:
        """
        Pop the given key from the distributed dictionary and return the associated
        value. If the given key is not found in the dictionary, then KeyError is raised
        unless a default value is provided, in which case the default value is returned
        if the key is not found in the dictionary.

        :param key: A key to be popped from the distributed dictionary.

        :param default: A default value to be returned if the key is not in the
            distributed dictionary.

        :returns: The associated value if key is popped and the default value otherwise.
        """
        msg = dmsg.DDPop(self._tag_inc(), self._client_id, chkptID=self._chkpt_id)
        manager_id, pickled_key = self._choose_manager_pickle_key(key)
        self._check_manager_connection(manager_id)
        self._send([(msg, None), (pickled_key, KEY_HINT)], self._managers[manager_id])
        try:
            return self._recv_dmsg_and_val(msg, key)
        except KeyError as ex:
            if default is None:
                raise ex

            return default

    def clear(self) -> None:
        """
        Empty the distributed dictionary of all keys and values.
        """
        self._traceit("clearing dictionary for checkpoint %s", self._chkpt_id)
        tag = self._tag_inc()

        broadcast = self._chosen_manager is None

        if broadcast:
            selected_manager = 0
            resp_num = self._num_managers
        else:
            selected_manager = self._chosen_manager
            resp_num = 1

        self._check_manager_connection(selected_manager)
        msg = dmsg.DDClear(
            tag, self._client_id, self._chkpt_id, self._serialized_buffered_return_connector, broadcast=broadcast
        )
        self._send([(msg, None)], self._managers[selected_manager])
        msglist = self._recv_responses(set([tag]), resp_num)
        for resp_msg in msglist:
            if resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                raise DDictCheckpointSync(resp_msg.err, resp_msg.errInfo)
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)

    def get_name(self):
        return self._name

    def start_batch_put(self, persist=False) -> None:
        """
        Calling other APIs except for put before the batch put ends could leads to a hang or exception.
        """
        self._batch_put_started = True
        self._batch_persist = persist

    def end_batch_put(self) -> None:
        if self._batch_put_started:
            self._batch_put_started = False
            # If broadcast put (bput) is called during batch put, cleanup resrouces claimed for bput.
            if self._bput_root_manager_sendh is not None:
                self._end_bput_with_batch()
            # close all send handles created for batch put
            for i in self._opened_send_handles:
                self._opened_send_handles[i].close()
            self._opened_send_handles.clear()

            expected_num_responses = len(self._num_batch_puts)
            resp_msgs = self._recv_responses(self._batch_put_msg_tags, expected_num_responses)
            self._batch_put_msg_tags.clear()

            for resp_msg in resp_msgs:
                if resp_msg.err == DragonError.MEMORY_POOL_FULL:
                    raise DDictManagerFull(
                        DragonError.MEMORY_POOL_FULL,
                        f"Distributed Dictionary Manager {resp_msg.managerID} is full. The key/value pair was not stored.",
                    )

                elif resp_msg.err == DragonError.DDICT_CHECKPOINT_RETIRED:
                    raise DDictCheckpointSync(resp_msg.err, resp_msg.errInfo)

                elif resp_msg.err == DragonError.DDICT_FUTURE_CHECKPOINT:
                    raise DDictFutureCheckpoint(resp_msg.err, resp_msg.errInfo)

                elif resp_msg.err != DragonError.SUCCESS:
                    raise DDictError(resp_msg.err, resp_msg.errInfo)

                elif resp_msg.numPuts != self._num_batch_puts[resp_msg.managerID]:
                    raise DDictError(
                        DragonError.FAILURE,
                        f"Failed to store all keys in the distributed dictionary. Expected number of keys to be written: {self._num_batch_puts[resp_msg.managerID]}, number of successful writes: {resp_msg.numPuts}",
                    )

            self._num_batch_puts.clear()

            # Destroy stream channels only after all managers are done with batch put.
            try:
                for i in self._batch_put_stream_channels:
                    strm_ch = self._batch_put_stream_channels[i]
                    strm_ch.destroy_process_local()
            except:
                pass
            self._batch_put_stream_channels.clear()
        else:
            raise DDictError(DragonError.INVALID_OPERATION, "Could not end batch put without starting.")

    def values(self) -> list[object]:
        """
        When called this returns a list of all values in the Distributed Dictionary.

        :returns list[object]: A list of all values in the DDict.
        """

        if self._chosen_manager is not None:
            self._check_manager_connection(self._chosen_manager)
            managers = [self._chosen_manager]
        else:
            self._check_manager_connection(all=True)
            managers = range(self._num_managers)

        return DDictValuesView(self, managers)

    def local_values(self) -> list[object]:
        """

        Returns a DDictValuesView of the keys that are local to the process
        invoking this method.

        :returns: A DDictValuesView of the current DDict which has only the
        co-located node local values of the DDict in it.

        """
        # connect to all local managers
        for i in self._local_managers:
            self._check_manager_connection(i)

        return DDictValuesView(self, self._local_managers, True)

    def items(self) -> list[tuple[object, object]]:
        """
        Returns a list of all key/value pairs in the Distributed Dictionary.

        :returns list[tuple[object,object]]: A list of all key/value pairs.
        """
        if self._chosen_manager is not None:
            self._check_manager_connection(self._chosen_manager)
            managers = [self._chosen_manager]
        else:
            self._check_manager_connection(all=True)
            managers = range(self._num_managers)

        return DDictItemsView(self, managers)

    def local_items(self) -> list[object]:
        """

        Returns a DDictItemsView of the keys that are local to the process
        invoking this method.

        :returns: A DDictItemsView of the current DDict which has only the
        co-located node local items of the DDict in it.

        """
        # connect to all local managers
        for i in self._local_managers:
            self._check_manager_connection(i)

        return DDictItemsView(self, self._local_managers, True)

    def update(self, dict2: DDict) -> None:
        """
        Adds all key/value pairs from dict2 into this Distributed Dictionary.

        :param dict2: Another distributed dictionary.
        :raises NotImplementedError: Not implemented.
        """
        raise NotImplementedError("Not implemented on Dragon Distributed Dictionaries.")

    def popitem(self) -> tuple[object, object]:
        """
        Returns a random key/value pair from the Distributed Dictionary.

        :returns tuple[object,object]: A random key/value pair.
        :raises NotImplementedError: Not implemented.
        """
        raise NotImplementedError("Not implemented on Dragon Distributed Dictionaries.")

    def copy(self, name: str = "") -> DDict:
        """
        Returns a copy of the Distributed Dictionary.

        :returns DDict: A second DDict that is a copy of the first assuming that no
            other processes were concurrently using this DDict.
        """
        if not self._creator:
            raise DDictError(
                DragonError.INVALID_ARGUMENT,
                "Could not copy from a dictionary without input arguments. The original dictionary is needed to perform copy.",
            )
        input_args = self._input_args.copy()
        input_args["name"] = name
        copy_dd = DDict(**input_args)
        self.clone([copy_dd.serialize()])
        return copy_dd

    @property
    def stats(self) -> list[DDictManagerStats]:
        """
        Returns a list of manager stats, one for each manager of the distributed dictionary. See
        the DDictManagerStats structure for a description of its contents.
        """
        self._traceit("Getting stats from ddict")
        return list(self.dstats.values())

    @property
    def dstats(self) -> dict[int, DDictManagerStats]:
        """
        Returns a dict of manager stats, one for each manager of the distributed dictionary. See
        the DDictManagerStats structure for a description of its contents.
        """
        self._traceit("Getting stats from ddict")
        tag = self._tag_inc()

        broadcast = self._chosen_manager is None

        if broadcast:
            selected_manager = 0
            resp_num = self._num_managers
        else:
            selected_manager = self._chosen_manager
            resp_num = 1

        self._check_manager_connection(selected_manager)
        msg = dmsg.DDManagerStats(tag, respFLI=self._serialized_buffered_return_connector, broadcast=broadcast)
        self._send([(msg, None)], self._managers[selected_manager])
        msglist = self._recv_responses(set([tag]), resp_num)
        data = {}
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
            item = cloudpickle.loads(b64decode(resp_msg.data))
            data[item.manager_id] = item

        return data

    def checkpoint(self) -> None:
        """
        Calling checkpoint advances the checkpoint for the distributed dictionary. In
        subsequent calls to the distributed dictionary, like gets or puts, if the chosen
        manager does not have the current checkpoint in its working set, the get/put
        operations will block until the checkpoint becomes available. But, calling this
        operation itself does not block.
        """
        if self._batch_put_started:
            raise DDictError(DragonError.INVALID_OPERATION, "Could not proceed checkpoint during batch put.")
        self._chkpt_id += 1

    def rollback(self) -> None:
        """
        Calling rollback decrements the checkpoint id to its previous value. Again this
        call does not block. If rollback causes the checkpoint id to roll back to a
        checkpoint that a chosen manager no longer has in its working set, then subsequent
        operations may fail with a exception indicating the Checkpoint is no longer available,
        raising a DDictCheckpointSync exception.

        """
        if self._batch_put_started:
            raise DDictError(DragonError.INVALID_OPERATION, "Could not rollback checkpoint during batch put.")

        self._chkpt_id -= 1
        if self._chkpt_id < 0:
            try:
                log.debug("Reset checkpoint to 0 as the checkpoint rollback result %s is less than 0.", self._chkpt_id)
            except:
                pass
            self._chkpt_id = 0

    def sync_to_newest_checkpoint(self) -> None:
        """
        Advance the checkpoint identifier of this client to the newest checkpoint across all managers.
        This does not guarantee that all managers have advanced to the same checkpoint. That is up to the
        application which may guarantee all managers are at the same checkpoint by setting and getting
        values from managers in checkpoints and checkpoints advance. See the ddict_checkpoint_pi.py demo
        in examples/dragon_data/ddict for an example of an application that uses this method.
        """
        self._traceit("Syncing to newest checkpoint")
        tag = self._tag_inc()

        broadcast = self._chosen_manager is None

        if broadcast:
            selected_manager = 0
            resp_num = self._num_managers
        else:
            selected_manager = self._chosen_manager
            resp_num = 1

        chkpt_id = 0
        self._check_manager_connection(selected_manager)
        msg = dmsg.DDManagerNewestChkptID(tag, respFLI=self._serialized_buffered_return_connector, broadcast=broadcast)
        self._send([(msg, None)], self._managers[selected_manager])
        msglist = self._recv_responses(set([tag]), resp_num)
        for resp_msg in msglist:
            if resp_msg.err != DragonError.SUCCESS:
                raise DDictError(resp_msg.err, resp_msg.errInfo)
            chkpt_id = max(chkpt_id, resp_msg.chkptID)

        self._chkpt_id = chkpt_id

    def filter(self, mgr_code: FunctionType, mgr_code_args: tuple, comparator: FunctionType, branching_factor: int = 5):
        """
        Calling this instantiates a tree of process groups where mgr_code is
        expected to be a function that is invoked as mgr_code(args) where
        args are (dd, out_queue)+mgr_code_args. For instance, if
        mgr_code_args are (x,) then mgr_code(dd, outqueue, x) is how
        mgr_code is invoked.

        The dd of the mgr_code arguments is this distributed dictionary directed
        toward one manager in the collection of dd managers. In other
        words, dd is as if the manager method had been invoked on this
        distributed dictionary so mgr_code only interacts with the
        manager it was provided. In addition, mgr_code is executed on the
        same node where the manager it is directed toward is running.
        This means that mgr code will get the best possible performance
        while filtering data that is associated with its manager. The
        mgr_code can do whatever computation is desired, but its chosen
        output is put into the outqueue.

        All data written to outqueue is aggregated with data coming from each
        manager in a tree-like fashion so as to be scalable to tens of
        thousands of nodes. All data put in the outqueue by mgr_code is
        assumed to be ordered from best to worst. When data is aggregated
        for sending up the tree, it is aggregated according to some kind
        of ordering which is determined by the comparator function. The
        comparator will be called as comparator(x,y) and should return
        True if x is better than y and False otherwise. If there is no
        ordering, or the ordering is not relevant to the filtering, then
        comparator(x,y) may return a constant value of False or True and
        there will be no ordering of the data.

        The branching_factor of the filtering tree has a default value, but may
        be provided by the user to create a tree of whatever width is
        desired. Note that branching_factor is the max branching factor.
        Depending on the number of managers, some nodes in the tree
        may/will have smaller numbers of children.

        The filter function returns a Context Manager that supplies an Iterator
        over which you can iterate on the filtered values. So you can write
        `with dd.filter(...) as candidates:` and then iterate over `candidates`
        inside the context to read the filtered values.

        Assuming your distributed dictionary is called dd, this will get
        num_needed elements from the result of filtering the distributed
        dictionary by calling the function get_largest on each
        distributed dictionary manager.

        :param mgr_code: A function taking arguments as described above that will
        run on the same node as a distributed dictionary manager and will
        be directed toward that manager.

        :param comparator: A function taking two arguments that should return
        True if the first argument of the values being filtered is
        "better" than the second and False otherwise. Note that returning
        a constant value of True or False will result in the filtering
        imposing no order.

        :param branching_factor: The maximum branching factor of any interior
        node in the filtering tree (i.e. any aggregator).

        :returns: A Context Manager that supplies an iterator over which you can
        iterate over the filtered values.

        """

        stats = self.dstats
        nodes = query_all()
        if len(nodes) == 1:
            managers_hosts = [(manager_id, nodes[0].name) for manager_id in stats]
        else:
            managers_hosts = [(manager_id, stats[manager_id].hostname) for manager_id in stats]

        # Order by hostname so aggregators are close to other processes they will control.
        managers_hosts.sort(key=lambda tup: tup[1])
        filter_queue = SentinelQueue()
        pickled_mgr_code = cloudpickle.dumps(mgr_code)
        pickled_mgr_args = cloudpickle.dumps(mgr_code_args)
        pickled_comparator = cloudpickle.dumps(comparator)

        filter_proc = mp.Process(
            target=filter_aggregator,
            args=(
                self,
                managers_hosts,
                branching_factor,
                pickled_mgr_code,
                pickled_mgr_args,
                pickled_comparator,
                filter_queue,
            ),
        )
        filter_proc.start()

        cm = FilterContextManager(filter_proc, filter_queue)

        return cm

    @property
    def current_checkpoint_id(self) -> int:
        """
        Returns the current checkpoint id of the client.
        """
        return self._chkpt_id

    @property
    def local_managers(self) -> list[int]:
        """
        Returns all local manager ids of all managers that are local to this node.

        :raises NotImplementedError: Not implemented yet.
        """
        return self._local_managers

    @property
    def local_manager(self) -> int:
        """
        Returns a local manager id if one exists. This is manager designated as the main manager for the client. If
        no local manager exists, the None is returned.
        """
        return self._local_manager

    @property
    def main_manager(self) -> int:
        """
        Returns the main manager id. This will always exist and will be the same as the local manager
        id if a local manager exists. Otherwise, it will be the id of a random manager from another node.
        """
        return self._main_manager

    @property
    def manager_nodes(self) -> list[str]:
        """
        For each manager, the serialized, base64 encoded FLI of the manager is returned.
        """
        return self._manager_nodes

    @property
    def empty_managers(self):
        """
        Return a list of manager IDs that restarted on new nodes.
        """
        msg = dmsg.DDEmptyManagers(self._tag_inc(), respFLI=self._serialized_buffered_return_connector)
        resp_msg = self._send_receive([(msg, None)], connection=self._orc_connector)

        if resp_msg.err != DragonError.SUCCESS:
            self._cleanup()
            raise DDictError(resp_msg.err, f"Failed to create dictionary! {resp_msg.errInfo}")

        return resp_msg.managers
